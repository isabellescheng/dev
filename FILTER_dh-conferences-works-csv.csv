work_id,conference_label,conference_short_title,conference_theme_title,conference_year,conference_organizers,conference_series,conference_hosting_institutions,conference_city,conference_state,conference_country,conference_url,work_title,work_url,work_authors,work_type,full_text,full_text_type,full_text_license,parent_work_id,keywords,languages,topics
966,2009 - UMD College Park,UMD College Park,,2009,ADHO,ADHO,"University of Maryland, College Park",College Park,Maryland,United States,http://web.archive.org/web/20130307234434/http://mith.umd.edu/dh09/,The Digital Humanities Observatory: Building a National Collaboratory,,Susan Schreibman;Jennifer Edmond;Dot Porter;Shawn Day;Don Gourley,panel / roundtable,"The Digital Humanities Observatory (DHO) was established
to support digital humanities research in
Ireland, and to manage and coordinate the increasingly
complex e-resources created in the arts and humanities
throughout the island. The DHO, founded in 2008, has
in its first year begun to implement a plan of support focusing
on three main issues: encouraging collaboration;
providing for the management, access, and preservation
of project data; and promulgating shared standards and
technology for project development.
The DHO sits solidly in the family of recent international
initiatives seeking collaboration, sharing, and preservation,
that signal a shift in perspective in the digital humanities
environment from a project based (digital silo)
approach to one in which the scholarly resources we
create are linked, interoperable, reusable, and preserved.
Collectively, we have entered a new phase of human and
technical infrastructure development. An overview of
just a few events from the past few years will serve to
put the establishment of the DHO in perspective.
In April 2007 The National Endowment for the Humanities
sponsored a meeting gathering some 60 representatives from US digital humanities centres and institutes,
and funding agencies that support their work, to discuss
short- and long-term priorities and to encourage collaborative
opportunities. The result of this meeting was
centerNet, “an international network of digital humanities
centers formed for cooperative and collaborative action
that will benefit digital humanities and allied fields
in general and centers as humanities cyberinfrastructure
in particular.”
In February 2008 the Mellon Foundation-backed initiative,
Project Bamboo, began a series of workshops to lay
the groundwork for the building of an international infrastructure
(technical, social, and institutional) to tackle
the question, “How can we advance arts and humanities
research through the development of shared technology
services?” Unlike the NEH-sponsored meeting that led
to the development of centerNet, which drew attendees
solely from established digital humanities centers and
institutes, the Bamboo Project participants are pulled
additionally from humanities departments, IT support,
libraries, and administration from each attending organization.
These groups are not the traditional practitioners
of digital humanities, and Project Bamboo represents a
distinct opening up or “popularization” of the field within
academia. (http://projectbamboo.uchicago.edu/)
At the same time Project Bamboo was launched, the EUfunded
Interedition: An Interoperable Supranational
Infrastructure for Digital Editions (funded as a COST
Action February 2008-April 2012) was launched “to
promote the interoperability of the tools and methodology
… for digital scholarly editing and research.” Interedition
and projects like it have the potential to serve
both established digital humanities practitioners, such
as those represented by centerNet, and those new to the
field, such as participants in Project Bamboo. (http://
www.interedition.eu/)
Slightly later in the year, in September 2008, the Council
on Library and Information Resources published a white
paper, “No Brief Candle: Reconceiving Research Libraries
for the 21st Century,” which sets forth recommendations
for collaboration between and among faculty, librarians,
and IT professionals.
Conceived in 2004 and with the official kick-off meeting
in October 2008, DARIAH (Digital Research Infrastructure
for the Arts and Humanities), a network of fourteen
partners in ten European countries, is working to develop
infrastructure to support the preservation of cultural
heritage in Europe and improve access to research material
for the humanities. Currently in a preparation phase,
the project will begin in 2010. (http://www.dariah.edu/)
Despite the momentum in international collaborative
ventures, only a month prior to the April 2007 meeting,
the UK’s Arts and Humanities Research Council withdrew
funding for the Arts and Humanities Data Service
(AHDS) from March 2008, despite the fact that the 12
year old network of digital humanities centres of expertise
was one of the oldest and most well-respected national
infrastructures in the world (press release: http://
www.ahrc.ac.uk/News/Latest/Pages/AHRCreshapesitsfundingofICTresearch.
aspx).
The withdrawal of funding from the AHDS came just
as researchers in Ireland were in the final phases of conceiving
the DHO. While those writing the grant realized
that they did not want to foster the model prevalent in the
early to mid-1990s of the when digital humanities was
coalescing as a discipline, i.e. the lone scholar model
modified to encompass a small team of postgraduates
and technical staff, frequently employed by a digital
humanities centre. A model moe in keeping with other
international initiatives previously cited was needed:
one that reaches across national boarders to encourage
shared infrastructures, frameworks, and ontologies. Thus
the Digital Humanities Observatory, a national digital
humanities centre based in Dublin, Ireland, was founded
in 2008 as a response to international developments and
to a national need for digital humanities infrastructure
in Ireland.
The Digital Humanities Observatory
The DHO was created as part of a larger national infrastructure
entitled Humanities Servicing Irish Society
(HSIS). HSIS is comprised of five of the six Universities
in the Republic of Ireland (National University of
Ireland, Galway; National University of Ireland, Maynooth;
Trinity College Dublin; University College
Cork; University College Dublin), the two Universities
in the North of Ireland (Queens University, Belfast and
University of Ulster), and several institutions of higher
education in the Republic (College of Art and Design,
Dundalk Institute of Technology, St Patrick’s Teacher
Training College, Royal Irish Academy).
These institutions came together as a result of a funding
call from the Higher Education Authority under
the Programme of Research in Third Level Institutions
(PRTLI). HSIS was awarded €28,000,000 in August
2007 to build a joint national platform for the coordination
and dissemination of humanities research, teaching
and training at an all-island level. This is probably the
single largest award to the humanities in the world to
date. Of that €28,000,000, some €18,000-20,0000 is for
capital funding. A majority of the remainder, however,
is being invested in digital humanities projects, training, and development.
The DHO is the centrepiece of the HSIS collaborative.
It was founded to be a collaborator on national digital
humanities initiatives, a centre of excellence, forward
looking but cognisant of past humanities and digital humanities
practice, and positioned to become a player in
international initiatives.
These extremely ambitious goals are being carried out
against a backdrop of changing expectations in the roles
of digital humanities centres; changing expectations
about the resources we create and the nature and rewards
for those intellectual products; and a realization that their
long-term viability and reusability must be designed for
in the very earliest stages of project conception.
To fulfil these goals, the DHO is developing three distinct
but integrated technical infrastructures. The first of
these is a Portal, which is the public face of the DHO.
Built on the content management system, Drupal, it not
only provides information about the DHO, its activities
and its partners, but features community spaces allowing
the Irish academic community, as well as those interested
in the work produced by that community, to stay
connected.
The second deliverable is a database entitled Digital Research
and Projects in Ireland (DRAPIer). This projects
and methods database (modeled on the UK’s ICT Methods
Database), also implemented in Drupal, provides
a publicly-accessible framework for the discovery of
digital humanities project in Ireland. Administratively,
DRAPIer has equally important functionally. It allows
us to identify projects at risk and to intervene before the
content is lost to the academic community. It provides a
national snapshot of the depth and breath of digital humanities
research in Ireland, funding sources, and methods
utilized.
The third deliverable is an access and preservation repository
based on Fedora. This repository will provide
public access to digital humanities resources created by
the HSIS partners. This infrastructure, currently under
development, is possibly the most ambitious IT deliverable
of the initiative. Some of the resources created
by HSIS partners will reside directly in the DHO’s Fedora
instance, others will be federated in other Fedora
instances maintained by DHO project partners. The interoperability
of these frameworks is based on shared
content modeling within Fedora.
By creating resources in which the underlying data share
some level of interoperability, based on common content
modeling, shared ontologies, named authority lists, and
metadata standards, the DHO expects to provide a level
access to a variety of heterogeneous resources having
both an eye to the future in their long-term preservation,
as well as an eye to the past, in providing access a wealth
of Irish cultural heritage, and well as the research of Irish
scholars, to a wider audience.
Making primary resources of Irish studies, as well as
the research of Irish scholarship more widely available
is particularly important for disciplinary studies areas
typically labeled as ‘minority’. It can be extremely difficult
for postgraduate students outside of Ireland or the
handful of Irish studies centres outside Ireland to obtain
access to the materials they need, further discouraging
research in these areas. Digital publication has the ability
to change this, leveling the playing field between
area studies and more resourced areas such as British or
American Literature.
Our panel will consist of four presentations on the political,
cultural and technical aspects of the foundation and
work of the Digital Humanities Observatory. We envision
this presentation being of value to other countries
and regional areas wishing to implement a similar multiinstitutional
centre. Edmond is on the DHO Consultative
Committee and was one of the authors of the PRTLI
proposal. She will describe the needs behind the DHO
and the thoughts behind its initial foundations. Schreibman
will introduce the DHO as it is today, and describe
its place in the political and cultural landscape of Ireland
and introduce some of the projects that have benefited
from its support. Porter will discuss the development of
the controlled vocabulary that supports DRAPIer and the
standards that support the access and preservation repository.
Day will demonstrate the Portal and DRAPIer, and
will discuss how these deliverables in particular serve
the growing community of digital humanities scholars
in Ireland.
References
Digital Humanities Centers Summit: Notes from the
NEH-hosted summit meeting of digital humanities centers
and funders, April 12-13, 2007<https://apps.lis.uiuc.
edu/wiki/display/DHC/Digital+Humanities+Centers+S
ummit>Accessed 14 November 2008
centerNet <http://www.digitalhumanities.org/centernet/>
Arts and Humanities Data Service (AHDS) <http://
www.ahrc.ac.uk/News/Latest/Pages/AHRCreshapesitsfundingofICTresearch.
aspx> Project Bamboo <http://projectbamboo.uchicago.edu/>
Interedition: An Interoperable Supranational Infrastructure
for Digital Editions <http://www.interedition.eu/>
Council on Library and Information Resources, “No
Brief Candle: Reconceiving Research Libraries for the
21st Century,” August 2008 <http://www.clir.org/pubs/
reports/pub142/contents.html>
Scholarly Communication Institute 6: Humanities Research
Centers. University of Virginia, July 13-15, 2008.
<http://www.uvasci.org/wp-content/uploads/2008/09/
sci-6-report.pdf>
Digital Research Infrastructure for the Arts and Humanities
(DARIAH) <http://www.dariah.eu/>
Digital Humanities Observatory (DHO) <http://www.
dho.ie>
Humanities Serving Irish Society (HSIS) <http://www.
hsis.ie> ICT Methods Database <http://ahds.ac.uk/ictguides/>",txt,This text is republished here with permission from the original rights holder.,,,English,
995,2009 - UMD College Park,UMD College Park,,2009,ADHO,ADHO,"University of Maryland, College Park",College Park,Maryland,United States,http://web.archive.org/web/20130307234434/http://mith.umd.edu/dh09/,Supporting the Creation of Scholarly Bibliographies by Communities through Social Collaboration,,Hamed M. Alhoori;Omar Alvarez;Miguel Muñiz;Richard Furuta;Eduardo Urbina,paper,"Many digital humanities projects maintain online
bibliography digital libraries (BDLs) that support
diverse users in locating a variety of references. The Cervantes
Project (CP) bibliography aims to represent the
best resources about Cervantes published since 1605 and
is drawn from many multilingual sources. The current
CP bibliography gathering and filtering process is carried
out by sets of contributors: the expert editors, the
reviewers, and the authorized international collaborators.
Delays, possibly months, can result from the filtering
process and also from the process of uploading the
new publications into the BDL, which is separate from
the gathering and filtering process. In addition, the ability
to find new entries online is limited. Current bibliographic
search engines show a limited scope of coverage
on literature. There is no single resource that handles the
entire 2.5 million articles that emerge yearly from the
25,000 peer-reviewed journals (Harnad, S. et al., 2008),
so these engines access only a fraction of the literature
(Hull, D. et al., 2008).
We compared various humanities BDL’s main supported
features. Table 1 summarizes the main outcomes. Note that the majority of these BDLs do not take advantage of
the social collaboration of Web 2.0. This paper’s premise is that social collaboration with
the right level of moderation can support and reduce the
costs of creating a scholarly bibliography by benefiting
from the “wisdom of the crowds” (Surowiecki, J., 2004),
while ensuring the accuracy of the bibliography. This
could lead researchers to needed and interesting resources
in better time. We have experimented with this issue
by implementing a set of functionalities built on the drupal
CMS. We have tested them on a group of CP users
from different countries who use a variety of languages
to gather, share, annotate, rank and discover academic
literature (Fig. 1). We report on these initial experiments
in the remainder of this paper.
Personalization
Zotero, Mendeley and Papers are personal reference
management tools. However they do not include social
collaboration features. We implemented a personal facility
named MyCibo (Fig. 2), where users can save or
edit their references, personal pages, and blogs with the
ability to make them private or public. They can import
and export in EndNote tagged, XML, RIS, and BibTeX
formats and manually connect related publications. Social technologies applied to
bibliographies
Social Bookmarking
Most online libraries and bibliographies provide one way
learning, in that they provide services to the users, while
prohibiting users from contributing. This results in a
huge loss of knowledge and almost a freezing of storage
rather than active libraries. The current state of the art is
moving toward two way learning, where the users can
both benefit from the available knowledge and contribute
to it. (Hendry, D.G. et al. 2006b) mentioned an ‘amateur
bibliography’ that is collected by nonprofessionals
and falls short of the standards of a professional bibliography.
Although large amount of references could be
collected in a short span of time, resulting issues such as
redundancy, spam, phantom author names, and phantom
citations are not a good sign of scholarly research (Jacso,
P . 2008).
Unlike some general online reference management software
such as CiteULike and Connotea that are based on
the concept of non-moderated social bookmarking, we
considered the previous issues and the need for an accurate
bibliography. To get this done, light moderating
and authenticating of the users contributions to the CP bibliography is provided, aiming to reach the scholarly
moderated bibliography (Hendry, D.G. et al. 2006a).
Users were given ranks according to their scholarly or
contribution level. For example, well known scholars
got higher ranks so that they could contribute directly
without moderating their contributions. New users’ public
contributions will be entered into a queue waiting
for an approval from a moderator. Users who contribute
with relevant and accurate contributions would mean
that they are most likely experts in their area, and were
given points, which promote their ranking and editing
permissions. We believe this provides accuracy without
losing the benefits of collaboration. Fig. 3 shows how to
process the queued publications and Fig. 4 shows points
gained by an administrator after several entries. Editors
can revert to any previous revision in case there is need
(Fig. 5). We allowed the researchers to share and discover academic
literature without worrying about inaccurate bibliographic
data. They can discover what the warm topics
are in the research field and what is significant to other
researchers by viewing what other researchers read and
tag. Hence, they can know the related researchers with
similar interest that they can network with. Social collaboration
is also important for papers that are not available
electronically for various reasons and may loss their
presence in the research community.
Social Tagging
Del.icio.us and Digg are of the best and fastest growing
social bookmarking sites that use a folksonomy tagging.
However, inaccurate and misleading tags are common
in such open environments, which cannot be accepted
in research communities. This is a classic Web 2.0 problem:
it’s hard to aggregate the wisdom of the crowd without
aggregating their inexperience or madness as well
(Torkington, N. 2006).
We prevent these effects by using social tagging with
light moderation and users privileges upgrading. This
provides us with a better quality tags than we would get
if we just accepted all the beginners’ tags; these users
may want to contribute to the scholarly community initially
but may loss their interest later on. We allowed the
users to create their own tags and reuse the previously
entered tags by them or other users using the AJAX technology,
which allowed us to provide auto-complete tags
in real time.
Social review and comments
There are different types of comments: approving, disapproving,
or just summarizing the resource. We implemented
a feedback environment to build an active online
research community. It provides social reviews and comments
from the users where the authors can interact with
and answer their questions.
Multilanguage Capability
As digital libraries expand their audience and content
scope, there is an increasing need for resources and access
tools to those resources in a variety of languages
(Larson, R.R. et al., 2002). Even for polyglot users, there
is a preference to use maternal language interfaces in order
to accelerate searching and browsing process, preferring
the language of the interface to match the language
of the content as well (Keegan, T . and Cunningham, S.,
2008). Hence, the CP international scope requires the
inclusion of content and system functionalities in multiple
languages. Based on the statements presented, we
provided a translation capability for interface elements (localization) and for content (internationalization). We
analyzed different translation strategies such as using
Web content (Wang, J. et al., 2004), documents in multiple
languages (Nie, J.Y. et al., 1999), and some available
APIs. After testing common searching phrases and
sample texts in our content domain in three different
languages (English, Spanish, and Arabic), we decided
to use the Google AJAX Language API because of its
detection and translation capabilities.
Users can choose the preferred available language at any
moment while using the system. This choice will translate
the interface to that language and would select only
the content with that language. Bibliographic data can be
entered in a language and then translated to a new language
or linked to an existing bibliographic data or publications
in other languages (Fig. 6). Users’ comments
and annotations can be translated to other languages, allowing
users to comment and discuss in their preferred
language (Fig. 7). The testing outcomes showed us acceptable
translation results.
Ranking
Bibliography ranking has been used as a way to give users
a confident Top-N resource from the searching results.
A normal user only reads the first, second, or third
page of results. Citations and references have been used
as a way to rank bibliography resources (Larse, B. et al.,
2002, Larse, B. et al., 2006, Yang, K. et al. 2007). Citation-
based methods deal with complex issues such as biased
or self-citations, hard to detect positive or negative
citations, multiple citations formats difficult to handle by
computer programs, unfair consideration of new papers,
venues not considered. (Yan, Su, et al. 2007) propose a
seed-based measure (considering top-venues and venues’
authors relevance) and the browsing-based measure
(considers user’s behavior) to rank academic venues.
However, the authors-seed needs to be updated frequently
to reconsider new relevant authors. We used a hybrid
approach. We allowed the users with higher ranking to
rate the publications and retrieve the publications that
got a vast amount of approved reviews and comments
since that would mean that they are hot topics. Discussion and Future work
Our initial experimental results indicate that using an
online social collaboration would improve the quality,
quantity and usage of scholarly bibliography. Furthermore,
it would help in building bridges among the international
researchers and facilitate scholarly collaboration.
We intend to automate some portions of the moderating
process and evaluate the reviews and comments (positive
or negative) by identifying and interpreting annotations
patterns and semantic to give some relevance weight to
each source which would help also in the ranking.
Acknowledgements
This material is based upon work supported by the National
Science Foundation under Grant No. IIS-0534314.
References
CiteULike. Available at:
http://www.citeulike.org (Accessed October 2008).
Connotea. Available at: www.connotea.org (Accessed
October 2008).
Delicious. Available at: http://delicious.com/ (Accessed
August 2008).
Digg. Available at: http://digg.com/ (Accessed August
2008).
Drupal. Available at: http://drupal.org/ (Accessed April
2008).
Google AJAX Language API, Available at: http://code.
google.com/apis/ajaxlanguage/, (Accessed April 2008.)
Harnad, S. et al. (2008) The Access/Impact Problem and
the Green and Gold Roads to Open Access: An Update.
Serials review, 34 (1). pp. 36-40.
Hendry, D.G. et al. (2006a). Hotlist or Bibliography? A
Case of Genre on the Web, hicss,pp.51b, Proceedings
of the 39th Annual Hawaii International Conference on
System Sciences, p.51.2, January 04-07, 2006.
Hendry, D.G. et al. (2006b). Collaborative bibliography,
Information Processing and Management: an International
Journal, v.42 n.3, p.805-825, May 2006.
Hull, D. et al. (2008) Defrosting the Digital Library:
Bibliographic Tools for the Next Generation Web. PLoS
Comput Biol 4(10): e1000204. doi:10.1371/journal.
pcbi.1000204.
Jacso, P. (2008). Testing the Calculation of a Realistic hindex
in Google Scholar, Scopus, and Web of Science for
F. W. Lancaster. Library Trends 56.4 (2008): 784-815.
Project MUSE.
Keegan, T. and Cunningham, S. (2008). Language Preference
in a Bi-language Digital Library, Proceedings of
the 5th ACM/IEEE-CS joint conference on Digital libraries,
Denver Colorado, USA, 2005.
Larson, R.R. et al. (2002). Harvesting Translingual Vocabulary
Mappings for Multilingual Digital Libraries,
Proceedings of the 2nd ACM/IEEE-CS joint conference
on Digital libraries, Portland Oregon, USA, 2002.
Larse, B. et al. (2002). The Boomerang Effect: Retrieving
Scientific Documents via the Network of References
and Citations, Proceedings of the 25th annual international
ACM SIGIR conference on Research and development
in information retrieval, Tampere, Finland, 2002.
Larse, B. et al. (2006). Using Citations for Ranking in
Digital Libraries, Proceedings of the 6th ACM/IEEE-CS
joint conference on Digital libraries, Chapel Hill, NC,
USA, 2006.
Mendeley. Available at: http://www.mendeley.com/ (Accessed
October 2008).
Nie, J.Y. (1999). Cross-language Information Retrieval
Based on Parallel Texts and Automatic Mining of Parallel
Texts from the Web, Proceedings of the 22nd annual
international ACM SIGIR conference on Research and
development in information retrieval, Berkeley, California,
United States, Pages 74-81.1999. Papers. Available
at: http://mekentosj.com/papers/ (Accessed October
2008).
Surowiecki, J. (2004). The Wisdom of the Crowds: Why
the Many Are Smarter Than the Few and How Collective
Wisdom Shapes Business, Economies, Societies and Nations.
1st ed. New York: Doubleday.
Torkington, N. (2006). Digging the Madness of Crowds.
http://radar.oreilly.com/archives/2006/01/digging-themadness-
of-crowds.html. (Accessed April 2008).
Wang, J. et al. (2004). Translating Unknown Cross-
Lingual Queries in Digital Libraries Using a Web-based
Approach, Proceedings of the 4th ACM/IEEE-CS joint
conference on Digital libraries, Tucson, Arizona, USA;
2004. Yan, S. et al. (2007). Toward Alternative Measures for
Ranking Venues: A Case of Database Research Community,
Proceedings of the 7th ACM/IEEE-CS joint conference
on Digital libraries, Vancouver, BC, Canada, 2007.
Yang, K. et al. (2007). CiteSearch: Next-generation Citation
Analysis, Proceedings of the 7th ACM/IEEE-CS
joint conference on Digital libraries, Vancouver, British
Columbia, Canada, 2007.
ZOTERO. Available at: http://www.zotero.org/ (Accessed
October 2008).",txt,This text is republished here with permission from the original rights holder.,,,English,
1057,2009 - UMD College Park,UMD College Park,,2009,ADHO,ADHO,"University of Maryland, College Park",College Park,Maryland,United States,http://web.archive.org/web/20130307234434/http://mith.umd.edu/dh09/,Platform Models for Scholarly Journal Publishing: A Survey and Case Study,,Sarah Toton,paper,"The past six years have seen a proliferation of online
scholarly journals. The number of new publications
added to the Directory of Open Access Journals
(DOAJ), for example, has gone from a trickle of twenty-
six new journals in 2002 to a torrent of five hundred
ninety-eight new titles indexed in the first nine months of
2008. 1 This proliferation has led to publications focusing
on more specialized topics, and publishing in nations
and languages traditionally underserved by Western-focused
academic publishers. Despite the wide variety of
emerging content, however, the technical infrastructures
behind these publications remains relatively uniform.
Constrained by current platform options and multimedia
literacy on the part of publishers, many journals still operate
akin to their print-based predecessors. By offering
electronic text-driven arguments, online journals may
challenge conventional models of delivering scholarly
content, but do little to augment the emerging shape of
digital scholarship. In short, a downloadable PDF is not
that different than a scanned photocopy.
This paper examines technologies behind current openaccess
journals, and it evaluates how journal content,
preservation, information architecture, and the review
process influence and are influenced by available publishing
frameworks. To do this, I survey platforms used
currently in open-access scholarly publishing. I then focus
on a particular journal, Southern Spaces, and its transition
from hardcoded HTML to a publishing/content
management system. Through this discussion, I intend
to address several questions, including: Can journals impact/
build scholarly communities? Can they shape and
enhance a participatory culture on the scholarly level?
Are existing models of peer-review unnecessarily limited
by restrictive platforms? Is the text-based model
of scholarly communication good enough for today’s
scholar?
The paper’s survey portion examines existing models
used in electronic publishing from XML schemas (like
NCBI’s Journal Publishing Tag Set) to the many opensource
platforms developed at digital scholarship centers
and research libraries, including Open Journal Systems
(OJS) and Digital Publishing System (DPubS). The
Public Knowledge Project’s OJS platform facilitates the
development of open-access scholarship by not only offering
an infrastructure for the online presentation of articles,
but also providing a management system for peerreview
and general editorial workflow. A local install,
ease of configuration, and submission management tools
allow users to develop a technical infrastructure relatively
quickly and with little need for system administrator
support in the maintenance phase. The relatively uniform
look of OJS journals, however, suggests little capability
for extensive customization. Cornell and Penn State’s
DPubS software offers more opportunities for customization
through its modular architecture, as well as the
potential for interoperability with Fedora and DSpace
repositories. In addition, like OJS, DPubS 2.1 offers
a service for peer-review management.2 While DPubS
robust design allows for more unique journal instances,
however, it also requires significantly more back-end
management and support than OJS. This makes DPubS
useful on an institutional level in a library, but the steep
learning curve and technical requirements may hinder
adoption among university faculty. Other systems developed
in Europe—Hyperjournal, SOPS, the ePublishing
Toolkit—seek to provide personal archives, workflow
support, publishing networks, and cross-referencing
tools, but have not yet been adopted by more than a
handful of scholarly publishers.
In addition to offering a survey of platform-based publishing
options, this paper examines open-source blog
publishing tools and content management systems developed
for commercial purposes that are also used by
electronic scholarly publishers. Flow, a media studies
journal, and the CodeForLib Journal, for example, uses
the popular blogging platform, WordPress.3 Museum
Anthropology Review used WordPress in 2007 before
switching to OJS in 2008. The journal’s editors still
maintain the WordPress site as supplementary weblog to
MAR.4 While implemented less commonly, the content
management system, Joomla, provides the platform for
Boston College Law School’s Intellectual Property and
Technology Forum.5 These cases address why publishers
chose and sometimes abandoned non-academic platforms,
and they offer insight into the possibilities and
drawbacks in modifying commercial-based open-source
tools.
This comparative survey illustrates the varied strengths
and weaknesses of particular publishing platforms, as
well as examines their influence on the final product: the
published scholarship. I then turn to Southern Spaces
(www.southernspaces.org), an open-access, interdisciplinary
journal housed in Emory University’s Woodruff
Library. My experience as an editor, media coordinator and the Managing Editor has shown me how technological
decisions influence the daily operations of journal
publishing, as well as the final shape of scholarly content.
This year, in particular, marks a major transition
for the journal. Southern Spaces will undergo a substantial
redesign in the coming months, transitioning from
HTML pages created in Dreamweaver to a Drupal 6
platform integrated with a Fedora repository.
To determine the new information architecture for
Southern Spaces, I conducted two surveys: one to board
members and peer reviewers, the other to junior and independent
scholars new to Southern Spaces. Based on
user feedback as well as an internal we conceived a new
workflow process, navigation structures, pedagogical
tools, as well as several opportunities for community development
within the site. The redesign, I anticipate, will
allow Southern Spaces authors and staff to develop multimedia
publications that incorporate links, video, audio,
and Flash features. In addition, the process will develop
interactive features that not only allow for reader feedback,
but also provide a better infrastructure for citing
and collecting Southern Spaces articles. These directions
extend from Southern Spaces’s goals, which are likely
similar to many open-access journals: offer a research
tool, expand the visitor community, sustain and preserve
digital content, streamline the pre-publication process,
develop a learning environment for interdisciplinary
scholars as well as scholars new to digital scholarship.
Choosing a publishing platform not only dramatically
influences published content, but also has the potential
to change the landscape of online scholarly communication.
Who edits, reviews, publishes and hosts a journal
influences technological decisions, but these decisions
also dramatically impact what can and cannot be
displayed as vetted scholarship, as well as the limits of
scholarly participation in a journal’s (or a discipline’s)
wider community. In this rapidly expanding landscape
of online scholarship, technological change and the decisions
behind this change aren’t always apparent. Yet, in
order to understand how digital publishing is changing,
it is key to look beyond the number of online journals to
the variety of online publishing options and their impact
beyond the computer screen.
References
Borgman, C. (2007). Scholarship in the Digital Age: Information,
Infrastructure, and the Internet. Cambridge,
MA: The MIT Press.
Brown, A. (2002). XSD Schemas in Book and Journal
Publishing. XML Europe. http://www.idealliance.org/
papers/xmle02/dx_xmle02/papers/03-01-02/03-01-02.
html (accessed 11 November 2008).
Sparc Europe (2008). Open Access Journals: Overview.
http://www.sparceurope.org/resources/hot-topics/
open-journals (accessed 11 November 2008).
Solomon D. (2008). Developing Open Access Journals:
A Practical Guide. Oxford: Chandos Press.
Willinsky, J. (2005). Open Journal Systems: An Example
of Open Source Software for Journal Management
and Publishing. Library Hi-Tech 23: 504-519.
Notes
1Gavin Baker (2008). Growth of DOAJ: Steady 2003-
2007, Major Spike in 2008. A Journal of Insignificant Inquiry.
October 17, 2008. This study is somewhat limited
as the DOAJ offers only two pieces of chonodata: date
journal was started and date journal was added to the
DOAJ. This study indicates when journals were added
to the DOAJ, but not necessarily when became open access.
2This build was launched in June 2007, and it remains
the most recent version as of November 2008. Documentation
for 2.1 was last update in March 2008.
3The CodeforLib Journal (http://journal.code4lib.org/)
currently uses WordPress 2.6.3. Flow uses 2.5 (for their
second redesign, Flow used a WordPress skin called The
Morning After, which gave the publication a magazine
style; they currently use the MimboPro template, also
designed for magazine style sites.) In late 2008, a DOAJ
Export Plugin was developed for WordPress, allowing
publishers of OA journals to provide article-level data
to the DOAJ, thereby opening articles up to discovery
through DOAJ’s interface.
4The Museum Anthropology Weblog’s “About” page
explains: “While, during its first year (2007) the journal
itself was published on this site, the journal is now
published using Open Journal Systems by the Indiana
University Bloomington Libraries.” http://museumanthropology.
net/about/ (accessed 10 November 2008).
5Paul Ham (2006). Using Joomla for an Online Law
Journal. Intellectual Property and Technology Forum.
Boston College Law School. Boston, MA. July 9, 2006.
http://bciptf.org/blog/2006/07/09/using-joomla-foran-
online-law-journal/. (accessed 9 November 2008).
Several comments discuss other publishing platforms,
including citing OJS.",txt,This text is republished here with permission from the original rights holder.,,,English,
1185,2010 - King's College,King's College,"Cultural expression, old and new",2010,ADHO,ADHO,King's College London,London,England,United Kingdom,http://dh2010.cch.kcl.ac.uk/,The TEI's Extramural Journal Project: Exploring New Digital Environments and Defining a New Genre in Academic Publishing,,Stephanie A. Schlitz,paper,"
1
The TEI's Extramural
Journal Project: Exploring
New Digital Environments
and Defining a New Genre
in Academic Publishing
Schlitz, Stephanie A.
sschlitz@bloomu.edu
Bloomsburg University
The Text Encoding Initiative’s (TEI) Extramural
Journal (EJ) project was conceived early in
2009 when the conveners of the TEI Education
Special Interest Group (SIG) proposed, as a
matter of urgency, the development of an online
publishing suite to address the shortage of TEI
educational resources.
1
Following approval by
the TEI Board and Council and the receipt of a
small SIG grant in support of the project, TEI-
EJ advanced into development.
Because TEI-EJ is being researched and
developed in an era where “print is no
longer the exclusive or the normative
medium in which knowledge is produced
and/or disseminated” (“A Digital Humanities
Manifesto”) and where electronic publication
is increasingly common (see Waltham; Maron
and Smith; Willett), it is crucial to point out
that typologically, TEI-EJ is positioned outside
of two disparate points on the web publishing
continuum, media-driven journals which are
designed primarily for the publication of media-
driven content (e.g.
Vectors Journal
<
http:/
/www.vectorsjournal.org/
>,
Southern Spaces
<
http://www.southernspaces.org/
>; also see
Toton and Martin) and text-driven journals
which are designed to reproduce the print
journal model in a web publication (e.g.
Journal of Writing Research
<
http://www.jow
r.org
>,
International Journal of Teaching and
Learning in Higher Education
<
http://www.is
etl.org/ijtlhe/
>).
Although TEI-EJ’s ‘journal’ designation is
suggestive of a single aim, from the outset,
project objectives have been defined as both
experimental and extramural, and TEI-EJ has
been envisaged not only as a publishing venue
but also as a community-driven online forum
that offers members of the TEI, whether
novice or expert, as well as the broader DH
community new educational insights into the
TEI. Significantly, the steps being taken to
achieve these objectives contribute to a newly
emerging body of scholarship which explores
the development of new digital environments
and which defines a new genre in academic
publishing.
The first stage of the project has been the
development of TEI-EJ as a born digital,
open access, peer reviewed scholarly journal
where communicative modes are bidirectional
rather than exclusively unidirectional, articles
are media-driven (including video, audio, and
image) as well as text-driven, and where the
aims of publication extend beyond print journal
mimesis to include education and community
building.
Given the hybrid nature of the project’s goals
(publishing and learning community; see Dal
Fiore, Koku and Wellman) and the novel design
for implementation, TEI-EJ’s site infrastructure
(see
Fig. 1
) was designed to be extensible,
capable of managing text articles (e.g. TEI-
XML as well as formats such as .txt and .doc
which are converted to TEI-XML), multimedia
articles (e.g. video, audio, image), moderated
responses to articles, and community-driven
communications (e.g. forum and blog) (see
Fig. 2
). This is achieved through Drupal,
2
a
customizable, open source content management
system, which facilitates the social media as well
as the publishing and educational aspects of the
project.
3
Fig. 1.
Schlitz TEI-EJ project planning 'mindmap'

2
Fig. 2.
Screenshot of TEI-EJ website
This paper will introduce the TEI-EJ project,
describing the
why
and
how
of the key
theoretical, technological and editorial decisions
that drove development as we advanced
from theory into practice. In doing so, it
aims to establish the project as a new
model for academic publishing which is
designed to harness emerging technologies,
to leverage the fact that “Open access is
changing the public and scholarly presence of
the research article” (Willinsky), to promote
learning objectives beside dissemination of
scholarship, and to elevate the role of reader/
end-user to the position of chief stakeholder.
References
A Digital Humanities Manifesto.
15 Dec. 2008
23 July 2009
http://dev.cdh.ucla.edu/digital
humanities/2008/12/15/digital-humanities-m
anifesto/
.
Dal Fiore, Filippo
(2007). 'Communities
Versus Networks: The Implications on
Innovation and Social Change'.
American
Behavioral Scientist.
(50)7
: 857-866.
Koku, Emmanuel F., Wellman, Barry
(2002). 'Scholarly Networks as Learning
Communities: The Case of TechNet'.
Designing
Virtual Communities in the Service of Learning.
Sasha Barab, Rob Kling (eds.). Cambridge:
Cambridge University Press.
Maron, Nancy L., Kirby Smith, K.
(2009). 'Current Models of Digital Scholarly
Communication: Results of an Investigation
Conducted by Ithaka Strategic Services for the
Association of Research Libraries'.
The Journal
of Electronic Publishing.
12(1)
.
http://quod.l
ib.umich.edu/cgi/t/text/text- idx?c=jep;vie
w=text;rgn=main;idno=3336451.0012.105
.
TEI: Text Encoding Initiative.
http://www.tei-
c.org/index.xml
(accessed 7 July 2009).
Terras, Melissa, Van den Branden, Ron,
Vanhoutte, Edward
(2009). 'Teaching TEI:
The Need for TEI by Example'.
Literary and
Linguistic Computing.
24(3)
: 297-306.
Toton, Sarah, Martin, Stacey
(2009).
'Teaching and Learning from the U.S. South
in Global Contexts: A Case Study of Southern
Spaces and Southcomb'.
Digital Humanities
Quarterly.
3(2)
.
http://digitalhumanities.o
rg/dhq/vol/3/2/000047.html
.
Waltham, Mary
(12 Oct. 2009). 'The Future of
Scholarly Journals Publishing'.
http://www.nha
lliance.org/bm~doc/hssreport.pdf
.
Willett, Perry
(2004).
Electronic Texts:
Audiences and Purposes.
'A Companion
to Digital Humanities'. Schreibman, Susan ,
Siemens, Ray , Unsworth, John (eds.). Oxford:
Blackwell.
http://digitalhumanities.org/comp
anion/
.
Willinsky, J.
(25 Sept. 2009). '9 Flavors
of Open Access'.
E-MEDICINE.
49
(3)
.
http://cssp.us/pdf/9%20Flavors%20of%20
Open%20Access.pdf
.
Notes
1.
Two of the notable TEI teaching resources which are
available include the Women Writers Project’s NEH-
funded series of “Advanced Seminars on Scholarly Text
Encoding” (see <
http://www.wwp.brown.edu/encoding
/seminars/neh_advanced.html
>) and TEI by Example
(see <
http://www.kantl.be/ctb/project/2006/tei-
ex.htm
> and Terras et al.).
2.
For a good, comparative discussion of content management
systems, see “Comparing Open Source Content Management
Systems: WordPress, Joomla, Drupal, and Plone,” available at
<
http://idealware.org/comparing_os_cms/
>. TEI-
XML content is being handled by the Drupal XML Content
module, and the journal’s publishing workflow is being handled
by the Drupal E- Journal module.
3.
A preview of the publishing website and a fully functional mock
journal issue are being presented at the TEI Members’ Meeting
in November 2009.",txt,This text is republished here with permission from the original rights holder.,,,English,
1346,2011 - Stanford,Stanford,Big Tent Digital Humanities,2011,ADHO,ADHO,Stanford University,Stanford,California,United States,https://dh2011.stanford.edu/,Introduction to Text Analysis With Voyeur Tools,,Stéfan Sinclair;Geoffrey Rockwell,workshop / tutorial,"Introduction to Text Analysis With Voyeur Tools
Sinclair, Stéfan, McMaster University, sgsinclair@gmail.com
Rockwell, Geoffrey , University of Alberta, geoffrey.rockwell@ualberta.ca
Description
Are you interested in using computing methods to analyze electronic texts? Geoffrey Rockwell (University of Alberta) and Stéfan Sinclair (McMaster University) will run a hands-on workshop on using the web-based Voyeur Tools text analysis environment (voyeurtools.org). Participants can follow along with example documents or use their own. Voyeur Tools is the latest text analysis web-based system developed by TAPoR collaborators and it brings together visualization and concording tools in a fashion that allows multipanel interactive analysis or single tool analysis. Voyeur Tools runs on a high performance computing cluster and is capable of scaling to handle multiple documents and larger texts than previous web based tools.

The workshop will provide:

An introduction to basic text analysis concepts and techniques (independent of the tool set being used)
An introduction to different ways of using Voyeur Tools. Voyeur can be used in a multi-panel view where the different tools interact or as individual tools. Users will be shown different ways of running Voyeur Tools and how to manage panels.
Understanding the Voyeur display. Voyeur provides a number of different panels with information from a summary of the corpus to distribution graphs. Participants will be taken through the different panels and the capabilities of each one.
Using Voyeur Recipes for analysis. Participants will be introduced to the Voyeur Recipes, which are tutorials on how to use Voyeur for research tasks. We will start by looking at how Voyeur can be used to explore a theme through a text. We will then look at using Voyeur for diachronic study of a collection of documents over time.
Quoting Voyeur results. Users will be introduced to Voyeur's ability to produce HTML fragments that can be used to quote results in other online documents. With Voyeur you can export your results in various ways, one of which is placing live panels into blogs or wikis.
Integrating Voyeur into remote sites. We have developed specialized plugins that integrate with frameworks such as WordPress, Drupal and OJS. Participants will learn about these as well as how to integrate Voyeur into almost any site with a generic plugin module.
Instructors
Stéfan Sinclair is an Associate Professor of Multimedia. His areas of interest include computer-assisted literary text analysis, experimental visualization interfaces, and 20th Century French literature (especially Oulipo). He is the creator or co-developer of online Digital Humanities tools such as Voyeur Tools, the TAPoR Portal, the Humanities Visualization Project.

Geoffrey Rockwell is a Professor of Philosophy and Humanities Computing at the University of Alberta, Canada. He has published and presented papers in the area of philosophical dialogue, textual visualization and analysis, humanities computing, instructional technology, computer games and multimedia.",txt,This text is republished here with permission from the original rights holder.,,,English,
1443,2012 - Hamburg,Hamburg,"Digital Diversity: Cultures, languages and methods",2012,ADHO,ADHO,Universität Hamburg (University of Hamburg),Hamburg,,Germany,http://www.dh2012.uni-hamburg.de/,Digital Language Archives and Less-Networked Speaker Communities,,Lise M. Dobrin,paper,"During the past decade, digital language archives have flourished as the field of language documentation has entered mainstream linguistics. Major international funding bodies, both public and private, support research on endangered languages, and it is now possible to publish practical and theoretical work on endangered languages and documentary methods in dedicated journals and book series. One of the themes that has emerged most clearly in this literature is that of collaboration. It is now widely agreed that language documentation should be equally responsive to both the technical questions posed by linguists and the more immediate practical interests of speakers and their communities. Issues of rights and access are no longer anxious afterthoughts; they are fundamental matters for negotiation between researchers and speakers, mandatorily addressed in research agreements and funding proposals, and threaded through documentation projects from their very conception (see, e.g., Yamada 2007; Czaykowska-Higgins 2009).
Yet paradoxically, this intense focus on collaborative methods seems to stop where the digital archiving of language data begins. Language archives are doing their best to ensure that source communities can in principle gain access to the language materials they produce, but they are only just beginning to consider possibilities for formally integrating speaker communities into the process of archive curation. Such involvement could potentially transform language archives from repositories of static objects into sites for ongoing dialogue and exchange with living communities.
The obstacles to collaborative archiving are particularly acute in the case of source communities located in hard-to-reach places – often the third world – where many of today’s small, minor, and endangered languages are spoken. As might be expected, a disproportionate percentage of documentary projects are being carried out in such locations (see, e.g., http://www.paradisec.org.au/blog/2010/09/where-does-the-dosh-go), with their outputs being deposited into first world archives. It is imperative that western scholars and institutions be cognizant of the impact their documentation projects have on such communities. In Melanesia, which is arguably the world’s most linguistically diverse area, local value is validated through relationships with outsiders. If community input ceases once the fieldwork phase of a western-sponsored project is over, this can reinforce the feelings of marginalization that motivate language shift in the first place (Dobrin 2008).
In April 2012, the University of Virginia’s Institute for Advanced Technology in the Humanities (IATH) is hosting an international group of scholars, technical experts, and community members for an intensive two-day meeting, sponsored by the National Endowment for the Humanities, to explore appropriate social and technical models for facilitating the ongoing involvement of less-networked source communities in the digital archiving of their endangered languages. Participants will present on the current state of the art in language archiving, the cultural and infrastructural situations of representative world regions (Papua New Guinea and Cameroon), and promising ‘bridging’ technologies. This paper will report on the results of the meeting, describing their implications for language archiving and for the digital humanities more generally.       
There are a number of reasons why digital language archives must begin to find ways to support direct, ongoing relationships with speakers and community members, and not just with data depositors or researchers. One of these is the unfolding nature and experience-dependence of informed consent. When speakers are first recorded, they may consent to certain levels of access for the resulting materials (full access, access with conditions, researcher-only, etc.; see http://elar.soas.ac.uk/node/32620 andhttp://www.mpi.nl/DOBES/archive_access/access_procedures). But they cannot possibly foresee all potential future uses of the recordings. And where communities have little or no familiarity with the medium of distribution, how can they be expected to grasp even the most basic uses to which their material will be put? Given the opportunity to make their wishes known, judgments made by communities at a given time may be modified, refined, or even reversed later.
Developing direct lines of communication between archives and communities will also improve the quality and discoverability of archived data. Changing circumstances may make possible the engagement of knowledgeable stakeholders who were formerly reluctant or inaccessible. Also, though linguists may be diligent about collecting detailed metadata when making recordings in the field, this process inevitably leaves gaps. After fieldwork is over, missing information, such as the identity of a speaker or dialect, might only be known by difficult-to-reach community members. Whole categories of metadata that once seemed irrelevant (details of local history, relationships between consultants, and so on) often take on new significance as projects evolve. By enabling mechanisms by which community members can identify and attach metadata to recordings that concern them or their communities, endangered language resources will be enriched, making them easier to find and more useful for all users.
Finally, there is a practical need for the kind of direct relationships we are proposing to facilitate, as archives are now receiving expressions of interest from individuals who speak the languages of their collections. At the University of London’s School of Oriental and African Studies (SOAS), the Endangered Languages Archive (ELAR) has registered speakers of Bena and Pite Saami as users; IATH occasionally receives queries from town-dwelling Arapesh people interested in the Arapesh Grammar and Digital Language Archive (AGDLA). ELAR depositors sometimes specify that access to their deposit requires community approval; the Archive of the Indigenous Languages of Latin America (AILLA) sets up ‘community controlled’ as a systematic level of access (though at present this must be mediated through the institution). Increasingly, we can expect archives to be routinely receiving user account applications from individuals who were either directly involved as research participants or who have ancestors, family members, friends, or community associates who were involved in language documentation projects. But at present there is a clear disconnect between what depositors are able to achieve using the tools and systems provided by archives, and what community members are able to do – especially those with limited internet access.   
Some digital language archives recognize these problems and are seeking to overcome them. Edward Garrett of ELAR has been experimenting with social networking technologies such as the open-source Drupal Content Management System in order to make their archive more user-centered; a desire to extend the ELAR system to include community members is one of the motivations for our meeting. The project BOLD-PNG (seehttp://www.boldpng.info/home) has begun putting digital recording equipment into the hands of community members in Papua New Guinea and training them in techniques of basic oral language documentation, giving people the resources to become data collectors, if not depositors. Kimberly Christen’s Mukurtu platform was designed to facilitate community-controlled archiving, digitally instituting traditional cultural protocols to manage access to archived objects (see, e.g., Christen 2008). But none of these projects develops a generalizable approach to integrating communities without internet access in the ongoing curation of digital language materials they have produced.
We are particularly interested in the possibilities afforded by the increasingly common presence of mobile communication technologies in areas where basic infrastructure such as electricity, running water, and even roads remain absent. In such areas, new social institutions involving cell phones have begun to evolve, for example, conventions for exchanging cell-phone minutes, or new gathering spaces defined by signal access. Exploiting this new form of technology, the non-profit organization Open Mind has developed a system called ‘Question Box’ (see http://questionbox.org/about-mission) which allows remote communities to access information in critical domains such as health, agriculture, and business.  Google’s voice-based social media platform SayNow is being used to allow cell phone users in Egypt and elsewhere to leave voicemail messages that appear online immediately as Twitter audio feeds (http://www.nytimes.com/2011/02/02/world/middleeast/02twitter.html). We believe that similar methods are worth exploring as a means to creatively connect less-networked language communities with researchers and archives.
references
Christen, K. (2008). Archival Challenges and Digital Solutions in Aboriginal Australia. SAA Archeological Recorder 8(2): 21-24.
Czaykowska-Higgins, E. (2009). Research Models, Community Engagement, and Linguistic Fieldwork: Reflections on Working within Canadian Indigenous Communities. Language Documentation and Conservation3(1): 15-50.
Dobrin, L. M. (2008). From Linguistic Elicitation to Eliciting the Linguist. Language 84: 300-324.
Yamada, R.-M. (2007). Collaborative linguistic fieldwork: Practical application of the empowerment model. Language documentation and conservation 1(2): 257-282.",txt,This text is republished here with permission from the original rights holder.,,,English,
1506,2012 - Hamburg,Hamburg,"Digital Diversity: Cultures, languages and methods",2012,ADHO,ADHO,Universität Hamburg (University of Hamburg),Hamburg,,Germany,http://www.dh2012.uni-hamburg.de/,The Digital Mellini Project: Exploring New Tools & Methods for Art-historical Research & Publication,,Nuria Rodríguez Ortega;Murtha Baca;Francesa Albrezzi;Rachel Longaker,paper,"The Digital Mellini Project: Exploring New Tools & Methods for Art-historical Research & Publication
Print Friendly
XML
Rodríguez, Nuria, University of Málaga, Spain, nro@uma.es
Baca, Murtha, Getty Research Institute, USA, mbaca@getty.edu
Albrezzi, Francesca, Getty Research Institute, USA, falbrezzi@getty.edu
Longaker, Rachel, Getty Research Institute, USA, rlongaker@getty.edu
background
This paper will present the project Digital Mellini: Exploring New Tools & Methods for Art-historical Research & Publication, a joint initiative of the Getty Research Institute (Los Angeles) and the University of Málaga (Spain). This paper shall discuss several aspects and challenges related to the development of digital resources for art-historical research.

The main objectives for the Digital Mellini Project are:

To explore new methods and tools with which to reinvent the concept of scholarly work and critical publishing in the field of humanities, particularly in the context of art history, in which the convergence of text and image is essential and provides an interesting context for research – a context that has not yet been adequately investigated.1
To create a methodological model for developing collaborative digital publications that incorporate texts, digital facsimiles, images, computational tools for linguistic analysis and visual communication, and forums for exchanging ideas and sharing knowledge. The ultimate goal is that the international community of specialists can utilize this methodological model and apply to a variety of art-historical projects.
These objectives can be seen within the framework of the recent growing interest that many of us in the art-historical community have in response to the challenges offered by the digital society – we need to analyze and reflect critically on the ‘digital status’ of the discipline of art history from both the methodological and epistemological perspectives. We must be willing to open innovative and creative paths in the design of digital resources for art-historical research – paths that will lead to new lines of investigation and new methods of and attitudes toward information-sharing.

Art historians must participate in the development of tools in the Digital Humanities. In an article published in the Chronicle of Higher Education in 2009,

Johanna Drucker pointed out that many of the critical debates that have characterized the development of the humanities in recent decades – subjectivity of interpretation, multicultural perspectives, a recognition of the social nature of the production of knowledge, etc. – are absent in the design of digital resources. Instead, more emphasis has been placed on adapting to the characteristics of digital media than to the epistemological and interpretive exigencies of the various humanistic disciplines.2 Lev Manovich made similar observations in a Digital Humanities conference held in June 2009 in Maryland, when he asked, ‘Is it sufficient to borrow techniques from the fields of computer science, information visualization and media art – or do we need to develop new techniques specific to the humanities?’ (Manovich 2009: xv).

Regarding the hope that Digital Humanities will assume this new role, we perceive the need to build environments that are intellectually productive and will facilitate interpretive and critical studies above and beyond simple information systems designed for the storage and retrieval of data.

The Digital Mellini Project is a response to these challenges, as the aim of the project is to contribute to the development of virtual research environments (VRE) to promote critical research in the field of art history.

The Digital Mellini Project includes another desideratum that has emerged in recent years: the production of knowledge based on aggregation and collaboration. The culture of open, shared knowledge, which was already being promulgated by UNESCO as early as 2005 as a fundamental way to transition from the information society to the knowledge society, along with the possibilities offered by Web 2.0 technologies, is an essential factor within the evolution of digital scholarship.

There are already several projects oriented toward the development of collaborative environments (NINES3, The VRE-SDM Project4, The Transcribe Bentham Project5, etc.); and there is a burgeoning bibliography on this topic (Deegan & McCarty 2011). Proof of this interest has also been shown by the high percentage of presentations that dealt with this problem in the last Digital Humanities conference, held at Stanford University in June 2011.

Electronic publications and digital editions are not new in the realm of Digital Humanities; the first reflections on what an electronic edition might be appeared in Finneran (1996) and Bornstein and Tinkle (1998). There are also reflections and proposals relating to collaborative editions of unpublished manuscripts in which a dispersed community of scholars could add annotations, as in the case of the Peirce Project (Neuman et al. 1992) and the Codex Leningradensis (Leningrad Codex Markup Project 2000).

Nevertheless, as Susan Hockey (2004) states in her summary of the history of Digital Humanities, ‘The technical aspects of this are fairly clear. Perhaps less clear is the management of the project, who controls or vets the annotations, and how it might all be maintained for the future’.

Similarly, the Digital Mellini Project intends to continue this line of research, addressing the questions that still remain open and launching new ones, cultivating debate and reflection.

digital mellini project: development and intellectual questions
The Digital Mellini Project focuses on a particular text, an unpublished manuscript written in 1681 by Pietro Mellini, a Roman nobleman. This text belongs to the Getty Research Library, Special Collections, found under the title Relatione migliori di Casa delle pittura Melini. Pietro Mellini poetically described the best works as he inventoried the collection of his family’s paintings. This text carries a self-interest for art historical research due to its uniqueness – it is the only document known to date that has a hybrid composition of ekphrastic description in poetic form with a factual descriptive inventory. In addition, it is an important source for studies related to collecting, the market valuation of art, the provenance of collections, etc. Most of the works are unidentified and to analyze their possible allocations through the descriptions provided by Pietro Mellini is a very attractive task for art historians. Lastly, this text gives us an opportunity to explore the possibilities that digital media offers to deepen the relationships between text and image. One of the objectives of the critical edition is to establish hypotheses about the identity of the works described within the manuscript.

To carry out this critical collaborative edition, the Digital Mellini Project is developing a computational prototype. Exploring the possibilities of Drupal, a digital environment is being designed and conceived specifically to this analytic objective. Currently, the project team has completed an alpha version of the digital workspace. The first prototype of a beta version is expected to be finished by the end of 2012.

This prototype was constructed around the object of the critical edition, Mellini’s text, and it includes:

text itself – A diplomatic transcription encoded in XML-TEI, translations into English and Spanish, and a digital facsimile – provided in an interface that allows comparative analysis [Fig. 1];

Figure 1

A representative image gallery, consisting of images of works proposed by experts as possible identifications of the paintings described by Mellini. Additionally, provisions within the environment make it possible to establish comparisons between images and with the linguistic descriptions, so scholars can deepen the analysis of the word-image relationship;
Tools to complement or extend into other areas of research. For example, a repository of thematic texts and/or those structurally related to the Mellini’s text.
Tools for aggregating critical annotations to the text. These annotations can be written by each participating scholar, according to a methodology of action and interaction developed for this purpose. Also, these annotations can be arranged as a response or reply to other entries, thus generating a critical debate among the community of scholars participating [Fig. 2].
Space for debate and reflection on issues related to the text where it can be more broadly discussed.

Figure 2

Note, therefore, that what is proposed as a digital critical edition is far from the conventional academic model of a print publication, which would exist in a static format as the end result of an interpretive study. The result may have been made ​​by one or several specialists, but in the end, what we find is the proposal of one particular interpretation. It is also far from the digital editions that fit this model (eg Van Gogh Letters Project) or electronic repositories focused mainly on the analysis and exploration of texts (The Orlando Project-History of British Women’s Writing). The proposed model is multifaceted. Digital Mellini simultaneously integrates multiple and sometimes contradictory and paradoxical views. It is open and dynamic. Due to processing system records and the dialogue between specialists in the critical annotation process, we can reconstruct the interpretive process itself, as aggregation and contributions grow over time. Therefore, the digital edition is not only the end product of a study and/or research, but also the intellectual and interpretive process through which it unfolds.

In the digital edition, in addition to showing this prototype and its performance, we will address in more detail the intellectual and methodological questions that confront us while developing a resource of this kind, taking into consideration the goal of advancing digital scholarship in the field of art history and the Humanities in general. These topics include:

How to show different viewpoints contributed by specialists, and displaying through visual means the diversity of interpretations and perspectives that converge simultaneously on a text from different contexts – geographical, ideological, theoretical, methodology, etc.?
How to deal with the problem of linguistic variability in a multilingual community of specialists without circumscribing to the most prevalent language?
How to face the problem of authorship? How to maintain the legitimate intellectual property rights of each participant in a publication that emerges from the aggregation of many?
What are the real behaviors and research practices of art historians using such environments? Do these practices lead to the emergence of a new art-historical knowledge of a digital nature or not?
How to develop specific methodologies and protocols for action to guide, assist, and coordinate the participation and interaction of specialists in this type of environment?
How to encourage the participation of specialists in the context of an academic system that does not ‘recognize’ the work developed in these digital environments? For example, what kind of scientific production – monographs, articles, papers – are contributions that fit within critical annotations to a digital edition when referenced in an academic CV? Should we think, then, of the incorporation of new categories of scientific production in line with the specificities of digital media?
Finally, the conclusive question: how can the discipline of art history move away from single authorial models toward more open, collaborative models of research and publication?

references
Bornstein, G., and T. Tinkle (1998). The Iconic Page in Manuscript, Print, and Digital Culture. Ann Arbor: U of Michigan P.

Deegan, M., and W. McCarty (2011). Collaborative Research in the Digital Humanities. Surrey: Ashgate.

Drucker, J. (2009). Blind Spots. Humanist must plan their digital future. The Chronicle Review 55, is. 30.  http://chronicle.com/free/v55/i30/30b00601.htm (acessed 15 October 2011).

Finneran, R. J. (1996). The Literary Text in the Digital Age. Ann Arbor: U of Michigan P.

Hockey, S. (2004). The History of Humanities Computing. In S. Schreibman, R. Siemens, and J. Unsworth, eds (2004), A Companion to Digital Humanities. Oxford: Blackwell.  http://www.digitalhumanities.org/companion/ (accessed 15 October 2011).

Leningrad Codex Markup Project (2000). Project ‘EL’: The XML Leningrad Codex. http://www.leningradensis.org (accessed 15 October 2011).

Manovich, L. (2009). Activating The Archive, or: Data Dandy Meets Data Mining. In Digital Humanities 2009. Conference abstract. Maryland: Maryland Institute for Technology in the Humanities, p. xv.

Neuman, M., M. Keeler, C. Kloesel, J. Ransdell,  and A. Renear (1992). The Pilot Project of the Electronic Peirce Consortium. In ALLC-ACH92 Conference Abstracts and Program. Oxford, pp. 25-27.

Rodríguez Ortega, N. (2010). Digital Resources for Art-Historical Research: Critical Approach. In Digital Humanities Annual Conference 2010. Books of Abstracts. London: Office for Humanities Communication – Centre for Computing in the Humanities, King’s College, pp. 199-201.

Rodríguez Ortega, N. (2010). La cultura histórico-artística y la Historia del Arte en la sociedad digital. Una reflexión crítica sobre los modos de hacer Historia del Arte en un nuevo contexto. Museo y Territorio 2-3: 9-26.

Zweig, R. W. (1998). Lessons from the Palestine Post Project. Literary and Linguistic Computing 13: 89–97.

UNESCO (2005). Hacia las sociedades del conocimiento. París: Ediciones UNESCO.

notes
1.Nevertheless, interesting research has been carried out on linking images to text. See Zweig 1998

2.‘Many humanities principles developed in hard-fought critical battles of the last decades are absent in the design of digital contexts (…) For too long, the digital humanities, the advanced research arm of humanistic scholarly dialogue with computational methods, has taken its rules and cues from digital exigencies’ (Drucker 2009).

3.http://www.nines.org/ (accessed 15 October 2011).

4.Virtual workspace for the study of Ancient Documents, University of Oxford. http://bvreh.humanities.ox.ac.uk/VRE-SDM (accessed 15 October 2011).

5.http://www.ucl.ac.uk/transcribe-bentham/ (accessed 15 October 2011)",txt,This text is republished here with permission from the original rights holder.,,,English,
1539,2012 - Hamburg,Hamburg,"Digital Diversity: Cultures, languages and methods",2012,ADHO,ADHO,Universität Hamburg (University of Hamburg),Hamburg,,Germany,http://www.dh2012.uni-hamburg.de/,"Building a TEI Archiving, Publishing, and Access Service: The TAPAS Project",,Julia Flanders;Scott Hamlin;Rafael Alvarado;Elli Mylonas,poster / demo / art installation,"Building a TEI Archiving, Publishing, and Access Service: The TAPAS Project
Print Friendly
 

XML
Flanders, Julia, Center for Digital Scholarship, Brown University, USA, Julia_Flanders@brown.edu
Hamlin, Scott, Library and Information Services, Wheaton College, USA, hamlin_scott@wheatoncollege.edu
Alvarado, Rafael, Humanities and Arts Network of Technological Initiatives, University of Virginia, USA, ontoligent@gmail.com
Mylonas, Elli, Center for Digital Scholarship, Brown University, USA, Elli_Mylonas@brown.edu
As the language of DH 2011’s ‘big tent’ suggests, in recent years the profile of digital humanities work has expanded to include many scholars and practitioners who draw on a multitude of digital technologies in research and educational contexts, without assuming all the roles required to implement these technologies. This new generation (though some are ‘new’ only to digital matters) of digital humanists are undertaking intellectually ambitious work with digital methods and tools, but their interest does not necessarily arise from a strong institutional history or infrastructure, or from personal expertise with digital methods. Rather, they are practicing scholars who are increasingly aware of the shifting stakes of technology for the humanities, and who want to explore what may be possible by working in a new way. As a result, their ambitions often outstrip what their own institutions can support: the available infrastructure of digital publishing, archiving, data curation, and repository services may be limited or absent. An individual scholar can gain expertise and achieve interesting results using the TEI Guidelines (http://www.tei-c.org) or GIS, but it is a slower and more challenging process for a university to develop the institutional infrastructure to support that expertise, in the way that traditional libraries (for instance) support traditional forms of humanities scholarship.

The TEI Archiving, Publication, and Access Service (TAPAS, http://www.tapasproject.org) is aimed at addressing this gap, by providing repository and publication services for small TEI projects. TAPAS began with a planning grant from the IMLS (TAPAS 2010), originally proposed by a group of small liberal-arts institutions including Wheaton College, Willamette University, Hamilton College, Vassar College, Mount Holyoke College, and the University of Puget Sound, and later joined by Brown University and the University of Virginia. This planning group conducted an intensive study of the profile of needs, and developed a specification for the TAPAS service. TAPAS is now operating under a two-year IMLS National Leadership Grant to Wheaton College and Brown University which funds the development of the service. TAPAS has also received an NEH Digital Humanities Startup Grant, led by Wheaton College and the University of Virginia, which funds the development of the user interface. Hosted at Brown University, the TAPAS service will provide repository storage, data curation, and simple interfaces for data management and publication. It will also provide an API through which the TEI data can be accessed and remixed. The service thus aims to fill a crucial niche, enabling both a new type of publication and a new model for how scholarly publication is supported. All of these needs are particularly urgent in the liberal-arts community that is the central focus of TAPAS, but they are also strongly evident in the humanities academy more broadly, at a national and international level.

This project takes place within a landscape already well populated with large-scale infrastructural projects (Hedges 2009), such as TextGrid (http://www.textgrid.de/), DARIAH (http://www.dariah.eu/), CLARIN (http://www.clarin.eu/external/), and the Canadian Writers Research Collaboratory (CWRC, http://www.cwrc.ca/). Projects of this kind must all confront a central set of strategic concerns and design challenges, including questions about how much uniformity to impose upon the data, how to accommodate variation, how to create interoperability layers and tools that can operate meaningfully across multiple data sets (DARIAH 2011a, DARIAH 2011b), and how to manage issues of sustainability (of both the data and the service itself). TAPAS is distinctive within this landscape because of its focus on a single form of data (TEI-encoded research materials) and also because of its initial emphasis on serving an underserved constituency (scholars at smaller or under-resourced institutions) rather than on providing an infrastructure that can operate comprehensively. TAPAS is thus able to tackle the questions above in a highly focused way.

The proposed poster will focus on several key areas of the TAPAS project that will be the focus of our attention in the early phases of the project:

Architecture and system design. The TAPAS service is built on a Fedora repository, and the user interaction will be managed as a set of modular layers using tools like Drupal. The design of these layers needs to take into account information about how scholars need to interact with the service for activities such as:
creating new project records
uploading new data files, uploading revised versions of existing data files
creating metadata for data files, updating the metadata for existing files
configuring options for dissemination, publication, and other modes of access and discovery, such as interface choices, stylesheets, and information to be exposed via APIs.
The poster will provide a detailed look at the internal architecture and the ways that standards like RDF and METS are used to organize information and enable flexible deployment of repository data.

TEI schema development and the challenge of eclecticism. TAPAS plans to accept a broad range of TEI data, but will also need to identify different classes of data that share specific properties, such as genre or the presence of certain encoding features, to determine what kinds of interface tools will or will not be appropriate for a given data set. TAPAS will also use various forms of validation to help TAPAS contributors ensure the consistency and quality of their data prior to upload. The design and use of schemas used within the TAPAS ecology – extending from training, through data creation and management, to long-term data curation  is complex and will be an important focus of the project’s research. The poster will provide a detailed view of the different roles that schemas of various kinds will play in this ecology, and the principles guiding their design and use.
Designing a hosted service. Although TAPAS was prompted by the needs of individual scholars, its implementation as a hosted service means that it also plays an important aggregative role. The TAPAS collection of TEI data has the potential not only to serve as an important corpus of TEI data (of value, for instance, to those interested in the historiography of digital humanities, or in studying how the TEI is used) but also to provide important inter-project connections that may benefit the individual TAPAS contributors and their readers. In addition, designing TAPAS as a hosted service raises a number of issues concerning long-term data curation, rights, and the fiscal sustainability of the service itself. The poster will examine these issues with a particular focus on:
the membership and sustainability model for the service
the handling of intellectual property rights
the design of cross-project tools for searching, exploration, and visualization
User interface. Because TAPAS is intended to support scholars who – although they may be expert users of TEI – are not necessarily experts in working with repositories and XML publication, the design of the user interface will be critical in making the TAPAS service approachable. In addition, because some users may be managing very large numbers of files, the user interface will need to provide productive, intuitive ways of visualizing one’s data from a management standpoint as well as a publication standpoint.
funding
This project is made possible by a grant from the U.S. Institute of Museum and Library Services.

references
DARIAH (2011a). Technical Work: Conceptual Modelling. DARIAH Work Package 8. http://www.dariah.eu/index.php?option=com_content&view=article&id=31&Itemid=35.

DARIAH (2011b). Technical work: Technical reference architecture. DARIAH Work Package 7. http://www.dariah.eu/index.php?option=com_content&view=article&id=30&Itemid=34.

TAPAS (2010). Roadmap. http://www.tapasproject.org/roadmap

TextGrid (2010). Roadmap Integration Grid/Repository. TextGrid, September 2010. http://www.textgrid.de/fileadmin/TextGrid/reports/TextGrid_R121_v1.0.pdf.

Hedges, M. (2009). Grid-enabling Humanities Datasets. Digital Humanities Quarterly 3(2). http://www.digitalhumanities.org/dhq/vol/3/4/000078/000078.html",txt,This text is republished here with permission from the original rights holder.,,,English,
1574,2012 - Hamburg,Hamburg,"Digital Diversity: Cultures, languages and methods",2012,ADHO,ADHO,Universität Hamburg (University of Hamburg),Hamburg,,Germany,http://www.dh2012.uni-hamburg.de/,The TEICHI Framework: Bringing TEI Lite to Drupal,,Christof Schöch;Stefan Achler,poster / demo / art installation,"The TEICHI Framework: Bringing TEI Lite to Drupal
Print Friendly
XML
Schöch, Christof, University of Würzburg, Germany, christof.schoech@uni-wuerzburg.de
Achler, Stefan, University of Kassel, Germany, stefan.achler@gmx.de
This contribution presents the TEICHI Framework, a light-weight set of modules for the Drupal content management system serving to display, search and download electronic documents encoded according to the Text Encoding Initiative’s Guidelines for TEI Lite (Burnard & Sperberg-McQueen 2006). This publishing framework was developed jointly by humanities scholars at the University of Würzburg and computer scientists at the University of Kassel in Germany. To showcase the various features and concrete usability of the framework, we present a poster as well as a tool demo.

In comparison to other available solutions for the publication of electronic scholarly editions, such as the combination of an eXist database with a Fedora Commons repository or the Scalable Architecture for Digital Editions (SADE), the focus of the TEICHI Framework is clearly to be a lightweight, low-resource, and easy-to-use solution. Instead of competing directly with more comprehensive but also more demanding solutions, the TEICHI Framework focuses on low technical barriers and optimal usability (see Pape, Schöch & Wegner 2012). The overall feature set of the TEICHI Framework focuses on the requirements of humanities scholars working on straightforward, small-scale digital edition projects and the needs of teachers of electronic textual editing. It allows both these target groups to set up a complete web-based digital scholarly edition with minimal resources and technical support and see the results of their or their students’ work online in a timely manner.

Currently, the TEICHI framework consists of four modules, one each for displaying documents, for searching them, for downloading them, and for browsing associated digital facsimiles. The modular approach means that not all of the modules need to be installed for the others to function, and additional modules may be added in the future. In addition, any digital edition published with the TEICHI Framework can take advantage of the full website infrastructure and community support Drupal provides, including useful features such as revision history for each page or fine-grained users’ rights management and optional extensions such as blog or forum modules. 

At the core of the framework is the TEI Content module, which lets editors upload TEI documents via the Drupal GUI, order them in a hierarchical structure, and modify them online. The module stores XML/TEI documents in the Drupal database and displays them through an XSL transformation and a CSS stylesheet using Drupal’s content filter mechanism. The default stylesheet included with the module provides specific support for many of the features that are part of the TEI Lite (P5) set of elements: author and editor notes are displayed in the right sidebar and scribal corrections and editorial emendations are catered for. A toggle mechanism allows for the display of two alternative readings via the ‘choice’ mechanism. There is currently no support for the use of encoded names, dates and places in an index.

There are several additional modules for the framework. The TEI Download module allows downloading various renderings of TEI-encoded documents. The editor/administrator can choose which of the downloading options should be available to the user for each document: EPUB files for ebook readers, plain text files for use with basic analysis tools, and XML/TEI files for more advanced analysis. There are various additional user-side options for downloading: either one of the alternative readings can be chosen, editor notes can be included or excluded, and several pages can be downloaded as one large file or as separate files. The TEI Image Viewer module allows to associate the displayed texts with their corresponding digital facsimiles. This module is based on Seadragon Ajax, a JavaScript library for interactively viewing high resolution images. It lets the user call up any given digital facsimile, scroll through the available facsimiles, and zoom in and move around in each of them. The TEI Search module, which is still experimental, adds some basic TEI-aware search functions to the framework, allowing the users of the edition to limit their search in ways, for instance, to exclude results from editor notes, to only include results from quotes, or to restrict the search to one of two alternative transcriptions distinguished by the ‘choice’ mechanism. A basic TEI Editor module is in development.

All modules are fully integrated into Drupal’s administrative graphical user interface, allowing the editor or administrator to adjust various settings online. For instance, many settings of the TEI Content module, such as color schemes and button texts can be modified with a few clicks; also, the TEI Download module lets the administrator select metadata fields and a cover image for the EPUB files made available for download; finally, the TEI Image Viewer’s control panel is defined as a block which can be moved to any block region on the website.

The poster will indicate the features each of the modules provides to the overall publishing framework and visualize how each of them is embedded into the Drupal environment from a technical point of view. The tool demo will showcase both the frontend and the backend of the framework: the ways in which readers/users can interact with an electronic scholarly edition supported by the framework will be shown based on one project using the TEICHI framework, the digital edition of Bérardier de Bataut’s Essai sur le récit (Bérardier de Bataut 2010). A local installation will be able to show how easy it is to perform some of the typical tasks of editors/administrators of an electronic edition using the TEICHI framework, such as installing the modules in a Drupal environment, adding, organizing and modifying XML/TEI files, changing some basic settings and associating digital facsimiles to the transcriptions.

references
Bérardier de Bataut, F.-J. (2010). Essai sur le récit, ou entretiens sur la manière de raconter [1776], digital edition ed. by Ch. Schöch http://www.berardier.org.

Burnard, L., and M. Sperberg-McQueen, eds. (2006). TEI Lite: Encoding for Interchange. Text Encoding Initiative http://www.tei-c.org/release/doc/tei-p5-exemplars/html/teilite.

Pape, S., Ch. Schöch, and L. Wegner (2012). TEICHI and the Tools Paradox. Journal of the Text Encoding Initiative 2  http://jtei.revues.org/432; DOI: 10.4000/jtei.432.

Software

Drupal. http://www.drupal.org.

eXist-db. http://exist.sourceforge.net.

Fedora Commons Repository. http://www.fedora-commons.org.

Scalable Architecture for Digital Editions (SADE). http://www.bbaw.de/telota/projekte/digitale-editionen/sade/sade-1.

The TEICHI Framework. http://www.teichi.org.

Seadragon Ajax. http://gallery.expression.microsoft.com/SeadragonAjax.",txt,This text is republished here with permission from the original rights holder.,,,English,
1591,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,The AIDS Quilt Touch Mobile Web App,,Mark NeuCollins;Kelly J. Thompson;Nikki J. Dudley;Lauren Haldeman;Jon Winet;Kayla Haar,poster / demo / art installation,"Q: ""What weighs 54 tons and can be held in the palm of your hand?""
A: ""The AIDS Memorial Quilt.""

AIDS Quilt Mobile Web App Development Team:

Mark NeuCollins (Lead Developer, Database, PHP, Drupal, CSS)
Lauren Haldeman (Drupal, CSS, jQuery Mobile)
Nikki Dudley (PHP, Drupal, Database, Import Scripting)
Kelly Thompson (Drupal, Mapping, Experimental Module Research, User Experience)
Kayla Haar (Graphic Design)
Jon Winet (Project Director, NAMES Foundation Liaison, Editor, Publicity Information)
The AIDS Memorial Quilt is the largest living monument in the world. Composed of over 48,000 three foot by six foot individual panels, it pays tribute to the lives of more than 98,000 individuals who have died during the AIDS pandemic. Maintained by the NAMES Foundation in Atlanta, each panel is painstakingly hand-crafted by those who knew and loved these individuals. Each panel carries the emotional weight of a life lived, of loving relationships, and of heartfelt loss.

Our interdisciplinary team —drawn from backgrounds in information technology, the humanities, and public art, and including undergraduate and graduate students, staff, and faculty — would like to share our experiences building a public digital humanities project that provides access to a virtual experience of the Quilt. Provided with data stored in a structure created in the 1980s, we were tasked with transforming this arcane information repository into something robust, agile, and modern. We aimed to design a system that would be usable by the community surrounding the quilt, comprised of people from all walks of life. The process of creating this public digital humanities project, with its goal of preserving the culture and purpose of the original Quilt, presented many learning opportunities we believe could be of value to others seeking to undertake similar projects. If accepted, we propose a discussion around the cultural, technological, artistic, and community-driven aspects of developing this project. We would also like to discuss the outpouring of community-sourced stories, memorials, and heart-felt comments contributed in the space of this digital memorial by those who visited the Quilt this summer, both physically and virtually.

Working in concert with project director Anne Balsamo at the USC Annenberg Innovation Lab, the University of Iowa Digital Studio for Public Humanities (DSPH) developed ""AIDS Quilt Touch."" A mobile web application for mobile devices and laptop|desktop computers, AIDS Quilt Touch is a digital extension of the Quilt, which celebrated its twenty-fifth anniversary this year.

DSPH was approached last spring to create this digital version of the Quilt in conjunction with a display of all 48,000 panels on the National Mall in Washington DC in July 2012. The immediate purposes of the app were to allow visitors to the Mall to find the panel of their loved one, and to leave digital remembrances. The timeline for the project was formidable, with just two months to produce a functional beta version of the app.

With records dating from the 1987, the NAMES database is a cobbled-together collection. Our first task was to parse the flat spreadsheet document from the NAMES foundation into a relational database format that we could use. Simultaneously we needed to learn enough about the Drupal Content Management System to deliver a complete and stable mobile web app based on these data. This web app was to be used by thousands of people trying to find the panel of their loved ones during the quilt’s display in Washington DC. The stakes were high, the challenges were great, but in an incredibly satisfying and successful collaborative effort, we pulled it off. You can see the results of our efforts here: http://www.aidsquilttouch.org/

To the question of “When will the quilt be on display?” we can now answer, “It is always on display.” To the question of “Where is it being displayed?” we can now answer, “Everywhere.” There is much that we can add to the quilt application, and plan to continue development with future displays of this living monument.

The AIDS Quilt Touch mobile web app allows people to leave comments, to extend the narrative that the quilt has begun to tell, and to create virtual celebrations of the lives lived. We plan to expand the types of media users are able to add in the near future: photographs, audio, video, and information and metadata about the quilt panels, those who constructed the panels, and those whose lives are memorialized by the panels.

The celebrations of life that users have left on the mobile app convey a deep resonance with the heart of the human experience. It is these comments that begin to show the promise of the technology. This is the good stuff—the material of human culture. It is not the mobile web app that is important, but the possibility that this technology can facilitate a deep conversation, can create a well of experience from which we can all draw. The mobile web app points to the possibility that these devices we carry in our pockets hold the potential to be portals to a larger and more inclusive cultural realm.",txt,This text is republished here with permission from the original rights holder.,,aids memorial quilt;community-sourcing;drupal;mobile web app;public art,English,"archives, repositories, sustainability and preservation;crowdsourcing;cultural studies;databases & dbms;digitisation, resource creation, and discovery;folklore and oral history;information architecture;interdisciplinary collaboration;interface and user experience design;maps and mapping;programming"
1597,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,Digitizing Serialized Fiction,,Kirk Hess,"paper, specified ""short paper""","Introduction
One barrier to locating serialized fiction in a digital newspaper archive is the fact that the serialized fiction themselves are not indexed, and individual articles do not have subject terms or tags associated with them that would identify them as fiction. As a result, articles are difficult to find unless the reader browses a large volume of issues or simply hits upon a salutary keyword search. Keyword searching of the collection is more effective for articles on topics in farming or farm life than for works of fiction. Unless the reader is looking for stories by a specific author or for a known story title, keyword searching of fiction is highly ineffective.

While the software used by most archives does a good job of connecting articles in a single issue, the reader does not know where to find the next installment in a serialized work of fiction, so he or she has to find it manually by browsing the collection or doing a keyword search. Finally, while the OCR scanning was done to the highest standard, this is an imperfect process, and much of the content cannot be adequately OCR’d due to background noise and broken letters, features of the original newspapers that impede scanning.

This paper summarized the process of our project that was completed over 15 weeks in the summer of 2012. Our goal was to complete the manual indexing process that had already been started previously, display serialized fiction articles in a new repository, evaluate multiple software packages to see which ones were the most promising for use in the future, and evaluate any automated ways of finding serialized fiction.

Method and Results
Manual Indexing
We experimented with three digital library systems, a Drupal/Fedora based repository (Islandora) (http://islandora.ca/), converting the fiction into TEI P5 and displaying it in the California Digital Library’s eXtensible Text Framework (XTF) (http://www.cdlib.org/services/publishing/tools/xtf/) and Omeka (http://omeka.org) a PHP-based publishing platform for digital library objects. We were unable to get Islandora’s OCR correction module installed so we stopped using it in favor of Omeka. We used XSLT to transform the PrXML into very simple TEI5 files, which we were able to upload to XTF, but the lack of an editor and the intensely manual process of text encoding was also rejected in favor of Omeka. TEI Example: http://uller.grainger.uiuc.edu:8080/xtf/search

Ultimately, we decided on using Omeka with the Scripto Plugin for correction. Serialized fiction articles in one title, the Farmer’s Wife, was manually indexed in a spreadsheet, and graduate assistants converted those stories from PrXML into an exhibit, added Dublin Core metadata and links to the newspaper archive from the new serialized fiction collection. The end result was index of serialized fiction that would increase the accessibility of these articles. Omeka Exhibit: http://uller.grainger.illinois.edu/omeka/

Crowdsourcing OCR correction
The University of Illinois Digital Newspaper collections are in Olive Software ActivePaper Archive, which has a method for administrators to correct text but not users. Omeka provides a plugin called ‘Scripto’ for text correction that we were able to successfully use to correct the text in selected articles. We also evaluated Veridian (http://www.dlconsulting.com/veridian/), which is a commercial digital newspaper library solution used by Trove Digitised Newspapers (National Library of Australia), From the Page (http://beta.fromthepage.com/) and Islandora (http://www.islandlives.ca/). From the Page and Islandora were both very difficult to install and administer, and while not free we felt Veridian was a much better approach and we are evaluating it as part of our future newspaper digitization efforts.

Text Analysis
How can we identify serialized fiction without having to have a human find it, index it in a spreadsheet and manually extract it from the archive? Certain n-grams are common within serialized fiction such as ‘chapter’, ‘the end’, ‘to be continued’ and could be used to simply search for keywords within documents; we could also calculate which words occur most frequently in fiction vs. other types of articles and use those terms to automatically tag articles.

We also evaluated using topic analysis to find fiction. We evaluated the 580 articles we had already identified as serialized fiction using Mallet to find 25 topics with 25 words each. Figure 1. shows the top 25 topics modeled as a network using Gephi, while figure 2 shows the topic words ordered by frequency.


Figure 1


Figure 2

Nodes were ranked by betweenness centrality and topic 14 had the highest at 51,321.01 and its component n-grams along with the other top topics could be used to find serialized fiction in other titles.

One final text analysis technique that could be useful is identifying proper names is Named Entity Extraction. While we made an effort to manually remove names from the topic analysis, as you can see they kept reappearing in the results. By using named entity extraction we could eliminate proper names from the topic analysis to make them more accurate, and to link fiction together by the character’s names. All three of these techniques (keyword frequency, topic analysis, named entity extraction) I plan on evaluating in a future study.

Conclusion
Serialized fiction is an important component of historical newspapers and by making it more accessible to patrons and researchers we can expand the use and usefulness of our digital newspaper collections. The manual indexing approach was relatively inexpensive to accomplish but was time consuming and difficult to do over a large corpus of pages. Two promising approaches to find and digitize serialized fiction in our newspaper archive are adding a crowdsourcing feature to enable users to identify article types and correct mistakes, and utilizing text analysis techniques to identify fiction programmatically. We hope to report on our efforts at the latter at the DH 2013 conference.

References
Bastian M., S. Heymann, and M. Jacomy (2009). Gephi: an open source software for exploring and manipulating networks. International AAAI Conference on Weblogs and Social Media.
Brandes, U. (2001). A Faster Algorithm for Betweenness Centrality. Journal of Mathematical Sociology25 163-177.
Cohen, D. (2008). Introducing Omeka. Dan Cohen's Digital Humanities Blog. http://www.dancohen.org/2008/02/20/introducing-omeka/.
Islandora. http://islandora.ca/.
Island Lives. http://www.islandlives.ca/
McCallum, A. K. (2002). MALLET: A Machine Learning for Language Toolkit. http://www.cs.umass.edu/~mccallum/mallet.
Veridian. http://www.dlconsulting.com/veridian/.",txt,This text is republished here with permission from the original rights holder.,,crowdsourcing;serialized fiction;text analysis,English,crowdsourcing;data mining / text mining;digitisation - theory and practice;text analysis
1610,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,"Bibliopedia, Linked Open Data, and the Web of Scholarly Citations",,Michael Widner,"paper, specified ""short paper""","Overview
Bibliopedia, which recently completed an NEH Digital Humanities Start-Up Grant, performs data-mining and cross-referencing of scholarly literature to create a humanities-centered collaboratory. Currently a working prototype, Bibliopedia can search resources including JSTOR and Library of Congress for metadata about scholarly articles and books, examine the articles and books for citations, then present the results in a publicly accessible database. Bibliopedia is designed to work with all humanities scholarship. It will also allow users to create browsable and customizable bibliographies of all the works cited by each article and book. Most importantly, it uses semantic web technology to enable automated textual analysis, data extraction, cross-referencing, and visualizations of the relationships among texts and authors. Using existing open source software, it extracts citation data from existing plain text resources and transforms them into linked open data. This process makes the information easily accessible to the wider scholarly and linked data communities, enables network visualizations of the scholarly landscape. This presentation will cover the details of the Bibliopedia system to show others how they can replicate it. We will also offer to all interested academic parties our existing installation and hosting platform for their experimentation. In particular, we will present our Drupal-based semantic wiki, which features a full web services API, and our custom citation crawler.

Linked open data, one of the core technologies of the semantic web, promotes open sharing of digital scholarly research while it encourages further, potentially unexpected uses. Bibliopedia's method for incorporating linked open data (via RDFa) requires only minimal technical expertise to reproduce. One of the central components of Bibliopedia is the Drupal content management sytem (CMS), which as of version 7 exposes data via RDF/RDFa as part of its core functionality. This functionality, moreover, is not limited to Drupal. For example, Omeka, another CMS developed at the Roy Rosenzweig Center for History and New Media, George Mason University, has some limited support for linked data through its DublinCoreExtended plugin. Bibliopedia demonstrates the power and flexibility of Drupal's approach to linked data while providing more general lessons for digital humanists who seek to incorporate this technology into their projects.

Project Details
Bibliopedia will aid humanities researchers of all levels of expertise by making simple the currently difficult tasks of discovering new scholarly works and the relationships among them. It will create an a scholarly community to verify and elaborate cross-referenced, linked bibliographic data through easy-to-use wiki pages. Scholarly literature will become browsable not only backwards in time, but also forwards, something that is currently impossible.

The semantic web is transforming the Internet from a collection of pages and data readable only by humans to one that machines can understand and process. Semantic web technology promises the ability automatically to determine meaning and then infer connections among different elements, thereby vastly improving search capabilities, discovery of new information, and the overall usefulness of the Internet. Just as information accessible only to humans comprises the great majority of the general Internet, so too is data about scholarly literature locked away in text that computers cannot process without great difficulty. At best, search engines for repositories such as JSTOR permit researchers to query author name, journal titles, and keywords, but once a work is found, the search stops. No connections among works are found precisely because machines cannot currently read that data. Although Google Scholar attempts to show citations of articles, its usefulness is highly limited because it does not make clear the relationships among articles, present very limited metadata about each article (if any), fails to provide for community elaboration or correction, and includes only works that are publicly available. Yet despite its limitations, Google Scholar stands as a significant technological advance beyond keyword-based search engines such as those provided by JSTOR and Project Muse.

Bibliopedia will, by aggregating data from as many sources as possible, converting citations into semantic web format, and then cross-referencing an ever-growing database of scholarly works, be able not only to overcome many of the limitations of Google Scholar and become a powerful research tool in its own right, but also to make a valuable contribution to the growing semantic web. Introducing high quality metadata about humanities scholarship to the semantic web will enable others in the semantic web/linked data world to process that data in new, unexpected ways that will accrue further benefits to the scholarly community. For example, the standards underlying the semantic web make data visualization and automated inferences about relationships trivially easy rather than the complex problems such tasks currently present. Bibliopedia will, then, through the innovation of placing metadata about scholarly literature into a linked data format, open up a vast range of possible future innovations and analyses based on that data, which is currently locked away and readable only by select humans.

Another virtue of a linked data format is that it will help resolve many of the challenges inherent in metadata, some will inevitably remain. Rather than attempt to solve this incredibly complex problem through automation alone, then, Bibliopedia will, in the process of displaying its results for human consumption, also provide for human feedback in the form of correction and elaboration. A common disadvantage of fully automated text analysis and data extraction tools such as Google Books, Google Scholar, and other digital research tools is that their automatic parsers have errors in their metadata that they do not allow subject matter experts to repair. Bibliopedia will pursue the goal of unifying that information into an environment that not only displays the information efficiently, but actively encourages crowd-sourcing metadata on books, articles, and publications of all kinds. In thus opening data up to revision by the scholarly community, Bibliopedia can build on the strong work of mature data silos, improve overall data quality, and provide the academic community at large a continuously evolving research tool.

There currently exists a multitude of projects and tools designed to work with book metadata, cross-reference scholarly articles (localized to the sciences), or create user communities around a chosen interest. Further, some of the most important trends currently revising the ways we use technology are social media, collaboration, and data aggregation. By incorporating the benefits realizable from each of these trends, Bibliopedia will create a powerful tool for scholarly research at all levels. None of the existing tools, however, focus on scholarship for the humanities, nor do they present the information in the linked data format necessary to the semantic web.",txt,This text is republished here with permission from the original rights holder.,,citation networks;drupal;linked open data;semantic web,English,crowdsourcing;databases & dbms;data mining / text mining;information retrieval;knowledge representation;linking and annotation;metadata;ontologies;publishing and delivery systems;semantic web
1661,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,The FAST-CAT: Empowering Cultural Heritage Annotations,,Gary Munnelly;Nicola Ferro;Owen Conlan;Cormac Hampson,"paper, specified ""long paper""","Introduction
The role of annotations in digital humanities is well known and documented (Agosti et al., 2004, 2007) (Bélanger, 2010). Subsequently, many different tools which allow for the annotation of digital humanities content have been developed. Unfortunately, tools designed specifically for an individual portal are typically only compatible with that system. More general solutions, which can be easily distributed across various sites, have been produced, but these systems often have limited functionality (only annotating a single content type, no sharing features etc.) (Okfn) (TILE, 2011).

FAST-CAT (Flexible Annotation Semantic Tool — Content Annotation Tool) is a generic annotation system that directly addresses this challenge by implementing a convenient and powerful means of annotating digital content. It provides a reliable, portable manner of annotating both textual and image content in documents. The annotations are stored remotely by the FAST service which means that they may be shared across different sites maintaining the same data without the need for modification.

FAST-CAT has been developed as a module for the Drupal 7 content management system making it extremely easy to add to an existing Drupal site.

This paper introduces FAST, the backend service providing powerful annotation functionalities, and CAT, the frontend Web annotation tool, and discusses how its features are tackling important challenges within the Digital Humanities field.

Features of FAST
The FAST annotation service adopts and implements the formal model for annotations proposed by (Agosti and Ferro, 2008). Since then, FAST has been completely reengineered with added functionality such as provenance, logging and extended searching. According to this model, an annotation is a compound multimedia object which is constituted by different signs of annotation. Each sign materializes part of the annotation itself; for example, a textual sign would contain the textual content of the annotation, an image sign would contain images, etc. Each sign is characterized by one or more meanings of annotation, which specify the semantics of the sign, e.g. a sign whose meaning corresponds to the “title” field in the Dublin Core (DC) metadata schema or a sign carrying a question from the author whose meaning may be “question” or similar.

The flexibility inherent in the annotation model allows us to create a connective structure, which is superimposed to the underlying documents managed by digital libraries. This can span and cross the boundaries of different digital libraries and the Web, allowing the users to create new paths and connections among resources at a global scale.

FAST defines three different scopes which determine the visibility of an annotation — private, public and group. With FAST-CAT, the scope of an annotation is set to private by default, meaning that only the person who created the annotation can see it. These annotations may serve as reminders or notes within the document for the user, akin to writing in the margin of a page.

The user can choose to make an annotation public, allowing other users to read their comments on a document. This has numerous applications for users of all levels of expertise. For instance, experienced users may choose to create public notes and annotations which can expand on the text of the document, helping less experienced users to comprehend the content. Less experienced users may indicate parts of the document which they would like to be explained further.

Group annotations allow users to provide viewing and editing permissions on an annotation to specific users. This means that a team of people working towards a similar goal could communicate directly through the medium of annotation. In this way, it can be seen that FAST-CAT can play a crucial role in collaboration.

Features of CAT
CAT is a web annotation tool whose development began in July 2012. It has been developed with the goal of being able to annotate multiple forms of document content and assist in collaboration in the field of digital humanities. At present, CAT allows for the annotation of both text and images. The current granularity for annotation of text is at the level of the letter. For image annotations, the granularity is at the level of the pixel. This allows for extremely precise document annotation, which is very relevant to the Digital Humanities domain due to the variety of different assets that prevail.

There are two types of annotation which may be created using CAT; a targeted annotation and a note. A targeted annotation is a comment which is associated with a specific part of the document. This may be a paragraph, a picture or an individual word, but the defining feature is that the text is directly associated with a specific entity. Conversely, a note is simply attached to the document. It is not associated with a specific item therein. Typically, this serves as a general comment or remark about the document as a whole. Further to allowing a user to comment on document text, the annotations created using CAT allow a user to link their annotations to other, external sources. Hence CAT can be used to construct a narrative through a number of documents. This is hugely beneficial for teachers using digital cultural collections and for students from primary to university level. For example, using these links a teacher can construct a predetermined path for their students to follow through a series of sites relevant to their chosen topic. Importantly, each link has comment text associated with it, allowing an educator to explain why this specific link is important or what the student should seek to gain from following this particular path.

While CAT is beneficial for researchers and educators, it is also being used as an important source of user data for the content provider. Websites such as Amazon and YouTube are able to provide increasingly accurate recommendations for their individual users. These recommendations are facilitated by a user model which is driven by a combination of ratings, recently viewed items and numerous other factors. For a digital humanities site, annotations provide an insight into which entities are of interest to a user. If a user is frequently annotating a document, it is likely that this document is of interest to them. Furthermore, if the text being annotated is analysed, it may be possible to discern specific items of interest within the document. A digital humanities site which can determine what a user is attempting to study, then anticipate and recommend sources that may be of use to them in the future is profoundly useful. If well implemented, curators of digital humanities portals will see a dramatic improvement in the effectiveness with which researchers interact with their domain.

FAST-CAT and CULTURA
At present, FAST-CAT is being developed as part of the CULTURA project (Hampson et al., 2012a, 2012b). A key aspect of CULTURA is the production of an online environment that empowers users, of various levels of expertise, to investigate, comprehend and contribute to digital cultural collections. FAST-CAT is a key component of this environment and is currently being trialled with the help of three different user groups.

A team of MPhil students and professional researchers will use the tool as part of their teaching, collaboration and research into the 1641 depositions. These users will be testing FAST-CAT in a free form manner. How they choose to annotate and what content they label is entirely determined by their own needs. The 1641 depositions are text only content, so these students will serve only to evaluate the text annotation aspect of the tool.

Providing an alternative insight is a group of secondary school students from Lancaster whose teacher will use the annotations to guide them through a lesson. These students will also be working with the 1641 depositions.

Masters students in Padua will test the image annotation functionality of FAST-CAT as part of their research into the IPSA collections of illuminated manuscripts. Similarly to the MPhil students, their approach to annotating documents will be determined by their own research methodology.

The various features offered by FAST-CAT and its user interface will be evaluated in detail and comparisons will be drawn between the manner in which different user groups availed of annotations depending on their level of expertise and document content. Furthermore, FAST-CAT will also help to drive CULTURA’s comprehensive user model by providing the site with updates on the user’s behaviour regarding document annotation.

Future Work
Much of the further enhancement of FAST-CAT will be based on the feedback given by the user groups mentioned in the previous section. However there are already plans to expand and improve the system for future versions.

While FAST-CAT is supported by modern browsers, to improve portability, implementations for older web browsers will be developed.

FAST-CAT is a Drupal 7 module which means that, at present, it is only available for the annotation of websites which are built using the Drupal content management system. However, it only utilises a small amount of Drupal functionality to relay messages from the client computer to FAST. This dependency is easily removed as the majority of functionality is either client side or independent of Drupal. Designing and implementing a more server agnostic php script will allow FAST-CAT to be deployed on any website. This is one of the main items for future development of the system and will help to ensure that FAST-CAT can be utilised by as wide a range of content based websites as possible.

References
Agosti, M., G. Bonfiglio-Dosio, and N. Ferro (2007). A Historical and Contemporary Study on Annotations to Derive Key Features for Systems Design. International Journal on Digital Libraries (IJoDL). 8.1. 1-19.
Agosti, M., & N. Ferro (2008). A formal model of annotations of digital content. ACM Transactions on Information Systems (TOIS). 26.1. 3:1-3:57.
Agosti and Ferro, I. Frommholz, & U. Thiel (2004). Annotations in Digital Libraries and Collaboratories — Facets, Models and Usage. In Proc. 8th European Conference on Research and Advanced Technology for Digital Libraries (ECDL 2004), 244-255. LNCS 3232, Springer: Heidelberg.
Bélanger, M.-E. (2010). Ideals. https://www.ideals.illinois.edu/bitstream/handle/2142/15035/belanger.pdf?sequence=2 (accessed October 25, 2012)
Hampson, C., M. Agosti, N. Orio, E. Bailey, S. Lawless, and O. Conlan (2012). The CULTURA Project: Supporting Next Generation Interaction with Digital Cultural Heritage Collections. Limassol, Cyprus.
Hampson, C., S. Lawless, E. Bailey, S. Yogev, N. Zwerdling, and D. Carmel (2012). CULTURA: A Metadata-Rich Environment to Support the Enhanced Interrogation of Cultural Collections. Cádiz, Spain.
Okfn. Okfn Annotator. http://okfnlabs.org/annotator/(accessed June 2012).
TILE. (2011). TILE: text-image linking environment. http://mith.umd.edu/tile/ (accessed July 2012).",txt,This text is republished here with permission from the original rights holder.,,annotation;document linking;recommender system,English,"networks, relationships, graphs;software design and development;teaching and pedagogy;user studies / user needs"
1736,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,Reliable Citation as a Foundation for Preservable Web-Based Digital Humanities Projects,,James Smith,poster / demo / art installation,"Preservation of digital humanities (DH) projects is an emerging problem. As languages, frameworks, platforms, libraries, and databases evolve, the effort required to maintain meaningful access to existing projects is a challenge that competes with efforts to produce new work. Creators of DH projects need to be able to continue to iterate and innovate but also demonstrate good stewardship of digital materials.
The field recognizes this problem, from the 2004 Sustaining Digital Scholarship (SDS) final report and continuing through such projects as Memento, SiteStory, and TAPAS, and is actively researching ways to enable sustainable, long-term stewardship for certain project components, but these efforts do not provide a comprehensive platform for the full DH project. All of these efforts consider a project to be a set of static pages or resources that seldom change. None of them would be sufficient for hosting or describing ongoing projects such as World Shakespeare Bibliography Online.
Many new DH projects build their web presence with open source platforms such as Wordpress, Drupal, or Omeka, extending them through customizations. The long-term sustainability of such projects is an issue, involving the cost of hosting as well as migration of customizations as platforms and languages evolve. SalahEldeen and Nelson show evidence that after only a year, an average of eleven percent of cited on-line resources are lost. It is not surprising then that none of the common systems used to build DH projects allows reliable citation since the same platforms used in DH host a share of the resources lost each year. It is surprising that reliable citation is not seen as a greater problem by the scholarly community. Reliable citation is necessary for long-term preservation and therefore expresses a need for a temporal content and data management system that allows for reliable citation of scholarly narratives and resources.
We might base the value of a scholarly work on its place in the larger scholarly conversation. If it is not part of the conversation, then it has little effect on the field and thus has little value. Scholarly work can only be part of the conversation if it can be referenced. This is well developed for traditional publications (e.g., citing a particular edition of a printed work), but remains a problem for web-based scholarly work, not because particular pages can not be addressed, but because the information presented as part of that page is not stable. Without being able to reference the particular version of the page, scholars can not make reliable arguments about the work. What a reader sees might differ from what the author saw when researching and writing the work referencing the web¬based project. These problems increase when referencing a dynamic, algorithmic project.
We see three fundamental requirements for such a system to enable reliable citation of scholarly works and resources: temporal citation, reproducible citation, and sustainable citation. Any platform meeting these requirements should be able to provide level four preservation as described in the SDS final report (11).
A temporal citation of a web-based scholarly work must be able to address the view within the context of the project’s history. Not only must a scholar be able to point a reader to a particular resource in a project, but the scholar must be able to point to a particular resource at a particular date and time.
A reproducible citation of a web¬based scholarly work must show the same content over time. Fetching the cited resource year after year should show no significant changes in the scholarly content of the resource.
A sustainable citation of a web-based scholarly work allows a scholar to cite a project and know that their readers will be able to see the same information they saw by following the citation, for as long as their citation exists. Sustainability is a social issue as much as a technical one. We are not trying to address the social issues involved in sustainability in this poster.
The poster consists of diagrams and text explaining how reliable citation works with respect to resource versioning and project timelines. In addition, a demonstration of a temporal content and data management system hosted athttp://alpha.ookook.net/ providing reliable citation will accompany the poster so that attendees can interact with the system and see the platform affordances in action. The demonstration will also provide an opportunity for attendees to interact with the developer.
Reliable citation does not require any unique data model or software architecture. The poster outlines both the data model and the architecture as they are developed in the demonstration software, principally by segmenting a project’s history into discrete editions that aggregate changes to the project.
Discussion will quickly muddy if we don’t establish some nomenclature for dates and times. A resource date and time is the date and time for which the resource should be rendered. For example, if I specify a resource date and time of noon on January 1st, 2012, then I expect the see a rendering of the resource as it appeared at noon on January 1st, 2012. A request date and time is the date and time at which the request is made, even if the request is for a resource with a resource date and time different than the request date and time.
The data model partitions the project into two classes of objects: editioned objects, such as a project or theme, and versioned objects, such as pages or stylesheets. Editions are published for a span of time during which no public changes are made to the pages. Any changes made to a page require the creation of a new page version which will be aggregated with other versions when a new edition is created and published. Only one project edition is active for a resource date and time. By tracking the time spans for which an edition is active, we can reproduce the project as it existed at a particular date and time.
The demonstration software separates information into two editioned resources: Projects (web sites) and Themes (collections of style information). Editions of projects and themes are independent of each other, with each managing their own history.
References between different editioned resources are done by naming the referenced resource as well as the referenced resource date and time. This allows a project to select a theme in a reproducible fashion. References to a versioned resource within an editioned resource (e.g., a page within a project) may reference the page without referencing a particular version. The appropriate version will be retrieved based on the edition selected by the resource date and time of the request.
This poster will be of interest to anyone wishing to see how a platform supporting reliable citation might be designed.
References
Drupal. http://www.drupal.org/
Memento. http://www.mementoweb.org/
Omeka. http://www.omeka.org/
SalahEldeen, M. Hany, and M. L. Nelson Losing My Revolution: How Many Resources Shared on Social Media Have Been Lost? (2012). arXiv:1209.3026.
Sustaining Digital Scholarship Final Report. (2004) http://www2.iath.virginia.edu/sds/SDS_AR_2003.pdf
SiteStory. http://mementoweb.github.com/SiteStory/
TEI Archiving Publishing and Access Service (TAPAS) Project. http://www.tapasproject.org/
Wordpress. http://www.wordpress.org/
World Shakespeare Bibliography Online. http://www.worldshakesbib.org/",txt,This text is republished here with permission from the original rights holder.,,content management system;platform;preservation;temporal;world wide web,English,"archives, repositories, sustainability and preservation;internet / world wide web;publishing and delivery systems"
1738,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,Contemporary solutions to retrieve and publish information in ancient documents using RDF and Islandora,,Charlotte Tupman;Anna Jordanous;Alan Stanley,"paper, specified ""long paper""","Introduction
This paper considers what can be gained from enhancing TEIencoded texts with RDF and OAC annotations and transforming to other representations, and how to facilitate their production, editing and storage. Our case study is the Sharing Ancient Wisdoms (SAWS) [1] project, which analyses the tradition of wisdom literatures. Scholarly interest is focused on semantic links within and between specific sections of these texts. SAWS produces TEIbased digital editions with semantic annotations in RDF to allow investigation of these links as Linked Data. This approach has the potential to be used widely to link and describe related sections of a variety of texts. Links can be extracted and transformed for manipulation and searching using alternative methods, illustrated by our TEItoRDF XSLT. In producing, storing and annotating such documents, the TEIediting process may present barriers to information enrichment for nontechnical users. The Islandora repository management software assists in creating and managing collections of documents through more intuitive, GUIdriven interactions with Fedora repositories. Within Islandora, the Digital Humanities Solution Pack provides a WYSIWIG online interface to help create, edit and annotate TEI documents, and simplifies the addition of semantic links. We demonstrate how TEI documents can be developed in diverse directions using Linked Data and RDF, and show how production of LinkedDataenhanced TEI documents can be facilitated using the Digital Humanities Solution Pack within Islandora. [2] RDF triples generated through the Islandora interface are exposed as standalone relationships which may be applicable in other contexts.

The Sharing Ancient Wisdoms (SAWS) use case
SAWS [3] [4] [5] is a key use case for this work, requiring an approach encapsulating various types of information including structural markup and semantic annotation. SAWS enables linking and comparisons within and between anthologies, their source texts, and their recipient texts, acting as a framework through which others can link their own materials via the Semantic Web.

SAWS focuses on gnomologia [6] , collections of sayings that transmitted moral or philosophical ideas. [7] These sayings were selected from earlier manuscripts, reorganised or reordered, and often modified or reattributed. The texts crossed linguistic barriers in the mediaeval period, and in later centuries were translated into western European languages. They form a complex network of interrelated texts, which when analysed can reveal much about the dynamics of the cultures that created and used them.

SAWS enables investigation of the relationships between specific sayings, tracing the links through different textual variants and languages. This has been achieved by enhancing our TEI with RDF: each saying can be linked to other relevant sections of text via a subject-predicate-object relationship defined as part of an ontology. The <relation> element, which has recently been updated with new attributes, allows us to enter RDF directly into the TEI document and combine this with information about scholarly responsibility. [8]

Combining TEI and RDF
TEI allows for extremely granular expression within a context; RDF is often meaningful in the absence of context. The strength of RDF lies in its apparent simplicity and its interoperability: its data is discoverable and reusable. Combining subject-predicate-object assertions can convey considerable metadata and tell complex stories. RDF can also be expressed as OAC annotations, which may have any number of targets of differing types. A target may indicate a section which overlaps another (via spatial or indexing coordinates) without breaking XML validation. SAWS implements the CITE/CTS citation scheme, [9] allowing overlapping sections to be described fully and referenced using anchor points in the TEI structure.

SAWS accommodates TEI and RDFcompatible markup within the same document and workflow, using established RDF syntax for marking up information of semantic interest. For SAWS, it is preferable to keep structural, syntactic and semantic markup in the same documents where possible, and to access the semantic information using standard tools such as XSLT. [10] [11] [12]

Previous approaches to the recording of semantic links within TEI documents have had limitations. The EARMARK ontology [13] provides an RDF model for XML information, but only for structure, so structural information is separated from text, and we cannot add semantic information while editing. RDFTEF [14] [15] requires documents to be edited in a separate environment within which standard XML tools cannot be used. [16] Approaches to incorporating RDF within XML documents do not transfer easily to a TEI representation: RDFa encodes RDF directly within specific XML attributes, but key attributes for RDFa [17] are not included in standard TEI schemas. [18]

While it would not technically be difficult to use RDFa by extending the TEI schema, this would introduce extra work which may not be necessary, and it would mean ignoring suitable alternatives proposed and accepted by the TEI community (discussed below) which require no extra schema work; if considered suitable, adopting such an alternative would enable SAWS to contribute towards establishing conventions within the TEI community for working with RDF within TEI.

Recognising the importance of combining RDF and TEI, a TEI Special Interest Group (SIG) in the use of ontologies [19] is developing XSLTs to transform TEI documents into RDF, using the CIDOC-CRM [20] as a basis. [21] [22] [23] [24] The SIG maps only a subset of elements to CIDOC-CRM, focusing on those that represent very particular entities. [25] SAWS would therefore not be able to retrieve many triples of scholarly interest such as manuscript structure and metadata.

To represent a wider range of data, a recent TEI recommendation [26] has been adopted by SAWS, using the <relation> element to represent links from one object [27] (@active) to another (@passive), using link types (@ref) which can incorporate a domain ontology. [28] [29] This increases the expressiveness of the markup without requiring changes within TEI. <relation> is an established element; the more recent addition of @ref has enabled <relation> to be used for RDF triples, along with the assertion of responsibility using @resp.

An XSLT stylesheet for extracting information from TEI to RDF
Semantic information can be accessed in limited ways via a TEI document, but when extracted, it can be placed in a triple store for access, querying and reasoning. [30] New knowledge can be derived by traversing internal links, and following links to related external Linked Data sources. [31]

We offer an XSLT stylesheet that transforms TEI, rerepresenting the structural, semantic and metadata information as RDF/XML triples. [32] Acknowledging practical difficulties concerning the size of the TEI tagset, we take the minimal required version of TEI, TEIBare. This forms a base for future extension, e.g. to TEILite. [33] [34] Using Dublin Core terms [35] such as dct:creator and dct:title, statements in the TEI header are transformed into corresponding RDF triples, and structural ordering of blocks within the TEI document are encoded using dct:isPartOf and dct:hasPart triples. We have extended the XSLT to include transformation of triples encoded through the <relation> element into RDF syntax, and further extensions can be added.

Transformations from TEI to RDF for the SAWS use case
The SAWS TEI version of the Kitāb alḤaraka (“Book of Happiness”), held at Ankara Üniversitesi, contains various metadata in its header. Applying the XSLT generates the following triples:


The SAWS TEI version of the Corpus Parisinum manuscript, held in the Digby collection in Oxford’s Bodleian library, contains a section <div xml:id=""Aristippus01""> which is contained by its parent, <div xml:id=""Part01"">. From this we can derive the following structural triples:


Feedback on the editing and linking process
SAWS scholars studying documents in ‘right-to-left’ (RTL) languages noted the difficulties in working with standard XML editing software, and also requested more intuitive interfaces for editing documents and adding <relation> links.

Islandora is an open source project allowing users to manage a Fedora repository through PHP using a Drupal front end. Fedora repositories are adept at maintaining and versioning metadata accompanying scholarly objects. Islandora provides an intuitive way to use Fedora to create, access and manage document collections, and is currently being used across a varied number of use cases. [38]

Various “solution packs” within Islandora are available for different types of projects. The Digital Humanities Solution Pack is specifically designed for text editing and annotation, based on Shared Canvas and CWRC (for editing TEI and adding links). These tools are used to access, edit, and retrieve information held in repositories, including TEI transcriptions of texts, OCR tools related images, annotations and metadata. This Digital Humanities project within Islandora is sponsored by EMiC to develop a suite of applications for managing and critically analysing Canadian modernism. As one of the authors of this paper is the lead programmer of both these projects, he can incorporate these transformations into the workflow to expose the data publicly. Of particular interest is the ability to extract data from TEI to build and maintain authority lists.

The Islandora Critical Editions module exposes a GUI allowing the addition and viewing of RDF entities and TEI tags. No knowledge of XML is required. Entities tie textual offsets to objects from authority lists, userentered notes, external links, or date ranges through RDF. Image annotations are OAC RDF annotations.


Concluding remarks
We offer a functional XSLT for converting TEI to RDF, incorporating the recent application of <relation> for encoding RDF within TEI and extracting TEI <relation> elements and selected structural markup as RDF files.

Future SAWS/Islandora collaboration will investigate the enhancement of TEIencoded documents and a more user-friendly environment for editing, managing and linking texts. The DH Solution Pack by Islandora is available by request but has not yet been released in beta version. It is intended that SAWS will have implemented and tested a working version of the DH Solution Pack by June 2013. Any DH project that wants to link TEI files with other sources of information will, we argue, benefit from investigating the DH Solution Pack. It has wide implementation possibilities and will be particularly useful for projects using right-to-left languages.

The outcomes of this SAWS/Islandora collaboration should apply across a wide variety of texts. It is hoped that this paper will stimulate further interest in RDF and Linked Data within TEI, particularly amongst Digital Humanists wishing to work with a broader range of Humanities scholars.

Notes
1. http://www.ancientwisdoms.ac.uk. Last accessed October 2012.

2. John Unsworth. (2003). Tool-Time, or 'Haven't We Been Here Already?' Ten Years in Humanities Computing. Delivered as part of ""Transforming Disciplines: The Humanities and Computer Science,"" Washington, DC. Available at: http://people.lis.illinois.edu/~unsworth/carnegieninch.03.html (last accessed 20th July 2012).

3. Anna Jordanous, K. Faith Lawrence, Mark Hedges, and Charlotte Tupman. (2012). Exploring manuscripts: sharing ancient wisdoms across the semantic web. In Proceedings of the 2nd International Conference on Web Intelligence, Mining and Semantics (WIMS '12), Craiova, Romania.

4. Tupman, Charlotte; Hedges, Mark; Jordanous, Anna; Lawrence, Faith; Roueche, Charlotte; Wakelnig, Elvira; Dunn, Stuart. (2012). Sharing Ancient Wisdoms: developing structures for tracking cultural dynamics by linking moral and philosophical anthologies with their source and recipient texts. In Proceedings of Digital Humanities (DH2012), Hamburg, Germany.

5. Hedges, Mark; Jordanous, Anna; Dunn, Stuart; Roueche, Charlotte; Kuster, Marc W.; Selig, Thomas; Bittorf, Michael; Artes, Waldemar;(2012). ""New models for collaborative textual scholarship,"", Proceedings of the 6th IEEE International Conference on Digital Ecosystems Technologies (DEST), Campione d’Italia, Italy.

6. F. Rodríguez Adrados, (1981). Greek wisdom literature and the Middle Ages: the lost Greek models and their Arabic and Castilian Translations (2001), English translation by Joyce Greer (2009), pp. 9197 on Greek models; D. Gutas, “Classical Arabic Wisdom Literature: Nature and Scope”, Journal of the American Oriental Society, Vol. 101, No. 1, Oriental Wisdom (Jan. Mar., 1981), pp. 4986

7. M. Richard, (1962). “Florilèges grecs”, Dictionnaire de Spiritualité V, cols. 475512

8. A full discussion of our TEI markup and use of the <relation> element can be found here: Tupman, Charlotte; Hedges, Mark; Jordanous, Anna; Lawrence, Faith; Roueche, Charlotte; Wakelnig, Elvira; Dunn, Stuart. Sharing Ancient Wisdoms: developing structures for tracking cultural dynamics by linking moral and philosophical anthologies with their source and recipient texts. In Proceedings of Digital Humanities (DH2012), Hamburg, Germany. 2012.

9. (http://www.homermultitext.org/hmtdoc/cite/)

10. M. O. Jewell. (2010). Semantic Screenplays: Preparing TEI for Linked Data. In Proceedings of Digital Humanities, London, UK.

11. 11 K. F. Lawrence. (2011). Wherefore Art Thou? Crowdsourcing Linked Data from Shakespeare to Dr Who. In Proceedings of Web Science, Koblenz, Germany.

12. Blanke, Tobias; Bodard, Gabriel; Bryant, Michael; Dunn, Stuart; Hedges, Mark; Jackson, Michael; Scott, David; (2012). ""Linked data for humanities research — The SPQR experiment,"" 6th IEEE International Conference on Digital Ecosystems Technologies (DEST), Campione d’Italia, Italy

13. S. Peroni and F. Vitali. (2009). Annotations with EARMARK for arbitrary, overlapping and outof order markup. In Proceedings of the 9th ACM symposium on Document engineering, pages 171180, Munich, Germany.

14. G. Tummarello, C. Morbidoni, and E. Pierazzo. (2005). Toward textual encoding based on RDF. In Proceeding of the 9th International Conference on Electronic Publishing (ELPUB 2005), Kath. Univ. Leuven, June, pages 5763.

15. RDFTEF sourcecode: http://rdftef.sourceforge.net/ Last maintained 2007.

16. P. Portier, N. Chatti, S. Calabretto, E. Egyed-Zsigmond, and J. Pinon. Modeling, encoding and querying multistructured documents. Information Processing & Management. Forthcoming.

17. e.g. @rel, @rev, @href, @resource, @property, @vocab

18. A more detailed discussion of existing methods for encoding RDF within TEI markup can be found in: A. Jordanous, A. Stanley and C. Tupman. Contemporary transformation of ancient documents for recording and retrieving maximum information: when one form of markup is not enough. In Proceedings of Balisage: The Markup Conference 2012. Balisage Series on Markup Technologies, vol. 8 (2012), Montréal, Canada, August 2012.

19. TEIOntologies Special Interest Group http://www.tei-c.org/SIG/Ontologies/

20. ChristianEmil Ore and Øyvind Eide. (2009). TEI and cultural heritage ontologies: Exchange of information? Literary and Linguistic Computing 24(2): 161172

21. http://www.edd.uio.no/artiklar/tekstkoding/tei_crm_mapping.html, http://www.edd.uio.no/tei/teiontsig/test_crm_model.graphml

22. http://www.tei-c.org/release/xml/tei/stylesheet/rdf/

23. http://www.teic.org/SIG/Ontologies/guidelines/guidelinesTeiMappableCrm.xml

24. CIDOCCRM only direct represents textual material through one class (E33 Linguistic Object) and its two subclasses (E34 Inscription, E35 Title), but its selection as a base model is partially influenced by research aims of the SIG members in enhancing cultural heritage and museum documentation (http://www.teic.org/SIG/Ontologies/guidelines/guidelinesTeiMappableCrm.xml). Dicussions (see http://wiki.teic.org/index.php/SIG:Ontologies ) about the use of FRBRoo, a bibliographical records model harmonised with CIDOCCRM (http://www.cidoccrm.org/frbr_inro.html) have not been acted upon, to date. Some mappings from TEI to Dublin Core (a model of metadata information: http://www.dublincore.org) are occasionally present in stylesheets created by the SIG (http://www.teic.org/release/xml/tei/stylesheet/rdf/dc.xsl) but this output has not been highlighted, despite Dublin Core also being a realistic option for more detailed mappings of document metadata, especially from the TEI header.

25. http://wiki.teic.org/index.php/SIG:Ontologies

26. Sourceforge.net discussion: Encoding RDF relationships in TEI ID: 3309894, at http://tinyurl.com/lrbz53b

27. The application of <relation> to express RDF triples has been documented by TEI at http://www.teic.org/release/doc/teip5doc/en/html/refrelation.html with supporting examples.

28. The SAWS ontology (an extension of FRBRoo for representing relations of interest for study of wisdom manuscripts) is available at http://purl.org/saws/ontology.

29. S. Dunn, M. Hedges, A. Jordanous, K. F. Lawrence, C. Roueché, C. Tupman, and E. Wakelnig, Sharing Ancient Wisdoms: developing structures for tracking cultural dynamics by linking moral and philosophical anthologies with their source and recipient texts, Digital Humanities 2012, Hamburg, Germany.

30. For the SAWS use case, a SPARQL endpoint to access the RDF data is available ( http://www.ancientwisdoms.ac.uk/sparql )

31. To date, SAWS links to various collections of ancient data interlinked through Pelagios (http://pelagios.blogspot.com ) references to the Pleiades historical gazetteer (http://pleiades.stoa.org/ ). We are also in the process of linking to existing relevant documents such as in the Perseus Digital Library (http://www.perseus.tufts.edu/ ) and would like to link to information on people mentioned in the texts, such as through the Prosopography of the Byzantine World resource (http://www.perseus.tufts.edu/ ). The facility to traverse links between sets of data and discover related information serendipitously is a major benefit of Linked Data for the SAWS project.

32. XSLT available at http://www.ancientwisdoms.ac.uk/media/ontology/tei_to_rdf.xsl , with working versions available through https://github.com/ajstanley/TEI_to_RDF.

33. The Dublin Core Metadata Initiative is the main source model for structural and metadata mappings from TEIBare to RDF: http://dublincore.org/documents/dcmiterms/

34. http://www.teic.org/Guidelines/Customization/

35. The namespace ‘dct’ represents http://dublincore.org/documents/dcmiterms/

36. The manuscript ID is in CITE/CTS format for document citation (see http://www.homermultitext.org/hmtdoc/cite/ )

37. The dct:conformsTo relationship requires the object of the triple to be a string, rather than a resource

38. List of current Islandora installations: http://islandora.ca/current_installations",txt,This text is republished here with permission from the original rights holder.,,islandora;linked data;rdf;semantic web;tei,English,"archives, repositories, sustainability and preservation;cultural studies;interface and user experience design;medieval studies;ontologies;philosophy;scholarly editing;semantic web;text analysis;xml"
1749,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,"Memoragram, a service for collective remembrance",,Renzo Arauco Dextre,"paper, specified ""short paper""","Introduccion
Desde la década de los 80s hasta los 2000 el Perú vivió en medio de un conflicto armado entre dos movimientos terroristas, Sendero Luminoso y el Movimiento Revolucionario Túpac Amaru (MRTA), los cuales buscaban derrocar el sistema democrático peruano, desatando uno de los más violentos pasajes de la historia moderna.
Estos movimientos fueron vencidos militarmente, desarmados y encarcelados, pero a un alto costo en vidas humanas. A pesar de la desarticulación de estos grupos aún se reportan células armadas y organizadas en los lugares más alejados de la selva peruana. Al mismo tiempo, en las ciudades buscan impunidad criminal y política para sus líderes arrestados a través de mecanismos legales que les permitirían introducirse en la política activa.
Hoy, casi 10 años después del fin de este episodio de la historia, el museo que conmemora el recuerdo de las víctimas de la violencia, el Lugar de la Memoria del Perú, no ha sido terminado. Las nuevas generaciones no conocen claramente lo sucedido debido al deficiente programa educativo; tal que estos grupos subversivos aprovechan la desinformación para entregar una imagen distorsionada de esta parte de la historia. (Pisa, 2009). Sin embargo, somos la octava comunidad con mayor uso de las redes sociales (Comscore Inc. 2012) e Internet en el mundo. Asimismo, Perú ya ha sobrepasado el 100% de penetración móvil.(Budde.com 2012)
Estos índices muestran que Perú tiene una oportunidad única y sin precedentes de desplegar servicios que asistan al desarrollo y mantenimiento de una memoria colectiva usando tecnologías móviles.
La Evolucion De La Idea
La misión del proyecto evolucionó con el tiempo y se ha trabajado y conversado con expertos del tema.
Un museo virtual
Memoragram nace a partir de la idea de crear un museo geo-localizado de la memoria, accesible desde cualquier dispositivo sin importar la hora ni el lugar. Los usuarios podrían explorar eventos históricos cercanos a su ubicación actual o hacia la fecha que deseen consultar, revisar los personajes involucrados, los lugares (en su mayoría demolidos), organizaciones e incluso revisar qué libros, revistas, periódicos, películas, reportajes de TV, cómics, entre otras fuentes, hacen referencia al hecho.
Por el lado tecnológico, los usuarios podrían usar smartphones o tablets para que, al acceder a una web móvil, esta solicite permiso para usar el GPS del dispositivo y así mostrar contenido relevante. La tecnología que hace posible este acceso es HTML5, CS3 y Javascript. Asimismo, los usuarios podrían descargar una tercera aplicación móvil llamada Junaio, la cual, usando el GPS, giroscopios, brújula y cámara puede mostrar contenido en una vista de tiempo real.
Si bien esta parte del alcance no ha sido modificada, surgieron otros problemas fuera del campo tecnológico.
El problema de las fuentes de datos
Con la idea y el alcance del proyecto se empezó por buscar fuentes de información fiables, entre ellas el Banco de Imágenes de la Comisión de la Verdad y Reconciliación Nacional del Perú (Comisión de la Verdad y Reconciliación del Perú), la cual se basa en los archivos fotográficos de los principales diarios y revistas locales peruanos. Este mismo banco se usó para formar la muestra Yuyanapaq (Programa de las Naciones Unidas para el Desarrollo — PNUD Perú) (término quechua que significa ‘Para recordar’). Este archivo posee alrededor de 1560 fotografías.
Luego de una evaluación de esta fuente y del mapeo de eventos posibles a registrar se concluyó que el número de eventos registrables sería mucho menor a 1560. De encontrarse otras fuentes de datos el número sería más limitado. Esta situación conllevaría a que la oferta de contenido fuera fija, lo cual podría evitar que los usuarios no se familiaricen con el sistema o no tengan una razón para regresar al servicio.
El fenómeno social del traspaso de la memoria y LoSoMo
La memoria colectiva se da como un proceso social y natural en el que las generaciones mayores traspasan conocimiento a las generaciones más jóvenes a través de relatos, documentos, mapas, elementos multimedia como fotos y videos, entre otros.
Este acto puede tener un efecto terapéutico en las personas que vivieron hechos traumatizantes.
A la vez, en el entorno de marketing en Internet (campo en donde se desenvuelve el autor) se hace presente la tendencia llamada “LoSoMo” que son las siglas de tres elementos que se recomienda considerar en una campaña digital: “Local”, “Social” y “Mobile”. Este enfoque se puede encontrar en las más exitosas start-ups como Foursquare y Groupon. A este momento, el alcance de Memoragram sólo abarcaba el primer y último aspecto. El fenómeno de traspaso de la memoria entonces se postulaba como el elemento social para generar una comunidad activa de usuarios.
Nuevas posibilidades
La inclusión de un fenómeno social en el alcance del proyecto abrió un nuevo espectro de posibilidades para la idea de formar un museo virtual. El sistema empezó a evolucionar: pasó de un servicio en el cual los usuarios sólo consumen contenido a una plataforma donde pueden aportar con sus propias memorias, sus propios recuerdos.
Este nuevo tipo de contenido obliga a generar nuevas acciones que se pueden expresar y medir con la plataforma, por ejemplo: los usuarios exploran el contenido y pueden registrar y medir el efecto que este genera en ellos, logrando que esta medida de rankings sea otro criterio de organización de los eventos listados.
El Proyecto
Con las consideraciones listadas previamente se definió el alcance del proyecto, el cual se alcanzará a través de una serie de prototipos funcionales en proceso de perfeccionamiento.
El alance elegido
Los usuarios podrán explorar los eventos históricos más cercanos a su ubicación actual, la fecha en que sucedieron, los personajes, organizaciones y las publicaciones que las mencionan como fotografías, video, audios, archivos periodísticos, libros, películas, entre otros. Asimismo podrán dejar sus propios recuerdos usando texto, audio, fotografías e incluso video, a manera de mensaje a las futuras generaciones (recuerdos públicos).
El contenido al inicio será creado con la asesoría de especialistas basados en fuentes confiables.
La Solucion
Eligiendo el gestor de contenido base
Para construir la plataforma se requiere de un paquete de software web modular, preferentemente gratuito para ahorrar costos y open-source para aprovechar el avance de la comunidad que lo mantiene. Se eligió entonces el CMS (Sistema de gestión de contenido) Drupal 7 basado en el lenguaje PHP, el cual se conecta a una base de datos MySQL (Metaio Inc.) Este sistema ya incorpora funciones de manejo de registro de usuarios, creación de perfiles de usuario, tipos de contenido y una robusta estructura de etiquetas así como soporte de distintos tipos de archivos multimedia.
Interfaces del usuario
En la versión 7 de Drupal ya se maneja HTML versión 5, llamadas asíncronas AJAX e incluso soporte de diseño responsivo, el cual permite crear una sola interfaz que adapta el contenido mostrado según el dispositivo desde el que se accede a él.
Asimismo, se decidió experimentar con una interfaz de realidad aumentada para móviles; para ello se eligió la aplicación móvil Junaio www.junaio.com, disponible para iOS y Android, a la cual basta alimentar con contenido desde una interfaz entre servidores usando XML.
Desarrollo de prototipo
A continuación pueden observarse unas capturas de pantalla. Todas las imágenes son de elaboración propia, se realizaron desde una PC con una navegador web y en un Apple iPhone 3GS.
 
Figura 1.
Home del sitio web prototipo con mapa de los últimos eventos registrados (Mostrando un pin del mapa con burbuja de información)
 
Figura 2.
Captura de pantalla web del detalle de un evento
 
Figura 3.
Vista en vivo en interfaz de realidad aumentada móvil usando la aplicación Junaio en un iPhone 3GS desde la Plaza Mayor de Lima
 
Figura 4.
Vista de mapa usando la aplicación Junaio (navegando a través de eventos)
 
Figura 5.
Vista de lista usando la aplicación Junaio (navegando a través de eventos)
 
Figura 6.
Vista de detalle de un evento usando la aplicación Junaio (los usuarios pueden realizar distintas acciones como obtener la ruta para llegar al punto, ver la imagen, entre otras)
Conclusiones
Es técnicamente viable crear una plataforma que llegue al alcance propuesto, sin embargo, a la fecha sólo se ha implementado el acceso web y realidad aumentada móvil sin la interfaz de creación de memorias por parte de usuarios. En el futuro cercano se irán extendiendo las funcionalidades y luego se pasará a realizar pruebas de usuario para diseñar una interfaz adecuada.
References
1. PISA 2009 Results: Executive Summary (Figura 1), OECD, 2010. Consultado el 2012-11-04.http://www.oecd.org/pisa/46643496.pdf.
2. Comscore Inc. “It’s a Social World: Social Networking Leads as Top Online Activity Globally, Accounting for 1 in Every 5 Online Minutes”. Consultado el 2012-11-04. http://tinyurl.com/bgn3sln
3. BuddeComm. “Peru - Telecoms, Mobile, Broadband and Forecasts”. Consultado el 2012-11-04.http://www.budde.com.au/Research/Peru-Telecoms-Mobile-Broadband-and-Forecasts.html.
4. Comisión de la Verdad y Reconciliación del Perú. Banco de Imágenes de la Comisión de la Verdad y Reconciliación Nacional del Perú. http://www2.memoriaparalosderechoshumanos.pe/apublicas/galeria/index.php
5. Programa de las Naciones Unidas para el Desarrollo - PNUD Perú. “Yuyanapaq. Para recordar”. Consultado el 2012-11-04. http://www.pnud.org.pe/yuyanapaq/yuyanapaq.html
6. Metaio Inc. “Junaio Demo Book”. http://www.junaio.com/fileadmin/upload/documents/Promo_Booklet/DOC-junaio_promo_book-EN-DIGI.pdf",txt,This text is republished here with permission from the original rights holder.,,mobile augmented reality;mobile web;remembrance;web,English,"GLAM: galleries, libraries, archives, museums;internet / world wide web;maps and mapping;mobile applications and mobile design;software design and development;virtual and augmented reality"
1840,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,An ontology for 3D visualisation of cultural heritage,,Valeria Vitale,poster / demo / art installation,"To date, 3D computer graphics and modelling techniques have been used in the study of the ancient world mainly as a means to display traditional research. The value of these digital techniques has been often assessed merely on the degree of graphic aesthetic quality (Favro 2006).
The pursuit of photorealism has proven ineffective in engaging the audience (Champion and Dave 2007) but also misleading, as it suggests that is possible to reproduce an artefact or a scene «exactly as it was» in the past (Baker 2012). Behind every scholarly 3D visualisation is a thorough study of excavation records, iconographic documentation, ancient literary sources, artistic canons and precedents (Hermon 2008). However, this valuable research is not always detectable in the final visual outcome.
The London Charter for the Computer-based Visualisation of Cultural Heritage1 made a huge step forward in the regulation of scholarly 3D visualisation—prescribing that researchers’ choices and motivation must all be documented. No 3D model can be considered a scholarly resource if its research method is not «transparent» (Forte 2012). The London Charter presents methodological guidelines for recording this data, but does not go as far as to offer a formal framework in which to place this information; each modeller is left to simply follow their own style. Moreover, the clients who commissioned the 3D model (such as museums or other cultural institutions) are frequently more interested in the final product than in its rationale, which is often completely overlooked and not circulated (or dropped from the budget line altogether). Time and resource constraints not only affect the accuracy and availability of the documentation, but also make it very unlikely that a researcher, or even a team, develops more than one visualisation of the same cultural heritage place/object, perpetuating the naive idea that only one visualisation is possible or correct.
The growing compatibility between 3D content and web browsers2 allows the use of RDF technology to, potentially, connect the 3D model and its parts, internally with each other—identifying and defining relationships—, and externally with online information about the material remains, previous publications, primary and secondary sources, and with available alternative visualisations of the same object (that share the same controlled vocabulary).
Ontologies for cultural heritage are already commonly used in the management of museum collections and databases3. However, they tend to focus on material artefacts and to meet the specific needs of museum curators and cataloguers. Therefore, they do not seem the most suitable means to deal with digital objects (that are hypothetical representations of material objects), to state methodological relationships, or describe a scholarly process.
The proposed ontology for 3D visualisation for cultural heritage will:
• Describe the 3D digital object. After assigning a specific URI to each element of a 3D digital visualisation (@prefix “obj”: <http://hypothetical3donthology.kcl.ac.uk/objects/>), they will be associated to metadata (such as creator(s), software(s) used, formats available) and to a formal description of the cultural heritage object they represent.
For example obj:001 rdf:type art:shaft. Where @prefix “art”: <http://hypothetical3donthology.kcl.ac.uk/ArtVocabs/>4
• Describe the 3D digital object’s relationships with other 3D digital objects5. Through a dedicated namespace (@prefix “tdvo”: <http:// hypothetical3donthology.kcl.ac.uk/threedvisontology/>) it will be possible to state and describe properties, values and relationships of the 3D digital objects such as
the relationship between a 3D digital object and the file it belongs to (obj:001 tdvo:isPartof obj:3Dfile.max);
the relationships between different objects within the same file (obj:001 rdf:type art:shaft. obj:010 rdf:type art:column. obj:001 tdvo:isPartof obj:010). 
• Describe 3D digital object’s relationships with its physical referent. Through a digital geographical gazetteer such as Pleiades6, the 3D digital object will be linked to the place where the visualised building (or other referent) is located. For example, for a file “3Dfile.max” visualising the Odeon in Aphrodisias, we will have: 3Dfile.max gawd:depicts pleiades:638753/odeon. Different 3D visualisations could be connected to the physical building and be available alongside the photographic documentation linked to Pleiades via Flickr. 
• Assess and represent the level of speculation involved in the creation of each element, presenting 3D visualisation more as a scientific hypothesis than an «exact reconstruction». For example obj:001 tdvo:hasCertainty tdvo:certainty6
where the level of certainty from 6 (maximum) to 0 (minimum) would be defined as follow:
tdvo:c6 rdfs:label “Certainty 6”; rdfs:comment “the ancient element is still in situ, and its dimensions and position can be measured”.
tdvo:c5 rdfs:label “Certainty 5”; rdfs:comment “the ancient element is not in situ but it has been visually documented in the past and the documentation is still available”. tdvo:c4 rdfs:label “Certainty 4”; rdfs:comment “the ancient element is not in situ but it can be geometrically derived from the surviving elements”.
tdvo:c3 rdfs:label “Certainty 3”; rdfs:comment “the ancient element is not in situ but it can be visualised according to well accepted standards and precedents”.
tdvo:c2 rdfs:label “Certainty 2”; rdfs:comment “the element is not in situ but it can be visualised according to the modeller’s experience, knowledge, intuition”.
tdvo:c1 rdfs:label “Certainty 1”; rdfs:comment “the element is not in situ and it has been added for communicative purposes”. 
tdvo:c0 rdfs:label “Certainty 0”; rdfs:comment “the element has not been created for scholarly purpose and does not aim to historical accuracy. However, some characteristics of an original referent can still be recognised”. 
Represent the relationships between the 3D digital visualisation, its sources, referents and interpretations.
For example: tdvo:isBasedOn rdfs:label “is based on”; rdfs:comment “the shape, dimensions or decoration of the element is based on visual or written information contained in a relevant document describing established practices, standards and rules”.
The object of the predicate could be traditional bibliographical references and/or the digital URI of the source and/or the URL of a digital edition of the source such as the ones available on open digital archives (obj:001 tdvo:isBasedOn dbpedia:De_architectura). 
tdvo:hasEvidenceIn rdfs:label “has evidence in”; rdfs:comment “the 3D element can be compared with specific verbal or visual evidence such as video/photographic documentation or official excavation records”.
The object of this predicate would be archive numbers or bibliographical references identifying physical documents or artefacts, and/or URIs of digital reproductions of them, available on digital databases such as Arachne7 or CLAROS8. For example, if obj 002 was an element of the 3D visualisation of the Basilica in Pompeii:
obj:002 tdvo:hasEvidenceIn <http://arachne.uni-koeln.de/item/marbilder/2015507>. 
tdvo:isMentionedIn rdfs:label “is mentioned in”; rdfs:comment “the visualised building
(or part of it) is mentioned in a ancient (or modern) text”.
tdvo:isDescribedIn rdfs:label “is described in”; rdfs:comment “the visualised building (or part of it) is described in a ancient (or modern) text”.
The latter predicates could link to bibliographical references and/or to the digital version of ancient texts such as the ones available through Perseus Library9.
The main goal of this proposal is not to present a detailed ontology, but to show the potential of the application of Open Linked Data to 3D visualisation, and how such an interaction will change the way 3D visualisation is applied in the study and understanding of cultural heritage. The ontology itself is not meant to be the work of a single researcher, but the collaborative effort of the different communities of practitioners.
To summarise, the suggested ontology will:
constrain and standardise the documentation, making it synthetic instead of verbose;
speed up the recording process thus reducing time/cost and making the documentation more likely to be retained in projects’ budgets;
allow 3D visualisations to join and enrich the growing network of linked digital resources to study the past; 
make 3D visualisations human- and machine-searchable, connecting them with the literary and historical sources that mention the visualised artefact or building;
allow and encourage comparison of different visualisations and interpretations of cultural heritage, as the same resource (historical, archaeological, literary) will be connected to all the related visualisations that share the same vocabulary;
allow citations, re-use and peer-review of 3D visualisations, as every 3D element (and its author) will always be identifiable and linkable through the URI;
contribute to transform 3D visualisation from a univocal display of traditional research to a collaborative virtual environment where different scholars work together not only to implement the content but also to refine the ontology itself. 
References

1. www.londoncharter.org
2. Thanks, for example to APIs such as OpenGL and WebGL
3. Cf., for example, CIDOC CRM or FRBRoo.
4. There are a few examples of formal vocabularies dedicated to art and architecture that can be used to describe the represented object. The Thesaurus developed by the Getty foundation, for example, has recently been released in Open Linked Data format (February 2014).
5. An interesting and useful precedent in using RDF to describe aggregations of files can be found in the open archive initiative.
6. pleiades.stoa.org
7. arachne.uni-koeln.de/drupal
8. www.clarosnet.org/XDB/ASP/clarosHome
9. www.perseus.tufts.edu/hopper
Baker, D. (2012). Defining Paradata in Heritage Visualization. In Bentkowska-Kafel,
A., Denard, H. and Baker, D. (eds) (2012). Paradata and Transparency in Virtual Heritage. Farnham: Ashgate
Champion, E. and Dave, B. (2007) Dialing Up the Past. In Cameron, F. and Kenderdine, S. (eds) (2007). Theorizing Digital Cultural Heritage. A critical discourse. Cambridge, MA: MIT Press
Favro, D. (2006) In the eyes of the beholder: Virtual Reality Recreations and academia. Journal of Roman Archaeology Supplementary Series Number 61(2006): 321-334
Forte, M. (2012) Cyberarchaeology: a Post-Virtual Perspective. www.academia.edu/3573281/Cyberarchaeology_a_Post-Virtual_Perspective (accessed 6th March 2014)
Hermon, S. (2008) Reasoning in 3D: a Critical appraisal of the role of 3D modelling and virtual reconstructions in archaeology. In Frisher, B, and Dakouri-Hild, A. (eds) Beyond Illustration: 2D and 3D Digital Technologies as Tools for Discovery in Archaeology. British Archaeological Reports International Series.
Online Resources:
The CIDOC Conceptual Reference Model:www.cidoc-crm.org (accessed 6th March 2014)
Getty Vocabularies as Linked Open Data:www.getty.edu/research/tools vocabularies/lod/index.html (accessed 6th March 2014)
The London Charter:www.londoncharter.org (accessed 6th March 2014)
Open Archives Initiative:www.openarchives.org (accessed 6th March 2014)
Pleiades Gazetteer:pleiades.stoa.org (accessed 6th March 2014)",txt,This text is republished here with permission from the original rights holder.,,3d visualisation;cultural heritage;linked data;ontologies,English,archaeology;ontologies;semantic web;virtual and augmented reality
1863,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Building the Princeton Prosody Archive,,Meredith Martin;Grant Wythoff;Meagan Wilson;Travis Brown,poster / demo / art installation,"1. Introduction

The Princeton Prosody Archive (PPA) is a full-text searchable database of nearly 10,000 digitized texts – comprising 800 million words – on prosody published in English between 1750 and 1923. During the 2012-2013 academic year, a grant from the Mellon Foundation supported the completion of the PPA’s first phase. This poster will reflect on the outcomes of the start-up stage, as well as some of the challenges and opportunities the PPA anticipates as the digital collection expands. Conference participants are encouraged to visit prosody.princeton.edu to access the Archive’s beta-site.
2. Prosody and Historical Poetics

In the nineteenth century, “prosody” – which refers to both pronunciation and the technicalities of versification – was codified as the fourth section of the grammar book, after “orthography,” “etymology,” and “syntax.” By the early twentieth century, “prosody” referred primarily to versification. In recent years, scholars of English literature have begun questioning the uniformity of poetic terminology, recognizing terms such as “prosody,” “meter, “tone,” or “rhythm” as culturally determined and fundamentally unstable concepts that have shifted through the centuries. By turning to historical texts, they are tracing how inherited notions of poetic form developed over time, and in turn, painting a more accurate picture of the evolution of English-language discourse on poetics.
As the field of historical poetics has grown, so too has our access to nineteenth-century materials through online archives. The majority of these digital resources, however, are primarily focused on prose works, and thus both technological and scholarly innovation has been made in the field of prose. The PPA is filling the gap as the only digital archive dedicated to the study of poetics, writ large, and allowing scholars to practice the kind of broad-view historical research the field demands. The PPA aggregates foundational texts in the history of poetics, reviews of these texts, debates about poetics in the public press, and grammar books and poetic handbooks that present contrary definitions and views of poetics so that “big questions” about literary movements and culture can be posed. With this large data-set, we can now ask: How did the changing science of linguistics and increased impulse toward education impact discussions of poetry over time? How often were particular poets used as examples in poetic pedagogy? How, when, and why did certain poetic terms and genres came in and out of use? 
3. Methodology

The PPA partnered with Google Books and HathiTrust in 2011, and the collection is currently composed of works digitized by Google and Hathi.[1] In 2013, the PPA began to develop a beta-site, which, though still under development, allows users to browse, search, and correct its content. To best serve its user community, the PPA functions as a freely-available, user-friendly repository, a trusted scholarly reference source, and a creative workspace that enhances traditional scholarly practices and pedagogies while enabling new ones.
3.1 Curation: Google Books and the HathiTrust Digital Library

The PPA’s initial corpus was selected from the holdings of the HathiTrust Digital Library. We began by gathering every out-of-copyright text referred to by prosody scholar T.V.F. Brogan in his annotated bibliography English Versification, 1570-1980 that had been digitized.[2] Though the availability of Hathi’s digital facsimiles and transcriptions is incredibly valuable, some aspects of their digitization and description present serious technical obstacles to the kinds of analysis the PPA intends to support. The most obvious is that the transcriptions were prepared by a range of Optical Character Recognition (OCR) systems, and few (if any) were hand corrected. Most were digitized as part of the Google Books program, whose OCR tools are not tailored to the vocabulary, orthographic conventions, or typefaces of eighteenth and nineteenth century texts. They were generally unable to capture indentation, italicization, or other formatting, variations in font size, or diacritical marks, not to mention musical notation or non-standard marks.
3.2 OCR and Diacritic Correction: Representing Scansion

When dealing with texts on prosody and versification, accurate representation of diacritics and typographical marks is particularly important. How do you render musical annotation, scansion, line spacing, or iambic markings, for example, into plain text? Because of the focus on notation and the transmission of concepts and terms, particular care must be taken to ensure that these issues do not interfere with (or silently distort) scholarly analysis. To that end, we are developing a model for encoding scansion – the non-textual elements such as musical notation, macron, breve, or other diacritics, including non-standard marks created by the many scholars who attempted to invent prosodic systems in English. Moreover, we will employ the kind of OCR that retains document coordinates for individual characters whose position on the page often conveys important information.
3.3 Metadata Correction: Scholarly Re-use and Linking Data

The metadata we ingest from HathiTrust also presents challenges. One of the PPA’s goals is to allow researchers to trace the development of prosodic discourse across time and place, and the ability to support this functionality depends on consistent and reliable metadata. While the HathiTrust provides the Machine-Readable Cataloging (MARC) records that have been supplied by contributing libraries, the fields indicating the place and date of publication are free text and vary widely in their conventions of encoding. In the PPA’s start-up phase, we developed an application that assembles text and metadata from the HathiTrust Digital Library, performs some initial automated correction, and loads the text and metadata into a Drupal 7 installation, where it can be browsed, searched, and corrected by scholars working with the Archive. Corrections to metadata can be credited to registered and authenticated users, and metadata fields can now even be versioned, using Drupal 7’s native revision control. In this initial phase, however, these corrections are essentially locked in the Drupal data store; they cannot be returned to the HathiTrust Digital Library or conveniently shared with other scholars working with the same HathiTrust volumes in other contexts. Going forward, the PPA will explore possibilities for enacting a workflow on its own metadata, engaging in the correction of HathiTrust metadata and connecting those corrections to linked data resources by working with the Maryland Institute for Technology in the Humanities and the Foreign Literatures in America project.
3.4 Connecting Prosody Networks: Topic Modeling and Visualization

Topic modeling, and specifically Latent Dirichlet allocation (LDA) has received attention in the digital humanities community over the past several years, in part because it is an unsupervised method – it does not require expensive training material or elaborate encodings – and also because it is relatively robust against textual errors. We have begun experimenting with LDA, not only to return a set of “topics” (which are simply distributions over the vocabulary) that often characterize the semantic and thematic composition of the PPA’s corpus in compelling ways, but also as a means by which we can identify mistranscription, special characters, and even musical notation. We also plan to begin experimenting with visualization tools in the following ways: 1) Plotting temporal and geographical metadata; tools such as Google Earth, MIT’s SIMILE, and Leaflet offer practical and intuitive ways to allow users to navigate temporally and geographically situated data sets interactively – for example, to view a three-dimensional chart on a globe indicating the relative prominence of cities as places of publication while moving a time slider through several centuries; 2) Mapping the documents in the corpus by its topical or lexical spaces; here, each document is represented as a point in a high-dimensional space, where the dimensions of the space are features such as counts or frequencies of individual words or n-grams, or the percentage of words allocated to a particular topic in a topic model; 3) tracking discursive networks by quotation identification and citation extraction; for example, the quotation of exemplars could be represented as a bimodal network, with nodes representing both volumes in the archive and lines of verse, and with edges from the former to the latter indicating instances of quotation.
3.5 Sharing Results

The PPA is committed to providing models so that other digital humanists struggling with the question of how to organize and present their own Hathi collections (in their research or in the classroom). Though these scholars might not be subject area experts in prosody or historical poetics, we would like to provide enough information that we might navigate unspecialized visitors through the corpus and share ideas about how they might build similar archives themselves. 
Notes

[1]  We negotiated a Google Distribution Agreement between the Princeton University Library, Princeton Counsel, and HathiTrust that allowed us to access, download, and host all of this data on our own servers.  A spreadsheet of all Archive monographs is available online at “Princeton Prosody Archive Database.” The PPA’s four collections can also be accessed through the HathiTrust site. See: 1) “Brogan's English Versification, 1570-1980” (578 works); 2) “Prosody Archive” (1,308 works); 3) “PPA Subject Search” (6,991 works); and 4) “Graphically/Typographically Unique“ (26 works set aside as possessing especially complex page images that would be misread by OCR).
[2] Brogan, Terry V. F. English Versification, 1570-1980: A Reference Guide with a Global Appendix. Baltimore: Johns Hopkins University Press, 1981.",txt,This text is republished here with permission from the original rights holder.,,archives;institutional support;poetics;poetry;prosody,English,"archives, repositories, sustainability and preservation;databases & dbms;digital humanities - institutional support;genre-specific studies: prose, poetry, drama;literary studies;prosodic studies"
1890,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,CURIOS: Connecting and Empowering Community Heritage through Linked Data,,David Beel;Gemma Webster;Christopher Mellish;Claire Wallace,"paper, specified ""long paper""","1. Moving Towards Community Digital Heritage

Rural areas are characterised by a strong identity of people with place. These identities draw on a repertoire of cultural norms, knowledge, histories, customs and practices which, taken together, construct unique place identities. This cultural distinctiveness is dynamic given traditional cultural practices are reproduced and others introduced as cultural systems evolve and adapt. Forms of cultural expression, such as story-telling, music and song, poetry and literature, dance and drama together with material objects, artefacts, sites and cultural spaces, are resources for interacting with the past and for experiencing the present. In the collection and transmission of these collections there has been a growing sense that the traditional methods for doing this are failing, Nora [1]. In order to address this problem, digital solutions have been sought but this has been a problematic process due to a number of variances. These include the constant changing of file types, software and codes of best practice, as well problems to do with cost and the sheer amounts of ‘analogue’ data to convert. Leading the way in this process have been national institutions but with the production of such local cultural repertoires, which as Flynn [2] suggests ‘are the grassroots activities’ where ‘control and ownership of the project is essential’ there has been a failure to consider the needs of community heritage groups in these processes. As such groups do not want to be subsumed into national archives, which they do not control, is not sensitive to their needs and is juxtaposed ideologically to the production of their own ‘place history’. Following Creswell’s [3] claim that such archives represent ‘spaces  of marginalized memory’ CURIOS is therefore seeking a solution using open linked data in which a system can be developed that is attuned to the specificity of a local heritage but can also take advantage of already collected materials from elsewhere.
2. Case Study – Hebridean Connections

In the past 40 years around 22 ‘Comainn Eachdraidh’[1] (CE), have been established in the Outer Hebrides[2]. CE are community run groups that began in the 1970’s with a very specific political and cultural purpose – to preserve the culture, history and language of the primarily Gaelic regions of Scotland. Such community heritage practices have been described as a ‘messy’ endeavour with a wide variety of different formal and informal practices [5]. The archives embrace different registers of social memory from tangible to intangible heritage, which have been collected and ordered in a variety of different ways. Different CE groups collect and order their archives in a variety of different ways: from the highly ‘professional’ to the more bespoke and sporadic. As the CE groups are voluntary community archives, they are rooted in local historical values, hence there is often little consistency between groups regarding cataloguing, archiving and content management.
Hebridean Connections (HC), which is a community managed, online historical resource was formed due to the driving force of a single member of a CE who saw the benefit of digitising and connecting the different historical catalogues [5]. The idea was proposed to the different CE and four groups were actively involved in a Heritage Lottery Fund (HLF) bid that funded the creation of the HC website[3]. The project website was launched in 2006, holding some 100,000 records relating to the genealogy, history, archaeology, and cultural traditions of the Outer Hebrides. Currently, the system allows users to search using keywords, selecting relevant images, or with a map-based interface. Additionally, the website encourages contributions from its users and, therefore, has the potential to foster reciprocal knowledge exchange across geographical boundaries.
 2.1 Sustainability

HC is one example of a community-built digital cultural heritage repository where their long-term future is unclear. Many issues with the current system have arisen since the initial grant, particularly surrounding funding and scalability. There is a real practical question about how a project of this kind can be maintained over time with the resources available to a small-dispersed community, especially as the initial system was developed by a private development company, using proprietary software. As the project developed, this situation raised the problem that any changes to the system required more financial investment in the software. For the small community heritage groups involved, this was not feasible, especially as the CE became aware of what was possible through digitisation and wanted to expand. The process of digitisation has created three primary issues for HC:
How to expand the project remit without additional funding for developers?
Scalability issues, how can more CE collections be integrated in a closed system
Can empowering communities to control their own digital heritage improve long-term sustainability?
2.2 An Archive for the Future?

Motivated by the limitations of the current HC system, the CURIOS project’s aim is to produce a sustainable system that allows a community of users to manage a digital archive of cultural heritage data, or ‘cultural repository’, releasing them from any specific proprietary software platform. To achieve this goal, CURIOS has made use of existing open source content management system (CMS) software and Semantic Web standards. The emergence of the Semantic Web [6] has led to several standard formats for representing and interchanging data [7, 8]. By making use of linked data, cultural repositories would have the potential for reuse and integration with further related data sources.
In recent years content management systems have gained popularity on the web by allowing users to build and publish web pages without requiring in-depth knowledge of the underlying web technologies. The CURIOS project has extended the web CMS approach to allow users to manage repositories of linked data. This Linked Data CMS approach makes use of existing CMS software to retain the usability and scalability of existing tools that are familiar to users, whilst allowing the users to exploit the benefits of linked data.
The Linked Data CMS approach has been implemented as a module for the popular open source Drupal CMS[1]. Building the next generation of Hebridean Connections on open source software and web standards has distinct advantages for future development and use of the system. The Drupal-based system can be maintained by its community of users and can be extended additional functionality developed by the Drupal open source community, e.g., to support blogging or e-commerce features. This community led maintenance allows for further future development of the cultural repositories as the archives develop.
3. Conclusion

Open linked data can help make local cultural repositories sustainable and collective. Linked data allows for collaboration, mutual authoring, distributed responsibilities through community projects and the utilisation of other community or national resources [4]. The CURIOS project is enabling local cultural heritage repositories to become a meaningful identity resource for an international community, who previously had no access to them. By falling outside of national institutional frameworks, local people are the 'gatekeepers' of their own heritage and are selecting what to commemorate based on their own customs of remembering. This kind of digital archive can have, therefore, potentially significant social impacts which need to be better understood. The vision of Hebridean Connections is to expand the collections to incorporate those held by other Comainn Eachdraidh. Additionally, by making use of linked data, there is now the possibility to integrate further sources of data into HC from other historical societies or even national organisations. 
4. Acknowledgments

We would like to thank Hebridean Connections and the Comainn Eachdraidh for their ongoing commitment to this research. This work is supported by the Rural Digital Economy Research Hub (EPSRC EP/G066051/1).
References

Drupal is a popular open source web content management system: drupal.org.
Comainn Eachdraidh is a Gaelic phrase meaning ‘Historical Society’.
The Outer Hebrides is a group of islands off the West coast of mainland Scotland.
The Hebridean Connections website is hosted at www.hebrideanconnections.com.
Nora, P. (1996). Realms of memory: rethinking the French past. Volume 1: Conflicts and Divisions. Columbia: University Press.
Flinn, A. (2007). Community Histories, Community Archives: Some Opportunities and Challenges 1 in Journal of the Society of Archivists. Volume 28, Issue 2.
Creswell, T. (2012) Value, gleaning and the archive at Maxwell Street, Chicago. Transactions of the Institute of British Geographers. Vol. 37 (1),1-13.
Mellish, C., Wallace, C., Tait, E., Hunter, C., & MacLeod, M. (2011). Can Digital Technologies increase Engagement with Community History?Digital Engagement 2011. de2011.computing.dundee.ac.uk/wp-content/uploads/2011/10/Can-Digital-Technologies-increase-Engagement-with-Community-History.pdf
Wallace, C., Tait, E., MacLeod, M., Mellish, C., & Hunter, C. (2011). Supporting Digital Humanities Creating Sustainable Digital Community Heritage Resources Using Linked Data. In Supporting Digital Humanities: Answering theunaskable Conference. 17–18.
Spector, A. Z. (1989). Achieving application requirements. In Distributed Systems, S. Mullender, Ed. ACM Press Frontier Series. ACM, New York, NY, 19-33. DOI= doi.acm.org/10.1145/90417.90738.
Berners-Lee, T., Hendler, J., and Lassila. O. (2001). The Semantic Web. Scientific American, 284(5), 34–43
Hitzler, P., Krötzsch, M., Parsia, B., Patel-Schneider, P.F., and Rudolph, S. (2009). (eds.) OWL 2 Web Ontology Language: Primer. W3C Recommendation, www.w3.org/TR/owl2-primer.",txt,This text is republished here with permission from the original rights holder.,,community heritage;cultural heritage;digital archives;open linked data,English,"cultural infrastructure;cultural studies;digitisation, resource creation, and discovery;folklore and oral history;information architecture;interdisciplinary collaboration;semantic web;software design and development"
1978,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Lacuna Stories: Building an Annotation Platform for Historical Thinking,,Michael Widner;Brian Johnsrud,"paper, specified ""short paper""","Participatory and collaborative sense-making of complex phenomena is central to productive learning and knowledge work in today’s information-rich world.1 The Lacuna Stories Project creates an exploratory, interactive, and collaborative online space where users can research and discuss significant historical events like 9/11. Lacuna Stories draws together primary source documents, fiction, scholarship, wikis, and user-generated forums and blogs. The online space extends current digital annotation software with functionality that encourages skills such as historical thinking, close reading, and comparison of media and sources concerning 9/11. When approached independently, individual sources, genres, and media inevitably fall short of stitching together the “whole story.” The Lacuna Stories Project’s diverse, multimedia environment provides tools for instructors, students, and the general public to “mend” the gaps in our knowledge of major historical events in order to develop their own narratives. User data generated through the site’s design also will allow researchers to compare and better understand reading and engagement behaviors of students; along with other forms of user experience research and assessment, this research also provides directions for improving the platform and instructional materials.

The Lacuna Stories Project is a cross-disciplinary collaboration that includes faculty whose research interests span literature, pedagogy, historical thinking, public humanities, and platform studies. This short paper will describe the research and pedagogical goals of the Lacuna Stories Project as well as the technological innovations developed to support these goals. By the time of the conference, Amir Eshel (PI) and Brian Johnsrud (Project Manager) will have taught a course during Stanford's Winter Quarter in which students use the Lacuna Stories Platform in and outside the classroom. Johnsrud and Michael Widner (Technology Director) will also have taught a course in the same quarter on building in the digital humanities that will use Lacuna Stories as its primary example. This paper will touch upon how the prototype encouraged student learning and collaboration by presenting our data gathered from student use, interviews, and focus groups. By the time of the conference will also have piloted Lacuna Stories for a test-group of up to 30 public users without a college degree to compare experiences of different kinds of users in and out of the classroom, with and without formal academic training in approaching different kinds of historical texts and media.

Compared to other annotation and archive projects, the Lacuna Stories platform provides three key innovations. First, it creates an integrated multimedia environment that encourages the development of core skills for learning and knowledge work: navigation, critical reflection, linking, synthesis, and collaborative sense-making. Second, no existing digital annotation tool connects multiple types of media together to compare and generate new narratives. We are coordinating with MIT's HyperStudio to add this functionality to Annotation Studio (www.annotationstudio.org) and incorporate it into an ecosystem of digital tools for collaborative learning. Third, Lacuna Stories will provide a novel, curated set of diverse 9/11 resources for users to engage with and connect in innovative ways.


Fig. 1: Annotation Functionality

Lacuna Stories is also a platform that fosters good habits of close reading and thinking historically, whether the users are students, researchers, or members the general public. Within the humanities specifically, the interactive, multimedia functionality of Lacuna Stories goes beyond simply replacing print reading and viewing practices; rather, it creates new and innovative experiences for engaging with various texts and media that reflects the networked state of knowledge today. The platform thus builds upon the work done by Sam Wineburg, another member of the project team and Margaret Jacks Professor of Education and (by courtesy) of History at Stanford, to promote historical thinking (sheg.stanford.edu). Wineburg's work to date, however, has been focused on print resources in high school classrooms. Lacuna Stories will bring Wineburg's deep engagement with these matters to digital resources and make them available in the university setting.

The project also seeks to develop an inclusive, empowering, and engaging open-source platform to gather and encourage these responses in a generative and reparative mode. The site aims not to develop a fully coherent or conclusive “truth” of the event, but to encourage the cognitive and imaginative work that inspires responses to and stories about the event, its complexity, and its diverse meanings. Lacuna Stories’ subtitle, “mend the truth,” refers to site’s ability to connect in novel ways the different text, media, and user-generated content. One of our primary contributions to the development of Annotation Studio will be to enable all aspects of the available resources—from images to individual words, lines, or documents to user-generated content—to be archived or collected by registered users into their personal “sewing kit,” which provides users a workspace for the collection, connection, and annotation of materials relevant to their learning and scholarship.

We will incorporate and extend pre-existing open-source projects for the platform, most notably Annotation Studio from MIT’s Hyperstudio group. Annotation Studio is an exemplary, user-centered tool for digital humanities work, currently under active development in conjunction with a wider set of projects to develop shared standards for annotation of multimedia content, a key requirement for widespread adoption both in and outside of classrooms. We are working closely with the Annotation Studio team to ensure that the innovations developed for Lacuna Stories make their way back into the central code base and are thus available to the broadest possible number of users and institutions. One of the core technologies powering Annotation Studio is the javascript library Annotator.js (okfnlabs.org/annotator), a project of the Open Knowledge Foundation that is quickly becoming one of the most popular annotation technologies. By focusing first on developing extensions to Annotator.js, the Lacuna Stories Project ensures that our work will be useable in Annotation Studio and by any other projects that use Annotator.js.

We are also working to bring the functionality provided by Annotator.js into the Drupal platform that powers much of the rest of the Lacuna Stories site. Once this work is complete, users will be able to annotate not only texts available through Annotation Studio, but also blog posts, wiki entries, and any other content in a single, integrated environment. In our talk, we will discuss some of the challenges in the prototype phase for this work and our reasons for using Annotation Studio as a replacement until the Drupal work is complete.

O’Malley and Rosenzweig argue for the growing importance of the web generally because it allows for communication and exchange of divergent interpretations of the past. The web demonstrates how “meaning emerges in dialogue and that culture has no stable center, but rather proceeds from multiple ‘nodes’” (154).2 Being able to create links between annotations and sources and annotate the quality of those connections is central to the academic process of synthesizing information across documents and reflects the natural associative mechanisms that are central to deep learning.3 4

This functionality, however, does not exist in any current digital annotation tools. Lacuna Stories seeks to change this fact, with a tool that is scalable for use in a multitude of sense-making settings. Lacuna Stories will allow users to create categories and links between items in their kit, such as connecting a line from a novel with a paragraph from a user-submitted story, a forum discussion thread, and a section from the 9/11 Commission Report. These links can additionally be connected to a larger theme as described by the user; there can also be a shared set of themes developed collectively by the group or by an instructor. Social learning will be enabled through opt-in sharing functionality, where other learners can view and extend links and notes. Such open linking from users’ digital “sewing kits” exemplifies the idea that connections among narratives can be made quickly and simply, empowering users to “mend,” create, or share meaningful associations. Moreover, this aspect of the project responds to the work done by Fred Turner (Associate Professor of Communication), another faculty member of the team, to create digital humanities projects that are public-facing and that encourage community engagement.

Lacuna Stories is, then, a platform driven by the complementary research interests of a cross-disciplinary team of faculty made possible through technological innovation based on existing, open source tools. Although this paper will focus primarily upon the technology used and plans for future work, a secondary focus will be how the tools and innovations are grounded in research and pedagogy and how these interests influenced our technology choices and strategy.

References
1. U.S. Department of Education. (2010). National Education Technology Plan. Retrieved March 8, 2013, from www.ed.gov/technology/netp-2010

2. O’Malley, M. and Rosenzweig, R. (1997). Brave New World or Blind Alley? American History on the World Wide Web. Journal of American History 84(1): 132–55.

3. Tashman, C.S. and Edwards, W.K. (2011). Active reading and its discontents: the situations, problems and ideas of readers. In Proceedings of CHI ’11, 2927–2936. CHI ’11. ACM.

4. Marshall, C. (2005). Reading and Interactivity in the Digital Library: Creating an Experience that Transcends Paper. In Digital Library Development: The View from Kanazawa, D. Marcum and G. George, Eds.",txt,This text is republished here with permission from the original rights holder.,,annotation;historical thinking;multimedia;platforms,English,"audio, video, multimedia;digital humanities - pedagogy and curriculum;historical studies;interdisciplinary collaboration;linking and annotation;teaching and pedagogy"
2053,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Scholarly primitives revisited: towards a practical taxonomy of digital humanities research activities and objects,,Luise Borek;Quinn Dombrowski;Matt Munson;Jody Perkins;Christof Schöch,"paper, specified ""short paper""","1. Introduction
Today we have more information at our fingertips than at any other time in human history. The problem is no longer finding information, the problem is being overwhelmed with the amount of information. This is no different in the realm of the digital humanities.  Information on people, projects, resources, methods, and tools exists in quantity everywhere we look, and yet we still have difficulty finding what we need. This paper will describe a transatlantic effort on the part of DiRT in the United States and DARIAH in Europe to construct a taxonomy of scholarly methods, that can be used not only to organize single collections of DH information and resources but also to allow these collections to interface with each other, creating a web of linked data that can be effectively searched for information across distributed collections. DiRT and DARIAH are not trying to impose a restrictive, monolithic scheme on DH; rather, our goal is to construct a lightweight, basic taxonomy of higher order goals and first-order methods that can be easily expanded in all directions by linking lower order techniques to multiple goals and/or methods to create machine-readable paths among the various resources. In building this taxonomy, we heavily rely on input and feedback from the digital humanities community. Still, at least for the intended use cases, we believe a stable taxonomy has advantages over more open, folksonomy-based solutions.

The taxonomy as it exists now is based upon three primary sources: 1) the arts-humanities.net taxonomy of tools of DH projects, tools, centers, and other resources, especially as it has been expanded by digital.humanities@oxford in the UK and DRAPIer in Ireland; 2) the DiRT collection of digital research tools, re-launched under Project Bamboo in the US but now continuing on after the end of that project; and 3) the DARIAH ‘Doing Digital Humanities’ Zotero bibliography of literature on all facets of DH. These resources were studied and distilled into their essential parts, producing a simplified taxonomy of two levels: 8 top-level goals that are broadly based on the steps of the scholarly research process and a number of general methods under these goals that are typically used by scholars to achieve these research goals. The updating of the taxonomy and the definition of the types of relationships to be described in the resulting ontology will be carried out by a joint working group in the DARIAH-EU and the NeDiMAH projects in Europe, which will conduct large scale desk and field research into scholarly practice to determine how best to describe the relationships between and among the goals, methods, and techniques of scholarly practice.  The future expansion of this organizational system will not be as a hierarchical taxonomy but, instead, as a linked ontology as lower-level techniques are attached to one or more methods, linking all the existing entities in the ontology together. The projects and collections that use this schema will play an important role here: as resources are added to these collections and linked to the taxonomy, the resulting ontology will grow in complexity.  This complexity will be more help than hindrance precisely because it will be a machine-actionable complexity.  Computers will traverse this web of relationships for us, only bringing back results that are closely related to our needs.

This may seem excessively optimistic, but this paper will support these claims by describing three very different types of resources that have used and expanded the taxonomy not only to improve the findability within their own collections but, more importantly, to link to each other in a machine-actionable way. These resources are the DiRT directory of digital tools, the DARIAH ‘Doing Digital Humanities’ bibliography, and the DARIAH-DE service-oriented project portal. A brief description of each of these collections and how they will profit from this taxonomy/ontology follows.

2. DiRT
DiRT (Digital Research Tools, http://dirt.projectbamboo.org) is a longstanding US-based directory for scholars interested in digital tools, which provides basic information about software that can facilitate the research process at different stages. The classification of tools by category has always been fundamental to DiRT: in its earlier incarnation as a wiki, wiki pages each corresponded to a category of tools, and the tools were presented in a list on the page. In 2011, DiRT was rebuilt using the Drupal content management system, which allowed information about each tool to be stored in a structured manner that enables faceted search and browsing. While users can now create complex queries on DiRT (e.g. using operating system and price to narrow their results), tool categories remain the primary way of navigating the site.

With support from the Andrew W. Mellon Foundation, DiRT is currently undergoing a new phase of development, with the goal of making information about digital tools available outside the DiRT directory itself. Since its inception, DiRT has used its own ad-hoc list of categories. All tools must belong to at least one category, though these categories can be supplemented with user-generated tags. The shortcomings of DiRT’s categories list can be illustrated through the example of OCR tools-- some are classified as “transcription”, others as “conversion”, and while neither is ideal, both are a reasonable approximation given the other options. Replacing DiRT’s former categories with the taxonomy will improve the consistency and quality of the data, and also provide a shared facet that can connect DiRT’s tool data with information provided by other projects, once DiRT’s contents are made available using RDF.

3. 'Doing Digital Humanities' bibliography
Another resource directly connected to the taxonomy is DARIAH-DE's ‘Doing Digital Humanities’ bibliography. The bibliography can be accessed on Zotero (www.zotero.org/groups/113737) or on the DARIAH-DE portal (https://de.dariah.eu/bibliography). Like DiRT, the bibliography is one of the seed activities for the taxonomy at the same time as being one of the already defined use cases, representing the application domain of making medium-sized collections of bibliographic references discoverable. This Zotero-based bibliography offers suggestions for introductory readings as well as more in-depth coverage of research literature in various areas of digital research, teaching and infrastructure planning in and for the humanities. The bibliography is carefully curated collaboratively, is freely accessible, currently has around 800 entries, and is being updated continuously.

Right now, the bibliography is already divided into thematic collections based on the ""goals"" defined in the taxonomy. Each collection, hence, covers one prototypical aspect or goal of the research process in the humanities as it is practiced with digital tools, methods and data. In addition, all entries in the bibliography are discoverable through keywords covering, on the one hand, typical research methods and activities in the humanities, and on the other hand, a wide range of objects of research. The current closed list of keyword represents an early draft version of the taxonomy described here.

Once a first stable version of the taxonomy is available, the bibliography's keyword implementation will be updated. Sharing a keyword system with other projects will make it easier for users to find related resources. And the public documentation of the taxonomy, including concise scope notes for all methods and techniques, will make the bibliography's keyword-based search more transparent and increase its usability.

4. DARIAH-DE portal
A third use case aims to examine the taxonomy as a functional structure for DARIAH-DE’s service-oriented website, the DARIAH-DE-Portal. Launched in a first version in May 2013, it will receive a makeover in the early stage of the upcoming German DARIAH II project scheduled for March 2014 that is based on the taxonomy.

The website is designed to offer a wide range of services concerning Digital Humanities in Germany and addresses both researchers who already work digitally and those seeking information or advice. The services provided are as heterogeneous as the DH landscape. They cover informational aspects on specific research projects, information on DH Centers, Bachelor/Master Programmes and tools as well as their documentation, tutorials and teaching materials. Services offered by DARIAH-DE (like the embedded bibliography mentioned above, the DARIAH-DE Working Papers, or hosting services and a developer’s portal) are complemented by external resources like blogs and a DH-calendar (a cooperation with calenda.org being currently on its way).

The variety of this content leads to multi-purpose requirements that enable a flexible access to information relevant to individual users. This use case meets that challenge by implementing the taxonomy in RDF, thus interlinking content and making it multi-purpose. In that way, the taxonomy will function as a ‘meta-service’ that meets the interests of an active and interlinked community, that visualizes Digital Humanities and promotes its results.

 5. Conclusion
The purpose of this talk is not to convince the audience that we in DiRT and DARIAH have all the right answers.  Instead, it is to continue a conversation about the importance of ontologies for managing the over-abundance of DH information, present our own work on this problem and our approach to gathering and incorporating community feedback, in hopes of spurring further work in this area.

References
Anderson, Sheila; Tobias Blanke; Stuart Dunn. (2010). Methodological Commons: Arts and Humanities E-Science Fundamentals. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 368, no. 1925 (2010): 3779 –3796. http://rsta.royalsocietypublishing.org/content/368/1925/3779.abstract.

Benardou, Agiatis, Panos Constantopoulos, Costis Dallas, and Dimitris Gavrilis. Understanding the Information Requirements of Arts and Humanities Scholarship. International Journal of Digital Curation 5, no. 1 (June 22, 2010): 18–33. doi:10.2218/ijdc.v5i1.141.

Borgman, Christine (2010). Scholarship in the Digital Age : Information, Infrastructure, and the Internet. Cambridge: MIT Press.

CLIR (Commission on Cyberinfrastructure for the Humanities and Social Sciences). (2006). Our Cultural Commonwealth: The Report of the American Council of Learned Societies Commission on Cyberinfrastructure for the Humanities and Social Sciences. New York: American Council of Learned Societies, 2006.

Gasteiner, Martin, and Peter Haber, eds. (2010). Digitale Arbeitstechniken für die Geistes- und Kulturwissenschaften. Vienna: UTB. http://www.utb-shop.de/digitale-arbeitstechniken.html.

Reiche, Ruth; Rainer Becker; Michael Bender; Matthew Munson; Stefan Schmunk; Christof Schöch. (2014). Verfahren der Digital Humanities in den Geistes- und Kulturwissenschaften. DARIAH-DE Working Papers Nr. 4. Göttingen: DARIAH-DE (to appear). Preprint: https://dev2.dariah.eu/wiki/download/attachments/2295542/M223_DH-Verfahren.pdf.

Siemens, Ray; John Unsworth; Susan Schreibman, eds. (2004). A Companion to Digital Humanities. Hardcover. Oxford: Blackwell. http://www.digitalhumanities.org/companion/.

Unsworth, John. (2000). Scholarly Primitives: What Methods Do Humanities Researchers Have in Common, and How Might Our Tools Reflect This? London: King’s College London. http://www3.isrl.illinois.edu/~unsworth/Kings.5-00/primitives.html.",txt,This text is republished here with permission from the original rights holder.,,linked data,English,crowdsourcing;metadata;ontologies;semantic web
2057,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Semantic Blumenbach: Exploration of text-object relationship with sematic web technologies in the history of science,,Jorg Wettlaufer,poster / demo / art installation,"Blumenbach-online, a project of the Göttingen Academy of Sciences and Humanities, started in January 2010 and aims at both digitizing and presenting the writings and collections of the influential Gottingen physician and naturalist Johann Friedrich Blumenbach (1752-1840), one of the founding fathers of physical anthropology, online. To date, almost half of the textual material (77.000 pages altogether) and roughly a quarter of the collections have been digitized and converted into TEI-encoded texts or entered into a database. It is through an exploration and application of Semantic Web technologies in a spin-off project called ""Semantic Blumenbach"" that we hope to establish robust and powerful methods for presenting and providing heterogeneous machine-readable linked data for Blumenbach-online. 

Two major tasks have been completed so far. The first is carrying out Named Entity Recognition (NER) on the TEI P5 Tite1 encoded full-texts that have been provided to Semantic Blumenbach2 by Blumenbach-online. These texts lacked the semantic markup e.g. for places, persons and objects from the natural history domain. In addition, we had to deal with historical and irregular orthography of multilingual texts from the second half of the 18th century. Currently we are able to recognize precisely (96%) most (96%) of the technical terms that appear in the text using a list-based algorithm. This algorithm is also able to detect binominal entities from the Linnaean taxonomy, even when they appear as separate strings in different parts of the text. For modeling the relationship between entities in the text and metadata in the collection, we use the WissKI Framework for scientific communication (www.wiss-ki.eu) that allows presenting and using data from various sources in a robust and open system, which is both scalable and reusable by other projects. With the help of the Erlangen CRM Ontology3, an OWL-DL 1.0 implementation of the CIDOC CRM4 and a special application ontology, we model the semantic relationships between objects described in TEI-encoded texts and metadata of these objects.5 We particularly focus on place names, persons and special terms from the natural history domain, including the Latin names of animals and geological objects and construct the relationship between both types of data by using our NER to encode reference strings in the TEI text. 

The Erlangen CRM provides a way to classify these objects in a meaningful way and to model the relationship between the occurrence of the objects in the writings of Blumenbach and the University of Göttingen’s collections. With the help of colleagues from the WissKI Project at Erlangen and Nurnberg we have been able to develop new modules for the Drupal-based system to ingest the TEI and triplify the metadata that we created in the texts. Following a policy of Open Access and Linked Open Data, we will test and implement ways to generate and publish results of academic research in a way that it can be reused in other contexts and by other researchers. Finally, we plan to use a full-text search index (Apache solr) to make both texts and object-related data available in a way that allows both triplyfied metadata and XML full-text to be searched efficiently. 

URL: dhfv-ent2.gcdh.de/blumenbach/wisski

Username and password available on request. 

References
1. www.tei-c.org/release/doc/tei-p5-exemplars/html/tei_tite.doc.html

2. Wettlaufer, Jörg & Thotempudi, Sree Ganesh (2013): Poster - NER in historical Text corpora. Lessons learned so far. 4.-6.03.2013, Mehr Personen – Mehr Daten – Mehr Repositorien, Tagung des Personendatenrepositoriums der BBAW, Berlin. www.gcdh.de/index.php/download_file/view/168/405

3. erlangen-crm.org

4. “CIDOC CRM,” n.d. www.cidoc-crm.org/index.html.

5. C.f. www.tei-c.org/SIG/Ontologies/meetings/m20131003.html",txt,This text is republished here with permission from the original rights holder.,,historical texts;history of science;ner;semantic web;wisski,English,concording and indexing;cultural studies;data mining / text mining;data modeling and architecture including hypothesis-driven modeling;natural language processing;ontologies;semantic web;xml
2062,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Single Page Apps for Humanists: A Case Study using the Perseus Richmond Times Corpus,,Trevor Borg;George Kuriakose Thiruvathukal,"paper, specified ""short paper""","TEI is good at what it does: static documents rendered in glorious detail. But TEI is old. Its age doesn’t make TEI irrelevant, but it’s important to be conscious of how the way we weave the fabric of the web has changed since TEI was conceived in 1994, and reevaluate some of our assumptions about its use. In this early work, we are exploring this rethinking as part of a larger study within the center on general methods for isolating the complexity frequently associated with XML-based frameworks.

The Richmond Times Dispatch corpus of TEI-encoded newspapers comprises the Confederate newspaper’s Civil War run, 1860 — 1865. It is compelling both in terms of organization and content and amounts to a comprehensive textual index. In addition to the historical allure of its content, the formal properties of the digitized documents made available through the Perseus Collection make the Dispatch an extraordinary raw material for building a rich interactive visual experience that augments the textual one.

The Dispatch is not in need of a new home; the Perseus Collection hosts a perfectly functional version and uses best practices for data encoding and organization. Rather than strive to be the primary resource for the source material, our project uses the source material to explore recent patterns in web development as well as alternative, more visually compelling ways to interact with XML corpora in a web application. The goal is to produce a powerful reading environment that is tailored to its source material to an extent that the generalized project of the Perseus Collection can afford.

With respect to its implementation, our project fits the genre of a ‘single page application’ (SPA). This project demonstrates best practices for implementing this type of software project using a particular suite of tools; as an open source example of an SPA that is considerably more complex than the usual teaching examples for this kind of thing, we hope that our implementation will be useful to other people who are considering using the same tools, and especially to humanists interested in presenting TEI-encoded documents.

The SPA is du juor. We prefer beautiful URLs and smooth transitions. We are less fond of all these big lists cluttering our sidebars and clunky arrays of checkboxes. We don’t expect websites to always be inert collections of documents. We want to be able to control the connective tissues. The web development community has responded to our current expectations with tools to suite them.

The client-side MVC (Model-View-Controller) libraries that have recently emerged have reached a high level of maturity A client-side MVC library codifies conventional solutions to the generic problems posed by web traffic. It provides semantics for describing the interaction layer between data and presentation. The codification of conventions that MVC libraries manifest is exciting. It deeply simplifies matters for those who want to make interactive documents. Humanists who have a grasp of the language and concepts involved will be that much better able to articulate and realize project architectures that delight the contemporary reader.

For instance, our application is built around a client-side router. The router formalizes protocols for state transitions that allow for timely and efficient request management. We rely heavily on the concept of the run loop, which exposes powerful document management techniques and is tightly linked with a client-driven templating engine. We are able to achieve a remarkably clean separation of concerns in a highly condensed space by exploiting the conventional roles organized and implemented by these libraries. And by shifting our application’s emphasis to the client, we have constant access to a unified programming environment, limiting the context-switching required when developing different parts of the application.

In addition to our project’s strong client-focused application architecture, we also demonstrate a data architecture solution to the problems posed by the corpus’ rich TEI markup. To expose the facets embedded in the source XML, the implementation transforms the deeply nested structure inherent into flat relational representations that can be searched efficiently. Furthermore our project demonstrates a novel, pythonic approach to transforming the source XML to browser-ready HTML that is particularly amenable to the constraints of an SPA.

XSLT wasn’t very well suited to our TEI transformation problem. One of the key UI features of our application was the ability to discover and search for special entities such as people and places in the text. By implementing a custom transformer in python, we had the flexibility to both translate the TEI tag names into valid HTML versions and retain the original TEI tag names and attributes as attributes on the HTML element.

In addition to serving content thus transformed as needed, the role of the server in our application is limited to various precomputation and preprocessing tasks that only need to be run whenever the source material changes--a process that is fully automated with Unix batch processing (via cron) in the cloud. Users never notice. Research projects are often quagmired in a chaotic sprawl of one-off scripts; we demonstrate a coherent architectural pattern for orchestrating these preprocessors.

Sometimes the affordances of an SPA make it worthwhile to depart from the original document’s presentation. Content on the web wants a different kind of exposure than a stack of newspapers. You want to be able to find things quickly. You want to be able to highlight and hyperlink, associate and drill down. Once you’ve computed a graph of your stack of newspapers, now you can move laterally, staying in the same section and moving from date to date, just as easily as you could stay on the same date and move top-to-bottom through the articles. We demonstrate a novel, minimalistic navigation scheme for the Dispatch.

If you’re taking full advantage of a Javascript environment to render your XML content, you can use modern libraries to plug in visualizations with simplicity, and furthermore to turn these visualizations into interactive filters for a very powerful browsing experience. Using a cluster of technologies surrounding Mike Bostock’s work, we demonstrate how to integrate a visualization library into an SPA.

And yet we believe that datafication shouldn’t overwhelm the content. You want to be discrete about placing your controls lest you scare the casual user, but they should be powerful. Live feedback from search inputs has come to be a common expectation for user interfaces and the SPA environment makes it easy to architect that. We show one effective way to make your XML live-searchable.

We close with just a few screenshots of the work in progress. It is important to note that this interface is being further refined based on new work Trevor is doing in his new role as a front-end software developer. 


Fig. 1: An early version of the splash page, presenting user interface controls for the issue date, section, and subsection. Selecting a subsection would reveal the list of headlines it contains. The date selectors reload the issue content asynchronously, without reloading the page.


Fig. 2: The early version of the article reader, presenting the text in a modal context. The summary of facets across the top act as toggles for corresponding highlights in the text. You can page through the section content using the controls at the bottom of the modal.


Fig. 3: This screen capture shows the direction taken in the latest development. The URL bar demonstrates stateful client-side routing. The content selection controls have been flattened into a trio of type-ahead controls with pagination buttons for navigating forward and backwards both in the document structure and across documents.

This stuff is fun. The tools are a joy to use. The free/open source community behind it is excellent and innovative. We want to see more humanists building applications, and moving away from consuming and rather heavyweight content management systems such as Drupal. Based on our experience, humanists can learn the tools and frameworks quickly with excellent results to boot. We hope that our implementation of the Dispatch will set a strong example for our (and others’) future DH projects.

References
We note that we are proponents of using XML, especially for its originally intended purposes of self-describing data interchange, which remains tremendously valuable in developing type safe RESTful web services. 

George K. Thiruvathukal is leading a separate and parallel effort to develop the Standoff Markup text editor, standoffmarkup.org,  which is aimed at simplifying the encoding and maintenance of XML texts (without exposing tree-oriented abstractions). This is where we started exploring the use of SPA when it comes to building DH-facing tools in general.

Richmond Times Collection, www.perseus.tufts.edu/hopper/collection?collection=Perseus:collection:RichTimes.

Single Page Applications original conception, code.google.com/p/trimpath/wiki/SinglePageApplications

Conventionally, a to-do list. See, for instance, todomvc.com.

The Model-View-Controller design pattern (and paradigm) was introduced as part of the Xerox PARC Alto computer, which used the Smalltalk programming language. An excellent historical read about this paradigm can be found at heim.ifi.uio.no/~trygver/themes/mvc/mvc-index.html.

We use Ember.js: emberjs.com. Our technical term for “high level of maturity” is “rock” but we eschew this Americanism for the purpose of a conference paper submission.

We rely throughout on the wonderful lxml library for Python: lxml.de.

Our present implementation is deployed on Heroku, an agile and scalable framework for deploying apps like ours. The overall project is moving to Linode (a dedicated Linux-based cloud hosting provider).

In this we rely on the large-scale application planning features offered by the Flask web framework for python: flask.pocoo.org.

Author of d3js.org among other things.

We have found dc.js (nickqizhu.github.io/dc.js) to be a perfect storm of visualization functionality.

We use the well-known elasticsearch library (www.elasticsearch.org) to achieve an effect like Google’s live search results.",txt,This text is republished here with permission from the original rights holder.,,ajax;archives;cloud;encoding;precomputation;preprocessing;single-page application;spa;tei;xml,English,"archives, repositories, sustainability and preservation;data mining / text mining;internet / world wide web;publishing and delivery systems;visualization;xml"
2070,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Spreading DiRT: extending the Digital Research Tools directory,,Quinn Dombrowski;Matthew K. Gold,poster / demo / art installation,"1. Background
The DiRT (Digital Research Tools, dirt.projectbamboo.org) directory is a longstanding resource for scholars interested in digital tools and methodologies, providing basic information about software that can facilitate different stages of the research process. DiRT was originally designed as a wiki, where a single wiki page contained information about all tools in a given category. In 2011, under the auspices of Project Bamboo, DiRT was completely rebuilt using the Drupal content management system, which allowed for data to be stored in a structured manner. This enabled more complex searching and browsing options (such as allowing the user to limit results based on criteria like platform or cost), and provided individual profile pages for each tool, which could then serve as a locus for specific comments, or be referenced in other tool profiles. For instance, if a profile page indicates that Neatline is a suite of add-on tools for Omeka, a link to Omeka appears on the Neatline tool profile page, and vice versa.

2. Current development project
One of the biggest limitations of DiRT has been the fact that its contents-- the product of a considerable amount of volunteer work-- have only been available via DiRT’s own web interface. Creating and curating the tool listings on DiRT is largely a manual process. A steering and curatorial board takes an active role in shaping the ongoing development of the site and ensuring data quality, but individual contributions by users make up a large portion of the data. DiRT is currently undergoing a new phase of development, supported by the Andrew W. Mellon Foundation, with the goals of making DiRT data available to others who want to incorporate information about tools into other projects, resources and environments, and also expanding the content provided by DiRT to more clearly situate the tools in the contexts of the projects, research workflows, and pedagogical activities that use them. This poster will demonstrate the accomplishments of the current development project and include information about opportunities to get involved with the project, by trying the DiRT API and plug-ins, or contributing to tool reviews and documented workflows.

3. Areas of work
The poster will also highlight the progress made on developing a DiRT plug-in for Commons In A Box (CBOX), an open source scholarly networking platform created by the City University of New York and used by the Modern Language Association (MLA), the regional NYC Digital Humanities group, and an increasing number of projects and organizations that could benefit from integrated access to information about tools. The CBOX plug-in will:  

provide users with the ability to display information about their DiRT site activity (e.g. tool contributions and edits, reviews, and tool usage information) on their Commons profile;
provide an interface for searching DiRT within the CUNY Academic Commons, for use by groups with an interest in digital humanities;
provide a link to DiRT to facilitate access for inputting new tools.
The poster will also illustrate other areas of development including:

Use of the DiRT API and the API for the DHCommons project directory to augment DiRT tool profiles with information about what projects are using a particular tool
Guidelines and examples of best practice for writing tool reviews, with potential pedagogical applications (e.g. providing a framework for instructors who want to assign students to write reviews of digital research tools, which could then be refined for ultimate publication on DiRT)
Guidelines and examples of best practice for documenting workflows or “recipes” that combine multiple tools in the DiRT registry to achieve some research objective.
Adoption of the taxonomy of research methods jointly developed between DiRT and DARIAH-DE, to replace the previous ad-hoc set of tool categories. DiRT will serve as one of three initial test cases for this taxonomy, which has benefitted from extensive public feedback.
Documentation for how to develop custom tool lists (e.g. tools to be used in a particular class, or tools that are particularly relevant for the disciplines that a subject specialist librarian supports) that pull from the information stored in DiRT, and display that information on other sites.
References
Babeu, Alison (2006). Rome Wasn’t Digitized in a Day”: Building a Cyberinfrastructure for Digital Classicists. CLIR reports, August 2011.Borgman, Christine L. “The Digital Future Is Now: A Call to Action for the Humanities.” Digital Humanities Quarterly 3, no. 4 (Fall 2009). http://www.digitalhumanities.org/dhq/vol/3/4/000077/000077.html.Unsworth, John. Our Cultural Commonwealth: The Report of the American Council of Learned Societies Commission on Cyberinfrastructure for the Humanities and Social Sciences. American Council of Learned Societies. http://acls.org/uploadedFiles/Publications/Programs/Our_Cultural_Commonwealth.pdf.",txt,This text is republished here with permission from the original rights holder.,,apis;integration;tools,English,crowdsourcing;databases & dbms;digital humanities - pedagogy and curriculum;interface and user experience design;teaching and pedagogy
2328,2015 - Sydney,Sydney,Global Digital Humanities,2015,ADHO,ADHO,Western Sydney University,Sydney,,Australia,https://web.archive.org/web/20190121165412/http://dh2015.org/,PhiloLogic4 And The Android PhiloReader Apps: Toward Building A Full-Featured PhiloLogic API,https://github.com/ADHO/dh2015/blob/master/xml/COONEY_Charles_M__PhiloLogic4_And_The_Android_PhiloRead.xml,Charles A. Cooney;Clovis Gladstone;Walter Shandruk;Robert Morrissey;Glenn Roe,poster / demo / art installation,"<?xml version=""1.0"" encoding=""UTF-8""?>
<TEI xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>PhiloLogic4 And The Android PhiloReader Apps: Toward Building A Full-Featured PhiloLogic API</title>
                <author>
                    <persName>
                        <surname>Cooney</surname>
                        <forename>Charles M.</forename>
                    </persName>
                    <affiliation>ARTFL, United States of America</affiliation>
                    <email>chu.cooney@gmail.com</email>
                </author>
                <author>
                    <persName>
                        <surname>Gladstone</surname>
                        <forename>Clovis</forename>
                    </persName>
                    <affiliation>ARTFL, United States of America</affiliation>
                    <email>clovisgladstone@gmail.com</email>
                </author>
                <author>
                    <persName>
                        <surname>Shandruk</surname>
                        <forename>Walter</forename>
                    </persName>
                    <affiliation>ARTFL, United States of America</affiliation>
                    <email>waltms@gmail.com</email>
                </author>
                <author>
                    <persName>
                        <surname>Morrissey</surname>
                        <forename>Robert</forename>
                    </persName>
                    <affiliation>ARTFL, United States of America</affiliation>
                    <email>rmorriss@uchicago.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Roe</surname>
                        <forename>Glenn</forename>
                    </persName>
                    <affiliation>The Australian National University</affiliation>
                    <email>glenn.roe@anu.edu.au</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2014-12-19T13:50:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Paul Arthur, University of Western Sidney</publisher>
                <address>
                    <addrLine>Locked Bag 1797</addrLine>
                    <addrLine>Penrith NSW 2751</addrLine>
                    <addrLine>Australia</addrLine>
                    <addrLine>Paul Arthur</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document </p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident=""DHCONVALIDATOR"" version=""1.9"">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme=""ConfTool"" n=""category"">
                    <term>Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""subcategory"">
                    <term>Short Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""keywords"">
                    <term>PhiloLogic4</term>
                    <term>API</term>
                    <term>Android</term>
                    <term>text databases</term>
                    <term>search and retrieval interface</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""topics"">
                    <term>databases &amp; dbms</term>
                    <term>interface and user experience design</term>
                    <term>project design</term>
                    <term>organization</term>
                    <term>management</term>
                    <term>publishing and delivery systems</term>
                    <term>software design and development</term>
                    <term>text analysis</term>
                    <term>programming</term>
                    <term>mobile applications and mobile design</term>
                    <term>English</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>The ARTFL Project would like to submit a proposal for a short paper on the development of an API for PhiloLogic4, our next-generation corpus query and text retrieval platform for digital humanities databases, and to demonstrate Android PhiloReader Apps that we are building to interact with that API.
                <hi rend=""superscript"">1</hi>
            </p>
            <p>A primary goal of the PhiloLogic4 project has been to allow ARTFL or any digital humanities group using this software to develop a variety of results display and user interfaces with ease.
                <hi rend=""superscript"">2</hi> For example, the databases ARTFL has already released in beta form under PhiloLogic4 for traditional browser access have features such as frequency sidebars for query results, links within those sidebars for faceted browsing, and dynamic Time Series reports.
                <hi rend=""superscript"">3</hi> ARTFL’s PhiloReader Apps extend and exemplify this fundamental design goal. They take advantage of PhiloLogic4’s simple set of query parameters and flexible results object formatting to enable text search and retrieval on handheld devices. 
            </p>
            <p>ARTFL intends these apps to serve as a lighter-weight alternative to web browser apps for interacting with the PhiloLogic4 installations of text databases on our main production servers. The interface has been designed with a focus on the reading functionality that PhiloLogic4 already offers in its web incarnation. Users can conduct word and/or metadata search from a toggling drawer with the aim of finding and reading text sections.
                <hi rend=""superscript"">4</hi> Word search results can be returned in concordance and frequency reports. Any metadata value can serve as a frequency report option, though the most common are word use by title, by date, by author, and even by speaker in collections of plays. More intensive text analysis would require the search form accessible through a web browser. 
            </p>
            <p>Functionally, the Android code interacts with PhiloLogic4 databases simply by sending search queries and then displaying the results that PhiloLogic4 sends back over the network. Those results, like all the data the PhiloLogic4 API passes both internally and externally, are formatted as JSON objects. From the search terms the user enters, the app builds a query URI compliant with the PhiloLogic4 API, which includes parameters such as number of results per page and metadata values, as well as a parameter that calls specifically for a JSON object (‘&amp;format=json’). For basic concordance and bibliographic searches, the URI currently points at the main script PhiloLogic4 uses to handle all interaction from web browsers (‘dispatcher.py’).
                <hi rend=""superscript"">5</hi> For example, a basic concordance query for ‘sun’ from the Shakespeare app has this URI: 
            </p>
            <p>http://artflsrv02.uchicago.edu/philologic4/shakespeare_demo/dispatcher.py?report=concordance&amp;q=sun&amp;method=proxy&amp;title=&amp;start=1&amp;end=25&amp;pagenum=25&amp;format=json </p>
            <p>For the web version of PhiloLogic4, the dispatcher directs calls for secondary queries, like frequency searches and table of contents requests, to scripts that run behind the scenes. In order to minimize extra coding on the server side, the app calls those CGI scripts directly for these same queries. A frequency by title search for ‘sun’ from the app points to the frequency generation code to get PhiloLogic4’s internal JSON object: </p>
            <p>http://artflsrv02.uchicago.edu/philologic4/shakespeare_demo/scripts/get_frequency.py?report=concordance &amp;q=sun&amp;method=proxy&amp;title=&amp;frequency_field=title&amp;format=json </p>
            <p>The JSON object of a concordance result, for example, can contain chunks of the search result, bibliographic metadata, and a PhiloLogic id used for linking into larger sections of the text. The Android code renders the JSON into a string array and displays search results in a listview.
                <hi rend=""superscript"">6</hi> The user can then select individual list items to get larger text sections. The Android code submits a second query to the PhiloLogic4 database for that specific text object which is, again, returned as JSON and also contains navigational links to the previous and next sections of the text. This full-text JSON is rendered for the user to read inside a webview.
                <hi rend=""superscript"">7</hi> We use a webview to display text objects in order to apply the same CSS formatting rules that we use for web versions. The PhiloReader also allows users to bookmark text objects for easy access in later sessions by retaining its PhiloLogic object pointer. 
            </p>
            <p>At the time of writing this proposal, we have succeeded in developing a functional API and Android code to work with it as proof of concept. Going forward, we will continue to work to make the API as generic and simple to use as possible, and fully RESTful compliant. </p>
            <p>Our ultimate goal is to allow other development teams to pick and choose any subset of PhiloLogic4 functionality through the API to build their own interfaces. For instance, developers could integrate concordance search functionality into a traditional desktop app under Windows or MacOS, or into a web environment like Drupal or Django. Developers could also use the API to plug frequency or collocation reports into modern visualization tools like d3.js. And since the API always returns pointers to original text objects, any new interface will be able to have links from results display into fuller document context. </p>
            <p>ARTFL chose Android Java as the initial development language for the apps because of existing in-house capability. At the time of the conference, we will make available a skeleton version of our Android code for other groups to adapt or simply examine to see how we interact with the API. In the coming months we intend to have parallel apps developed for iPads, though we are not certain they will be ready in time to present at DH. Nevertheless, ARTFL believes these Android apps demonstrate the ease and flexibility of using the PhiloLogic4 API to develop new ways of interacting with text databases. </p>
            <p>Screenshots</p>
            <figure>
                <graphic n=""1001"" width=""14.2875cm"" height=""22.86cm"" url=""Pictures/image1.png"" rend=""block""/>
            </figure>
            <p>Figure 1. Search for term ‘sun’ in 
                <hi rend=""italic"">Romeo and Juliet</hi> in the Shakespeare database. 
            </p>
            <figure>
                <graphic n=""1002"" width=""14.2875cm"" height=""22.86cm"" url=""Pictures/image2.png"" rend=""block""/>
            </figure>
            <p>Figure 2. Full-text view from 
                <hi rend=""italic"">Romeo and Juliet </hi>with search term ‘sun’ highlighted.
            </p>
            <figure>
                <graphic n=""1003"" width=""14.2875cm"" height=""22.86cm"" url=""Pictures/image3.png"" rend=""block""/>
            </figure>
            <p>Figure 3. Frequency by title search results for ‘flay’ in ECCOTCP.</p>
            <p>
                <graphic n=""1004"" width=""15.24cm"" height=""12.329583333333334cm"" url=""Pictures/image4.jpeg"" rend=""inline""/>
            </p>
            <p>Figure 4. JSON object of frequency by title search results for ‘flay’ in ECCOTCP, http://artflsrv02.uchicago.edu/philologic4/ecco_tcp_demo/scripts/get_frequency.py?report=concor dance&amp;q=flay&amp;method=proxy&amp;title=&amp;author=&amp;frequency_field=title&amp;format=json.</p>
            <p>Notes</p>
            <p>1. At the time of writing, we have built demonstration apps around the ECCOTCP text collection and the MONK project’s Shakespeare’s Plays data. Versions of these apps can be downloaded from http://artflsrv02.uchicago.edu/downloads/app_download/.</p>
            <p>2. For general background on PhiloLogic4, see Allen, T., Gladstone, C. and Whaling, R., PhiloLogic4: An Abstract Query TEI System, 
                <hi rend=""italic"">Journal of the Text Encoding Initiative, </hi>5 (June 2013). Development is ongoing; code resides at https://github.com/ARTFLProject/ PhiloLogic4.
            </p>
            <p>3. See http://artflproject.uchicago.edu/.</p>
            <p>4. See the ‘Screenshots’ section for images of the interface and an example of a PhiloLogic4 JSON object.</p>
            <p>5. These calls that the app makes to the dispatcher script are subject to change as we continue to refine the API. Eventually, we might choose to bypass the dispatcher, as we do for frequency and table of contents calls from the app, and instead communicate with CGI scripts exclusively.</p>
            <p>6. ListView is Android terminology for a layout that displays a vertically scrollable list. See http://developer.android.com/guide/topics/ui/layout/listview.html.</p>
            <p>7. Again, Android terminology for a view that displays web pages. See http://developer.android.com/reference/android/webkit/WebView.html.</p>
        </body>
    </text>
</TEI>",xml,Creative Commons Attribution 4.0 International,,android;api;philologic4;search and retrieval interface;text databases,English,"databases & dbms;english;interface and user experience design;mobile applications and mobile design;programming;project design, organization, management;publishing and delivery systems;software design and development;text analysis"
2483,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,Deux Projets D'Édition Numérique Dans Le Cadre Du Projet SyMoGIH: Le Journal De Léonard Michon Et Les Actes Des Synodes Des Églises Réformées De Bourgogne,,Christine Chadier;Rosemonde Letricot;Francesco Beretta;Sylvain Boschetto,"paper, specified ""short paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>Le portail d'édition numérique de sources, 
                <ref target=""http://xml-portal.symogih.org/"">
                    <hi rend=""Lien_Internet"">http://xml-portal.symogih.org/</hi>
                </ref>
                <hi rend=""Lien_Internet"">,</hi> que nous présentons aux DH2016 au travers de deux projets d’édition numérique n'est pas un outil de visualisation d'images ou de textes comme pourraient l'être des systèmes de type Omeka ou Drupal. Il s’agit de mettre à disposition de manière dynamique à la fois le texte d'une source mais aussi les données d’une base relationnelle qui constituent son apparat critique. Il faut concevoir ce portail d'édition davantage comme une brique de développement du projet symogih.org. 
            </p>
            <p>Le 
                <ref target=""http://symogih.org/"">
                    <hi rend=""Lien_Internet"">projet symogih.org</hi>
                </ref> (Système Modulaire de Gestion de l'Information Historique) a développé un modèle générique de stockage des données historiques permettant leur interopérabilité et leur publication [
                <ref target=""http://symogih.org/"">http://symogih.org/</ref>, tous les sites web ont été consultés le 30 octobre 2015]. À partir de ce modèle a été mis en place un système d'information collaboratif pour la recherche en histoire qui est aujourd'hui utilisé par 15 projets de recherche et environ 50 utilisateurs individuels. La plateforme du projet symogih.org offre un outillage numérique accessible et pérenne pour le stockage et la publication de données extraites de l'étude de documents archivistiques et bibliographiques. Il est possible d'intégrer des données de nature variée qui décrivent l'activité humaine, sociale, économique ou intellectuelle rassemblant, autour d'événements datés et sourcés, des acteurs individuels ou collectifs, des concepts ou des objets géographiques. Le système autorise également l'
                <ref target=""https://groupes.renater.fr/wiki/symogih/symogih_manuel/edition_de_textes_en_xml-tei"">
                    <hi rend=""Lien_Internet"">articulation de ces données avec des textes codés en XML</hi>
                </ref> – traités selon les recommandations de la Text Encoding Initiative [
                <ref target=""https://groupes.renater.fr/wiki/symogih/symogih_manuel/edition_de_textes_en_xml-tei"">
                    <hi rend=""Lien_Internet"">https://groupes.renater.fr/wiki/symogih/symogih_manuel/edition_de_textes_en_xml-tei</hi>
                </ref>] – ou encore la mise en relation avec des images et leurs métadonnées. La réalisation d'un 
                <ref target=""http://geo-larhra.ish-lyon.cnrs.fr/"">
                    <hi rend=""Lien_Internet"">système d'information géographique</hi>
                </ref> (SIG) [
                <ref target=""http://geo-larhra.org/"">http://geo-larhra.org/</ref>] joue un rôle essentiel dans ce modèle afin d'associer à ces différents objets leur “empreinte spatiale” et ainsi permettre des analyses spatiales diachroniques.
            </p>
            <p>L'un des derniers développements du projet symogih.org a été la mise en ligne d'un 
                <ref target=""http://xml-portal.symogih.org/web-publications.html"">
                    <hi rend=""Lien_Internet"">portail de publication des éditions numériques</hi>
                </ref> élaborées au sein du LARHRA. Si symogih.org permettait aux chercheurs de mettre à disposition les données structurées issues de leur travail analytique, le portail d'édition rend désormais possible la contextualisation d'une source écrite grâce à ces mêmes données. Les textes encodés en XML/TEI sont stockés sur un serveur eXist-db à partir duquel sont développés différents services (visualisations du texte et des données, géolocalisation, possibilité d'export, moteur de recherche). Le point de jonction entre le texte numérique et la base de données se situe à l'intérieur même des balises TEI où sont intégrés les identifiants du système d'information qui font le lien avec les données structurées. Des fonctionnalités de visualisation peuvent être développées à partir des textes encodés stockés dans une base de données native XML, offrant davantage d'interactivité avec les données et augmentant l'expérience de lecture. De plus, grâce aux technologies web, via les langages XQuery + HTML/Javascript, le portail d'édition numérique permet de collecter des données au-delà du référentiel commun de symogih.org directement à partir du web de données (DBpedia, IdRef, ...). 
            </p>
            <p>À ce jour, la partie publique du portail recueille deux projets, l'un d'édition de sources (les 
                <hi rend=""italic"">Mémoires</hi> de Léonard Michon), l'autre d'annotation sémantique et de contextualisation de documents concernant l'histoire des savoirs scientifiques à l'époque moderne (Society religion science) [
                <ref target=""http://xml-portal.symogih.org/web-publications.html"">
                    <hi rend=""Lien_Internet"">http://xml-portal.symogih.org/web-publications.html</hi>
                </ref>]. Un troisième projet d'édition de documents est en cours d'élaboration et verra bientôt le jour : l'édition des Actes des synodes des églises réformées de Bourgogne.
            </p>
            <p>L'édition numérique des 
                <hi rend=""italic"">Mémoires</hi> de Léonard Michon fait partie d'une recherche doctorale [Letricot, Rosemonde. 
                <hi rend=""italic"">Édition critique numérique des </hi>Mémoires
                <hi rend=""italic""> de Léonard Michon (1715-1746).</hi> Sous la direction de Hours, Bernard. Université Jean Moulin Lyon 3, LARHRA UMR5190] qui vise à mettre à disposition du public et de la communauté scientifique l'édition critique du Journal historique d'un notable lyonnais relatant la vie des élites bourgeoises de la ville de Lyon de la première moitié du XVIIIe siècle. Le travail d'encodage XML/TEI s'est principalement centré sur l'identification des segments d'information et des entités nommées (personnes, institutions, lieux, etc.) ce qui permettra de recourir à des analyses quantitatives sur les pratiques d'écriture (fréquence, récurrence de noms, etc.) et sur la nature des informations données dans l'ouvrage, que nous pourrons ensuite traduire en parcours de lecture pour le public (parcours thématique, biographique, chronologique, etc.).
            </p>
            <p>Quant aux Actes des synodes des églises réformées, ils représentent une source essentielle pour la connaissance du protestantisme français sous l’Ancien Régime. Ces assemblées réunissent régulièrement des représentants de toutes les églises d’une province pour traiter des affaires qui leur sont communes : questions financières, disciplinaires, doctrinales, etc. Si les sources sur le protestantisme français font l’objet d’une édition chez Droz dans une sous-série de la collection “Travaux d’Humanisme et Renaissance” intitulée “Archives des Églises réformées de France”, il n’y avait pas de projet en Humanités numériques sur le sujet [Une édition “papier” des Actes des Synodes Provinciaux des Églises Réformées est en cours chez Droz, le premier volume édité par Didier Boisson a été publié en 2012 et concerne l’Anjou-Touraine-Maine (1594-1683). Le second volume proposera les actes des églises de Bourgogne et sera édité par Yves Krumenacker]. Ce sera bientôt le cas avec l'édition sur le portail XML du projet symogih.org des Actes des synodes des églises réformées de Bourgogne au XVIIe siècle, réalisée sous la direction de Yves Krumenacker de l'Université Jean Moulin Lyon 3.</p>
            <p>Ces deux projets d'édition ne se limitent pas à la seule publication de sources mais proposent une édition enrichie par des renseignements récoltés dans des sources complémentaires, saisis dans la base collaborative du projet symogih.org et utilisés non seulement pour préciser l'un ou l'autre renseignement fourni par le texte, selon la démarche d'annotation classique d’une édition papier, mais en proposant également une explication contextuelle dynamique – que permet l'édition numérique – en croisant toute sorte de données et tout en conservant leur traçabilité, par l'enregistrement des sources et de la bibliographie pour chaque information. Il est ainsi possible de reconstituer, par exemple, la généalogie et la carrière d’un pasteur, ou sa bibliographie [L’utilisation d’une base de données collaborative et cumulative permettant de multiplier les sources : journal de pasteur (Bernus, Auguste (1888). Le ministre Antoine de Chandieu d'après son journal autographe inédit 1534-1591. 
                <hi rend=""italic"">Bulletin historique et littéraire publié par la Société de l'histoire du protestantisme français</hi>, Tome XXXVII), sources régionales (Papillon, Philibert (1742). 
                <hi rend=""italic"">Bibliothèque des auteurs de Bourgogne</hi>. Dijon: Philippe Marteret), 
                <hi rend=""italic"">Registres de la Compagnie des pasteurs de Genève</hi>, édités chez Droz].
            </p>
            <p>Les lieux mentionnés dans les écrits et dans la documentation annexe, sont renseignés et géolocalisés dans le 
                <ref target=""http://geo-larhra.ish-lyon.cnrs.fr/?q=gazetteer/named-places"">
                    <hi rend=""Lien_Internet"">gazeteer du projet symogih.org</hi>
                </ref>. À partir des données spatiales encodées dans le texte XML, on pourra ainsi réaliser des cartes interactives illustrant différents aspects du codage : villes organisatrices des synodes, lieux d'origine des pasteurs, carte des églises absentes, parcours professionnels des pasteurs. Des fonctions interactives permettront de rebondir de la carte vers les textes ou les informations respectives.
            </p>
            <p>Nous soulignerons lors de la présentation l'apport de la visualisation dynamique des documents pour les deux éditions, chacune avec ces spécificités. En particulier, l'intégration directe des données de la recherche permet une plus grande interactivité : l’apparat critique peut être à tout moment complété au fil des découvertes des chercheurs par la mobilisation simultanée de données provenant d'un silo d'information commun. De plus, les interfaces de visualisation spatiale, ou les graphes mettant en évidence les relations entre les textes et leurs contenus, facilitent l'accès aux documents édités. Les nombreuses possibilités d'enrichissement et d'exploitation des textes amènent les chercheurs en histoire, mais aussi le public, à une découverte sous d'autres angles et avec de nouvelles perspectives du contenu des textes édités. </p>
            <p>Nous souhaitons montrer l’apport pour la recherche historique d’un portail d'édition numérique de sources, en insistant sur les bénéfices d'une utilisation croisée de textes encodés en XML/TEI et d'une base de données collaborative. Les projets présentés permettent de retracer le processus de traitement de l'information, de la source à son édition numérique dynamique. </p>
        </body>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,ã©dition numã©rique de sources historiques;ãglises rã©formã©es;base de donnã©es collaborative;lã©onard michon;systã¨me dâinformation gã©ographique,English,"content analysis;corpora and corpus activities;cultural studies;databases & dbms;digital humanities - diversity;digital humanities - facilities;digitisation, resource creation, and discovery;encoding - theory and practice;historical studies;knowledge representation;maps and mapping;publishing and delivery systems;spatio-temporal modeling, analysis and visualisation;standards and interoperability;text analysis;visualization;xml"
2595,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,"Making George Washington's Financial Documents Accessible: Transcription, Data, And The Drupal Solution",,Elisabeth Jennifer Stertzer;Erica Fallon Cavanaugh,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>The George Washington Financial Papers Project exists at the intersection of two challenges editors currently face: managing complicated editorial work and navigating the world of digital publication. By focusing on a particularly difficult and dynamic dataset—financial documents—work has advanced on three interconnected fronts: 1) developing document templates for both traditional financial documents, such as account books and ledgers, as well as receipts, journals, and memoranda; 2) developing taxonomies and data visualizations; and 3) constructing an open-source content management/editorial/publication platform. The work has resulted in the development of both an open-access digital edition of Washington’s financial documents as well as the groundwork of Drupal for Editors prototype—a Drupal-based, open-source, editorial/publication platform—providing editors with a stable, flexible, and powerful platform to build engaging digital editions of financial documents.</p>
            <p>In 2013, The Papers of George Washington received a grant from the National Historical Publications and Records Commission (NHPRC) for the perfection and population of the content management database (DocTracker) with Washington’s three major ledger books; preparation of Gouverneur Morris’s 1811-1816 account book and its entry into the content management database in partnership with the Gouverneur Morris Papers at the New-York Historical Society; and the completion of a primary version of a web interface that will provide users with free access to the edition’s entire content and permit downloading and data manipulation. </p>
            <p>During our partnership with DocTracker we helped design a viable content management and customized editorial workflow solution built on the proprietary, commercial database software FileMaker Pro. DocTracker allowed us to manage both document records and content identifications, and associate both with transcriptions. But as a publication platform it was limited because of its use of XML. We investigated alternative publication options and decided on Drupal, a highly-configurable open-source content management system. We determined Drupal to be the best publication solution for several reasons: 1) at its core, Drupal is a database in which imported content can be mapped to fields, allowing for robust displays and searching, querying, and browsing; 2) Drupal is accessible, both in terms of cost and usability and has a large user community; 3) both the backend (content/data) and frontend (website interface) are managed in the system; and 4) Drupal is open-source and its core and add-on (module) code are developed and actively maintained by a large international developer community. </p>
            <p>Drupal has allowed the project to confront the numerous challenges inherent in these documents: (1) different types of financial documents are formatted in distinct, though standardized, ways, and the formatting of financial documents carries implied meanings; (2) transactions are full of dittos, abbreviations, and short hand, that raise a question of what kind of fields should be created to capture the transcription and clear text, thereby making both the text and content searchable; (3) the documents present issues of currency, valuation, and barter; and (4) a hierarchy of documents exist, and therefore the same transaction may be recorded in a day book, account, and ledger, etc., generating multiple instances of the same transaction.</p>
            <p>Indeed, one of the primary goals of the Project is to make accurate transcriptions of the documents <hi rend=""italic"">available</hi>, in keeping with the long tradition of the Papers of George Washington documentary editing project. However, the types of information, or the “data,” contained in these documents are not easily <hi rend=""italic"">accessible</hi> using common search and query techniques. The challenges, as described above, make it impossible to simply transcribe and put online, ready to be searched and understood document transcriptions. The solution involves a combination of transcription and corresponding data fields (where dittos, abbreviations, and short hand have been expanded), node references associating various content types, and term references connecting taxonomies. Additionally, Drupal provides a place to develop and manage taxonomy lists for specific content types, such as financial documents, to enhance the grouping and sorting of content and be used to identify relationships between different types of content.</p>
            <p>Developing this system has challenged us to think creatively about all aspects of the editorial and publication process, resulting in innovative ways for users to explore, analyze, and interact with content. This poster and hands-on demonstration will explore these issues and the technological solutions to make these documents available, as a free online resource as well as highlight strategies for content searchability, including annotation, glossaries, indexes, and linking. </p>
        </body>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,digital editing;drupal;relational database;web publication,English,"databases & dbms;digitisation, resource creation, and discovery;historical studies;interface and user experience design;linking and annotation;metadata"
2644,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,The North Carolina Jukebox Project: Archives Alive and the Making of Digital Cultural Heritage,,Victoria Szabo,"paper, specified ""long paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>The North Carolina (USA) Jukebox project transforms an inaccessible audio archive from the 1930s, of historic North Carolina folk music collected by Frank Clyde Brown, into a vital, publicly accessible digital archive and museum exhibition. Led by Trudi Abel, a librarian in the Rubenstein Special Collections Library at Duke University, and Victoria Szabo, a faculty member in Visual and Media Studies and Information Science + Studies at Duke, this interdisciplinary, collaborative effort also involves scholars in music and folklore, music and preservation librarians, digital media specialists, descendants of the original performers, and contemporary musicians who play this music professionally today (Archives Alive Initiative | Trinity College of Arts and` Sciences, 2016). </p>
            <p>As a teaching experience associated with the Library’s Archives Alive initiative, the project offers opportunities for students from arts, music, computer science, and engineering programs to learn about the collection and develop an exhibition from initial concept to execution, and to do so in collaboration with a diverse set of mentors and collaborators who help them understand the histories and technologies involved, as well as stakes of their presentation choices. As a ongoing archival project, it demonstrates the challenges and opportunities inherent in working out a major library archive and preservation effort alongside a live curricular intervention and planned public exhibition. As a research project, it offers scholars in media studies a firsthand view of how material recording and playback technologies and their affordances help shape subsequent cultural histories, and affect what we can recirculate and share today. Taken together, these strands demonstrate that introducing digital cultural heritage project development as a shared objective enriches student learning, encourages library archiving and preservation projects to consider their public facing dimensions as they construct new resources, and offers digital humanities and media studies scholars meaningful opportunities to collaborate with colleagues in historically minded disciplines around new forms of scholarly production – in this case data-driven exhibitions at the Mountain Music Museum in Western NC, at the Rubenstein Special Collections Library at Duke, and online.</p>
            <figure>
                <graphic url=""561/image1.png"" rend=""inline""/>
                <head>Figure 1. NC Jukebox Project Advertisement</head>
            </figure>
            <p>Our project begins with the songcatcher himself. In the 1930s Frank Clyde Brown, Duke Professor of English, and co-organizer of the North Carolina Folklore Society (1913) as Zeke Graves in our Library tells us, began recording and archiving Western North Carolina folk music (Graves, 2015). Following in the tradition of folklorist Alan Lomax, and songcatcher/musician Bascom Lunsford as chronicled by Loyal Jones, along with other famous songcatchers of the period, he drove around region capturing a range of singers and songs using the technology available in the period, a notebook and Dictaphone equipped with first wax cylinders and later aluminum cylinders (Jones and Forbes, 1984). Like us, Brown involved his students in the project as well, encouraging them to capture songs and research their origins. Today most of those recordings are still housed on wax cylinders and glass disks in the Duke Libraries, and in the Library of Congress, with about 400 songs having already been converted to digital formats. The rest are being converted as part of a substantial Council of Library and Information Resources grant.</p>
            <p>In addition to learning about Brown, histories of the music, songcatching, and folklore practices of the period, students in a Fall 2015 NC Jukebox course began to work closely with the 400 digitized recordings we currently have available, developing metadata, transcribing songs, and organizing their materials in spreadsheet, blog, and database form. Our project also explores biographies of the singers, transcribes the songs as heard on the tapes in in comparison to other versions, and traces the Scotch‑English history and contemporary analogues of the songs themselves through research in Child’s 
                <hi rend=""italic"">Ballads</hi> and other key sources (Child et al., 2001). In addition, we have begun to demonstrate change over time and space through maps, patterns, flows, timelines, and networks of the music - a kind of distant listening, or viewing of its collection and presentation, with more to come. In the physical exhibits, interactive touchscreens, period photos, and hybrid analog‑digital audio playback machines – a radio, a Jukebox, and perhaps a 78‑playing phonograph – will invoke the historical conditions of production and reception of the music for diverse audiences.
            </p>
            <p>As an historiographical research project, NC Jukebox is also offering opportunities to explore firsthand how social and material conditions affect the writing of cultural history itself. Over the course of this project we have learned about the history of songcatching and folklore as social and academic practices designed to verify expectations about a specific kind of musical heritage. Brown died before his work could be compiled into the published versions of his work. As his papers reveal, not all of the songs he collected were included in the final, posthumous collection of his work (Guide to the Frank Clyde Brown Papers, 1912-1974, 2016). His subsequent editors, like other before them who were seeking a pure musical tradition descended from that of the Scots-Irish settlers in the region, as seen in Ritchie and Orr’s 
                <hi rend=""italic"">Wayfaring Strangers,</hi> for example, picked and chose songs to include in the published work (Ritchie, Orr and Parton, 2014). Their criteria are (helpfully for future researchers) sketched out in their editorial notes, which are also in the archives. Songs that were too popular, had been published, were too religious, or, perhaps most significantly, were from African American traditions, were excluded from the published collection, even if familiar from other sourdes like the Library of Congress Checklist of Recorded Songs (U.S Library of Congress, 1942). On another note, we also experienced the lingering effects of musical and cultural segregation first-hand as a class when our well-intentioned musical guest, a contemporary folksinger who is helping keep the Mountain Music traditions alive today, explained that he was going to perform a song as he had heard his “colored” neighbors sing it growing up (McKinney, 2015). This moment became part of a class conversation about representing tradition while at the same time acknowledging our contemporary perspectives upon it in how we frame the music.
            </p>
            <p>This project is also about the history of recording and reproduction technologies, which has deeply affected the content. Encountering recordings made from wax cylinders and glass disks, which included a white-glove visit from Special Collections as well as examining photographs of Brown and other “in the field,” encouraged conversations around our continuously evolving standards and expectations for archiving and reproduction. Our students had to consider whether it was more “authentic” to leave in the hisses and crackles that had made it into the third generation audio files they were listening to, or whether they could and should attempt to clean up the sound quality so it was closer to the “original” source – the singers themselves. Were we archiving the archives, or the ur-performances? Our students also learned about and from Charles Bond, the onetime Duke undergraduate (and now lawyer in San Francisco) who serendipitously took it upon himself in the 1980s to transfer some of the existing recordings to reel-to-reel tapes, using a moog synthesizer to clean up some pops and crackles. </p>
            <figure>
                <graphic url=""561/image2.JPG"" rend=""inline""/>
                <head>Figure 2. Wax Cylinder from the FCB collection</head>
            </figure>
            <p>Further, we discovered how the limits of the recording medium itself reveal the priorities of the documentarians and archivists involved. Wax disks could only record 6-7 minutes of a song, as the 
                <hi rend=""italic"">Federal Cylinder Project</hi> editors note (Gray and Schupman, 1990). Songcatchers might record just one stanza of a song, rather then the whole performance, a fact that highlights that it was the intellectual process of abstracting data from the performance, to be converted to written notation and lyrics, rather than the performance itself, that was most valued for academic folklore purposes. The recordings were data to mine rather than songs to hear. We have also confronted challenges to our efforts in repatriation and exhibition as we began to develop our downloadable “Greatest Hits” of the FCB collection. Because of copyright restrictions on published songs, in the end we may need to limit our final song choices to the purely folkloric – putting us in some cases right back in line with Brown’s purity-seeking editors! We have even begun to wonder about the NC Jukebox idea itself as a title and concept, given that the term jukebox didn’t come into common parlance until the 1940s, as we learned from 
                <hi rend=""italic"">Jukeboxes:An American Social History</hi>, and the music we are sharing was most likely shared at the time either locally or over live radio, as our guest Terry McKinney told us (Segrave, 2002). Our presentism is a both a problem and an acknowledgement of our own historicity and subjectivity.
            </p>
            <p>NC Jukebox is serving as a prototype for future “Archives Alive” projects at Duke University in terms of pedagogical approach, access to primary and secondary source materials from Special Collections, community engagement, and digital platform and exhibition development. As a digital heritage project designed to serve multiple audiences, the digital and onsite exhibition components are being built with an eye towards multiple display formats and locations for a single set of materials within a grown growing database of content. This includes exhibition in a regiona museum organized by McKinney himself (Neufeld, 2015). It also necessitates metadata standards and library infrastructure, a conversation that is ongoing with our Library staff. For the exhibits in Western North Carolina and at the Rubenstein Library we have websites, Omeka exhibits, and interactive web graphics, as well as the downloadable playlist and physical exhibits and listening stations. For the archive, however, we are working with Library and technology partners on a more sustainable and flexible content management system in Drupal that draws content from the more permanent institutional repository solution, which will serve as the substrate for future development as well as, we hope, drive later installations. Our hope is that subsequent generations of students, librarians, and scholars will be able to build upon what we have done in bringing the archives alive.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Brown, F.C.</hi> (2016). The Frank C. Brown Collection of North Carolina Folklore; the folklore of North Carolina, collected by Dr. Frank C. Brown during the years 1912 to 1943, in collaboration with the North Carolina Folklore Society : Frank C. Brown Collection of North Carolina Folklore : Free Download and Streaming: Internet Archive. (2016). [online] Internet Archive. Available at: https://archive.org/details/frankcbrowncolle00fran [Accessed 6 Mar. 2016].
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Gray, J. and Schupman, E.</hi> (1990). 
                        <hi rend=""italic"">The Federal cylinder project</hi>. Washington: American Folklife Center, Library of Congress.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Graves, Z.</hi> (2015). 
                        <hi rend=""italic"">...and We're Putting it on Wax (The Frank Clyde Brown Collection) - Bitstreams: The Digital Collections Blog</hi>. [online] Bitstreams: The Digital Collections Blog. Available at: http://blogs.library.duke.edu/bitstreams/2015/06/19/and-were-putting-it-on-wax-the-frank-clyde-brown-collection/ [Accessed 6 Mar. 2016].
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">McKinney, T.</hi> (2015). 
                        <hi rend=""italic"">North Carolina Mountain Music</hi>. Durham, NC. Available at: http://bit.ly/1MAPQRs [Accessed 6 Mar. 2016].
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Neufeld, R.</hi> (2015). Visiting Our Past: 1930s a Golden Age for music in WNC. [online] <hi rend=""italic"">Citizen Times.</hi> Available at: http://www.citizen-times.com/story/life/2015/02/01/visiting-past-golden-age-music-wnc/22705455/ [Accessed 6 Mar. 2016].
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Segrave,K.</hi> (2002). 
                        <hi rend=""italic"">Jukeboxes: An American Social History</hi>. London: McFarland. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Child, F., Heiman, M., Heiman, L. and Child, F.</hi> (2001). 
                        <hi rend=""italic"">The English and Scottish popular ballads</hi>. Northfield, Minn.: Loomis House Press.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Duke University</hi>. (2016). Archives Alive Initiative | Trinity College of Arts and Sciences. [online] Trinity.duke.edu. Available at: https://trinity.duke.edu/initiatives/archives-alive [Accessed 6 Mar. 2016].
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Duke University. </hi>(2016). Guide to the Frank Clyde Brown Papers, 1912-1974. [online] David M. Rubenstein Rare Book and Manuscript Library. Available at: http://library.duke.edu/rubenstein/findingaids/brownfrankclyde/ [Accessed 6 Mar. 2016].
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Jones, L. and Forbes, J.</hi> (1984). 
                        <hi rend=""italic"">Minstrel of the Appalachians</hi>. Boone, N.C.: Appalachian Consortium Press.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ritchie, F., Orr, D. and Parton, D.</hi> (2014). 
                        <hi rend=""italic"">Wayfaring strangers: The Musical Voyage from Scotland and Ulster to Appalachia</hi>. Chapel Hill: UNC Press.
                    </bibl>
                    <bibl>
                        <hi rend=""selectable"">
                            <hi rend=""bold"">U. S. Library of Congress</hi>. (1942). Division of music. Archive of American folk song.,
                            <hi rend=""italic"">Check-list of recorded songs in the English language in the Archive of American folk song to July, 1940. Alphabetical list with geographical index</hi>. Washington, D.C.: Library of Congress, Division of music.
                        </hi>
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,digital cultural heritage;exhibition;music;pedagogy;public humanities,English,"archives, repositories, sustainability and preservation;audio, video, multimedia;digital humanities - pedagogy and curriculum;digitisation, resource creation, and discovery;folklore and oral history;historical studies;interdisciplinary collaboration;media studies;music;project design, organization, management"
2668,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,"#dariahTeach: online teaching, MOOCs and beyond",,Susan Schreibman;Agiatis Benardou;Claire Clivaz;Matej Ďurčo;Marianne Ping Huang;Eliza Papaki;Stef Scagliola;Toma Tasovac;Tanja Wissik,"paper, specified ""long paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Background</head>
                <p>Online education has been advocated as the ultimate way of democratizing knowledge, but recent research indicates that there are reasons for concern. As the Allen & Seaman 2014 report underlines, 66% of higher education institutions report that online education remains critical to their long-term strategy while 74% of chief academic officers consider the learning outcomes for online courses to be ‘as good as or better’ than traditional face-to-face courses. But “despite this confidence in online education, researchers continue to report ‘compromised quality in online courses’ as one of the concerns of faculty, administration, and the general public” (Kidder, 2015; Selingo, 2014). In the landscape of online teaching, MOOCs (Massive Open Online Courses) have received much attention in both academic and popular publications  (Bayne and Ross, 2015; Bulfin et al., 2014; Clara and Barbera, 2013) despite the fact that they are not representative of the diverse modalities of online teaching.</p>
                <p>Siemens (2012) makes a useful distinction between xMOOCs (behaviorist MOOCs) and cMOOCs (connectivist MOOCs). The former emphasizes “a more traditional learning approach through video presentations and short quizzes and testing” with a focus on “knowledge duplication”, whereas the latter focus on “knowledge creation” (Siemens, 2012). Along the same lines, Ozturk recently reported that new variations of MOOCs have emerged becoming more market oriented “aligning with instructivist, cognitive, and behaviourist pedagogy” (Ozturk, 2015). Moreover, the financial model of the MOOCs raises questions about the audience for and motivations behind this method of teaching (Ozturk, 2015; Manjoo, 2015).</p>
                <p>Conscious of this present situation, the #dariahTeach project (funded by an Erasmus+ Strategic Partnership) is developing a  network based in seven partner countries exploring the production, dissemination, and promotion of high quality, dynamic, extensible, localisable, and integrated educational materials for the digital humanities specifically tailored for third level education. It is adopting a cMOOC philosophy which focuses on ‘creation, creativity, autonomy, and social networked learning’ (Siemens, 2012) to provide pedagogical content that can be easily integrated into diverse teaching and learning situations. </p>
                <p>A key consideration in the design of the platform is interoperability between courses/modules (and units within those modules) since DH draws on a wealth of methods and tools from a variety of disciplines. Moreover, it is envisioned that these modules will be used beyond the DH community as the societal impact of a culturally-driven digital transition grows opening up new ways of collaborating on productive theory and critical thinking (Hayles, 2012). Thus a goal of #dariahTeach is to develop rich educational materials that 1) instructors in the growing number of Digital Humanities programmes can use as appropriate to their own institutional settings and learning outcomes; 2) instructors in other disciplines can draw on and; 3)  students who are not at institutions that have DH expertise can use to develop the skills and methods, as well as understand the theoretical basis, to engage in digital humanities and humanities research.</p>
                <p>The project team is currently developing the infrastructure and design of the modules based on the production of five modules: Introduction to Digital Humanities, Text Encoding, AudioVisual Media and Multimodal Literacies, Retrodigitizing Dictionaries, and Ontologies and Knowledge Management. This paper will present the results of preliminary research carried out through an extensive study of user requirements, as well as desk research on module and platform design informed by a workshop in Belgrade funded by the Digital Research Infrastructure for the Arts and Humanities (DARIAH) on developing open educational materials. </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Analysis of User Requirements</head>
                <p>The design and the implementation of a successful platform-based learning environment  melds concepts from psychology, education, and human-computer interaction. Poor interface design can become a serious obstacle to the learning outcome, as it may slow the process down and impose cognitive obstacles. To this end, a qualitative analysis and interpretation of online teaching practices and recommendations in the DH domain and the elicitation of corresponding user requirements was based on a series of semi-structured interviews with experienced instructors of online courses within Europe.</p>
                <p>Findings of the user requirement process are a key component of the development of the #dariahTeach platform. These indicate that the platform needs to cater for the following needs: be adaptable to different learning methodologies; allow for persistent roles; provide an API or advanced forms of web services so that new unforeseen components can be added to the environment; support  ad hoc groupings and grouping of materials across modules and units; allow for both synchronous and asynchronous collaboration and communication and enable user customization.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Module Design</head>
                <p>#dariahTeach modules are designed as building blocks tailored to the exigencies of teaching situations in different educational and cultural contexts, allowing  for localization and adaptation (via translation, subtitles, domain-specific examples etc.). By offering examples of and encouraging further adaptation of training materials to specific linguistic/cultural contexts, #dariahTeach will dispel any notion that the use of ICT methods leads to abstract representations of culturally impoverished outputs. </p>
                <p>It is important to stress two levels of translatability of module design: 1) translatability and adaptability of the language of instruction; and b) selectability, translatability and adaptability of primary sources and materials that are used in instruction. This means that an English-language module on Text Encoding, for instance, is localizable both in terms of the instructional narrative, as well as the kind of texts that are used to exemplify the taught principles and methods of text modeling: different genres (poetry, prose, drama) but also language (Latin, Greek, Serbian, Dutch etc.) </p>
                <p>Our “Introduction to DH” module will also not attempt to impose a single pedagogical narrative on what is a constantly evolving and highly diverse, interdisciplinary field. Instead, our Introduction to DH is based on a micromodular, polycentric approach: a collection of mutually-linked, cross-referenced, metadata-rich short videos that shed light on DH as a community of practice from multiple perspectives without creating a false sense of uniformity.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Platform Design</head>
                <p>Modules will be made available via an online portal/ web application based on existing solutions. This paper will explore the decision tree in adopting a solution including whether to use a well-established Content Management Systems (eg Drupal, WordPress, Joomla) with Learning Management System plugins and appropriate customizations or the use of a customizable Learning Management System (such as  Moodle or Blackboard). Considerations feeding into the decision tree include the platform being open source, freely available, well documented and customizable with plugin development support;   support for multilinguality; an embedded xml editor; collaboration and interaction functionalities (eg chats, forums and wikis); test and assessment functionalities; extended search functionalities for available metadata (mapped to Dublin Core and LOM to facilitate sharing and support interoperability and reusability (Roy et al., 2010); and copyright attribution and licence management functionalities. </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Conclusion</head>
                <p>The paper will conclude with longer-term prospects for the project. Oversight of #dariahTeach will be maintained after the grant has ended by a General Editor and Editorial Board under the oversight of the DARAH’s Research and Education Competency Centre.</p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl rend=""normal"">
                        <hi rend=""bold"">Allen, I. E. and Seaman, J.</hi> (2014). 
                        <hi rend=""italic"">Grade Change – Tracking Online Education in the United States, </hi>Babson Survey Research Group and the Sloan Consortium, LLC. http://www.onlinelearningsurvey.com/reports/gradechange.pdf
                    </bibl>
                    <bibl rend=""normal"">
                        <hi rend=""bold"">Bayne, S. and Ross, J.</hi> (2015). MOOC Pedagogy. In Kim, P. (ed.) 
                        <hi rend=""italic"">Massive Open Online Courses: The MOOC Revolution</hi>. Oxford: Routledge.
                    </bibl>
                    <bibl rend=""normal"">
                        <hi rend=""bold"">Bulfin, S., Pangrazio, L. and Selwyn, N.</hi> (2014). Making ‘MOOCs’: The construction of a new higher education within news media discourse. 
                        <hi rend=""italic"">International Review of Research in Open and Distance Learning</hi>, <hi rend=""bold"">15</hi>(5): 209-305.
                    </bibl>
                    <bibl rend=""normal"">
                        <hi rend=""bold"">Clarà, M. and Barberà, E.</hi> (2013). Learning online: massive open online courses (MOOCs), connectivism, and cultural psychology. 
                        <hi rend=""italic"">Distance Education</hi> <hi rend=""bold"">34</hi>(1): 129-36. 
                    </bibl>
                    <bibl rend=""normal"">
                        <hi rend=""bold"">Ferguson, R. and Sharples, M.</hi> (2014). 
                        <hi rend=""italic"">Innovative Pedagogy at Massive Scale: Teaching and Learning in MOOCs. Open Learning and Teaching in Educational Communities</hi>. Springer.
                    </bibl>
                    <bibl rend=""normal"">
                        <hi rend=""bold"">Hayles, N. K.</hi> (2012). 
                        <hi rend=""italic"">How We Think. Digital Media and Contemporary Technogenesis</hi>. Chicago University Press.
                    </bibl>
                    <bibl rend=""normal"">
                        <hi rend=""bold"">Kidder, L. C.</hi> (2015). The Multifaceted Endeavor of Online Teaching: The Need for a New Lens”. In Hokanson, B., Clinton, G., Tracey, M. (Eds.) 
                        <hi rend=""italic"">The Design of Learning Experience Creating the Future of Educational Technology</hi>. Springer, pp. 77-91.
                    </bibl>
                    <bibl rend=""normal"">
                        <hi rend=""bold"">Manjoo, F.</hi> (2015, 16 September). ‘Udacity Says It Can Teach Tech Skills to Millions, and Fast’. 
                        <hi rend=""italic"">The New York Times</hi>,  http://nyti.ms/1ihbcp7
                    </bibl>
                    <bibl rend=""normal"">
                        <hi rend=""bold"">Ozturk, H. T. </hi>(2015).
                        <ref target=""https://www.academia.edu/attachments/38952342/download_file?st=MTQ0NTg5OTc2NywxODguNjAuODguMzYsNDYyMTg%3D&s=swp-toolbar"">Examining Value Change in MOOCs in the Scope of Connectivism and Open Educational Resources Movement</ref>”. 
                        <hi rend=""italic"">International Review of Research in Open and Distributed Learning 16/5, Creative Commons 4.0.</hi>
                    </bibl>
                    <bibl>
                    <hi rend=""bold"">Peters, D.</hi> (2014).
                        <hi rend=""italic"">Interface Design for Learning: Design Strategies for Learning Experiences</hi>. San Francisco: New Riders.
                    </bibl>
                    <bibl rend=""normal"">
                        <hi rend=""bold"">Roy, D., Sarkar S. and Ghose S. </hi>(2010). A Comparative Study of Learning Object Metadata, Learning Material Repositories, Metadata Annotation and an Automatic Metadata Annotation Tool. In Joshi, M., Boley, H., Akerkar, R. (eds.). 
                        <hi rend=""italic"">Advances in Semantic Computing</hi> <hi rend=""bold"">2</hi>: 103-26.
                    </bibl>
                    <bibl rend=""normal"">
                        <hi rend=""bold"">Selingo, J. J.</hi> (2014). “Demystifying the MOOC”. 
                        <hi rend=""italic"">The New York Times</hi>, 
                        <ref target=""http://nyti.ms/1u6MYCL"">http://nyti.ms/1u6MYCL</ref>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Siemens, G.</hi> (2012). MOOCs are really a platform, In idem 
                        <hi rend=""italic"">Elearnspace blog</hi>, 
                        <ref target=""http://www.elearnspace.org/blog/2012/07/25/moocs-are-really-a-platform/"">http://www.elearnspace.org/blog/2012/07/25/moocs-are-really-a-platform/</ref>
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,collaboration;dariah;online teaching;partnership,English,"audio, video, multimedia;digital humanities - multilinguality;digital humanities - pedagogy and curriculum;digitisation, resource creation, and discovery;digitisation - theory and practice;encoding - theory and practice;interface and user experience design;knowledge representation;multilingual / multicultural approaches;ontologies;project design, organization, management;scholarly editing;semantic web;teaching and pedagogy"
2730,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,Free FannyPacks: A Model for the Easy Digital Publication of Archival Periodical Material,,Kevin McMullen,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>The nature of nineteenth-century culture, particularly literary, publication, and print culture, meant that many female writers plied their trade in periodicals. Even many American writers who we think of today primarily as novelists—Harriet Beecher Stowe, for instance—first published much of their material in periodicals. Stowe's 
                <hi rend=""italic"">Uncle Tom's Cabin</hi> first appeared serially in 
                <hi rend=""italic"">The National Era</hi>, an abolitionist newspaper. But writers like Stowe gained and have maintained notoriety in part because their work also existed in book form. Many writers whose work remained trapped in periodicals have since fallen off the literary map, their writing accessible only in increasingly fragile and scattered print runs of newspapers held in libraries and archives, or on microfilm. Attempts to digitize cultural heritage material in periodicals have been far spottier than comparable attempts to digitize books for a variety of mostly practical reasons.
                    <note place=""end"">
                        <p rend=""annotation text""> While there have been a number of large-scale newspaper digitization projects, most of them have been undertaken by large commercial entities that charge a fee for access to the content and provide material of mixed quality. Readex and Proquest are two of the more popular services, and genealogy site Ancestry.com has also undertaken its own mass digitization of government records, census data, and newspaper and periodical material; all three charge for access to the content. While these services can certainly be useful for certain types of research work, the quality of the digitization, particularly transcription, is in nearly all cases quite poor, with transcriptions being derived from optical character recognition (OCR) software, which often has difficulty accurately transcribing the small and often smudged print of nineteenth-century periodicals. </p>
                    </note> Over the past decade or so, scholars such as Kenneth Price, Susan Belasco, and Meredith McGill have argued for both an increased acknowledgement of periodicals and periodical writing as a key site of intellectual and literary exchange in the nineteenth century, and the increased utilization of digital tools for the editing, study, and dissemination of periodicals.
                    <note place=""end"">
                        <p rend=""endnote text""> See Price and Belasco's introduction to their edited collection, 
                            <hi rend=""italic"">Periodical Literature in Nineteenth-Century America</hi>. Charlottesville, VA: University Press of Virginia, 1995. Also, see: Belasco. ""
                            <hi rend=""italic"">Whitman's Poems in Periodicals</hi>: Prospects for Periodicals Scholarship in the Digital Age."" 
                            <hi rend=""italic"">The American Literature Scholar in the Digital Age</hi>. Ann Arbor, MI: University of Michigan Press, 2011; McGill, Meredith. 
                            <hi rend=""italic"">American Literature and the Culture of Reprinting, 1834-1853. </hi>Philadelphia: University of Pennsylvania Press, 2003.
                        </p>
                    </note> While the advent of the digital archive has afforded well-documented possibilities for the recovery and, more importantly, dissemination of previously unknown and/or largely inaccessible material,
                    <note place=""end"">
                        <p> Perhaps the most relevant example of a digital recovery project, for the purposes of my presentation, is the 
                            <hi rend=""italic"">Women Writers Project </hi>(http://wwp.northeastern.edu), now run out of Northeastern University and directed by Julia Flanders. Begun in 1988, the project has been instrumental in the recovery of rare or inaccessible work by early modern women writers (the project covers a period from 1526-1850, although the vast majority of texts are from the sixteenth and seventeenth-centuries). In addition to providing access to digitally encoded texts, the project has also provides various research and teaching materials. However, the 
                            <hi rend=""italic"">Women Writers Project</hi> is only accessible with a paid subscription and does not deal with periodicals.
                        </p>
                    </note> digital transcription, encoding, and publication of literary texts still remain skills that many academics feel are well beyond their technical capabilities. This means that many of the people best positioned to undertake such recovery work—literary scholars and other subject-specialists—are held back merely by a technological learning curve that they feel is too steep.
            </p>
            <p>For the past year I have been using basic and widely-available digital tools and resources to build a digital archive of the newspaper writing of the nineteenth century American writer Fanny Fern, who, in the 1850s, was the highest-paid periodical writer in the country, writing for the widest circulated American periodical of its day. The project, 
                <hi rend=""italic"">Fanny Fern in The New York Ledger</hi> (http://fannyfern.org) is the first attempt to make available the full run of Fern's newspaper columns. Using the Drupal content management system, I have been making TEI-encoded transcriptions of Fern's columns, high-resolution digital images of the complete 
                <hi rend=""italic"">Ledger</hi> issues, and brief critical apparatuses about both Fern and the 
                <hi rend=""italic"">Ledger</hi> available for free public and scholarly use. While the digital methods and tools used to construct my project were fairly simple, the functionality and appearance of the finished product belie the relative ease (from a technical standpoint) of its creation. But the 
                <hi rend=""italic"">Ledger </hi>is just one paper and Fanny Fern is just one writer, albeit a significant one. ""There are countless other periodicals and writers, particularly women writers, that deserve this sort of attention,"" I thought. ""If only other 19
                <hi rend=""superscript"">th</hi> century lit scholars could see how easy this is!"" And an idea was born.
            </p>
            <p>I already had a relatively simple TEI template designed to handle the metadata, textual content, and linking of digital image files for periodicals. I had an XSLT style sheet that converted the TEI to HTML. I had project documentation for how I had set up my own domain name and hosting space (using Reclaim Hosting, a web hosting service specifically designed for educators and students). I had documentation about how I had installed and configured Drupal. I had documentation about how I had incorporated the HTML of the transcribed newspaper columns into the Drupal architecture. And I had documentation about how I had tweaked and played around with the design of the site. In short, I had everything anyone would need to build his or her own digital archive; all they would need is the content. If I could just share these files and instructions with other literature scholars, scholars who themselves are experts and masters of all kinds of content, then piece by piece and site by site, the gaps in literary and historical scholarship could to begin to close, be it ever so slightly. While I knew this had long been one of the (increasing number of) mantras of digital humanities, I now felt that I had the means to do my small part to contribute to its realization. Thus, using 
                <hi rend=""italic"">Fanny Fern in The New York Ledger</hi> as an example and template, I plan to soon begin providing other literature scholars with a single package, a package that contains the tools and instructions they will need to construct their own digital archives—I'm going to start handing out FannyPacks.
            </p>
            <p>Geared mainly for those wishing to gather and display texts in a digital environment, these FannyPacks (a zipped collection of files) will include the needed TEI templates, style sheets, and thorough but simple documentation about digital imaging, hosting setup, and Drupal installation and execution. Scholars with a bit of tech savvy can choose to begin hosting their own projects right away. For those looking to first experiment before fully diving in, Reclaim Hosting easily allows for multiple subdomains to be hosted under an existing domain free of charge. Thus, I will provide testing space on my ""fannyfern.org"" domain where users can download their own installations of Drupal and experiment with adding their own content. While I have chosen to take the time to encode Fern's newspaper columns in TEI—for the preservation, interoperability, and potential for enhanced functionality of the files and their content—there are some users who might wish to simply ""get their material out there"" using HTML. Drupal's graphical user interface allows for the easy input of basic or full HTML. Thus, users unfamiliar with or not wanting to take the time to encode in TEI could simply encode their transcriptions in basic HTML and paste them into Drupal's GUI. The setup of both Reclaim Hosting and Drupal also provide ample opportunity for student involvement in the creation of such projects.</p>
            <p>Projects such as 
                <hi rend=""italic"">Fanny Fern in The New York Ledger </hi>and those facilitated by the FannyPacks occupy a scholarly space somewhere between large-scale, institutionally-hosted TEI-based projects such as the 
                <hi rend=""italic"">Willa Cather Archive </hi>or 
                <hi rend=""italic"">Walt Whitman Archive</hi> (both of which are well-funded and boast a host of technical and subject specialists) and a low-cost collaborative TEI repository such as the TAPAS Project (http://tapasproject.org). While not requiring any institutional technical resources, projects based on the FannyPacks model possess greater customization and more autonomy than TAPAS projects. And, in the world of nineteenth-century American women's literature at least, there seems to be a desire for just such a model. A recently created listserv of the Society for the Study of American Women Writers (SSAWW) is geared specifically to those scholars already working on or interested in creating digital projects devoted to American women writers. And initial communication on the listserv has made clear the appeal and potential of a method and means for facilitating the publication of digital scholarly content centered around women writers, particularly periodical writers. But while my main and initial goal will be to work specifically with nineteenth century literature scholars, particularly scholars of women's literature, the FannyPacks certainly have a broader application and could be adapted to fit any manner of digital archival project.
            </p>
            <p>My poster will thus provide a brief overview of my current project, 
                <hi rend=""italic"">Fanny Fern in The New York Ledger</hi>, a discussion of the project's potential to serve as a model, and specifics of the FannyPacks and their creation, distribution, and application.
            </p>
        </body>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,digitization;periodicals;publication model;tei,English,"archives, repositories, sustainability and preservation;digital humanities - pedagogy and curriculum;digitisation, resource creation, and discovery;english studies;literary studies;publishing and delivery systems;scholarly editing;xml"
2852,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,Working with WissKI - A Virtual Research Environment for Object Documentation and Object-Based Research,,Martin Scholz;Dorian Merz;Guenther Goerz,workshop / tutorial,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>In recent years semantic technologies have become increasingly popular to represent, manage and publish data in the humanities. Virtual research environments with semantic backends are used to build complex networks, data is exposed as triples using RDF, and important vocabularies and thesauri are available as linked data. Ontologies like the CIDOC Conceptual Reference Model (CRM) are the semantic backbone of this approach and provide interoperability and data exchange beyond pure linking.</p>
            <p>WissKI (wiss-ki.eu) is a ready-to-be-used web-based virtual research environment and publishing framework that in its core relies on Semantic Web technologies to represent the curated knowledge. The user experience for data acquisition and presentation, however, intentionally borrows from traditional modes, while the user profits from the possibilities of linked and semantically enriched data. Thus, the system enables digital humanists to produce high-quality linked data, without having to cope with technical issues of the Semantic Web and ontologies in general or the often-quoted pecularities of CIDOC CRM in particular. This is achieved by defining a mapping between traditional index card or tabular style on the one hand and graph-based linked data on the other hand. The mapping may be opaque to the users and only be managed by an (ontological) administrator. Also, mappings may be shared between systems and projects, so that best practice patterns may evolve; this actually already has happened and still happens.</p>
            <p>By default, data may be input and displayed either as free text or as structured data via forms. Free text may be input through a graphical editor and is semantically indexed in terms of named entity recognition results, calendar date specifications, mentioned events, and also technical terms as far as appropriate authority files are available (e.g. Getty's Art and Architecture Thesaurus). Form input provides mechanisms for error reduction like spelling variants, e.g. by showing autocompletion hints that are again backed by available authorities. From the textual annotations, RDF triples may be generated and be reused as structured data. Furthermore, the system allows the upload, derivation and display of images. Other, more application-specific ways of data acquisition like mass imports or 2D/3D annotation may be included through extensions.</p>
            <p>From the technical perspective, WissKI is based on Drupal (drupal.org). Drupal is a widely used Web Content Management System with a big and active user and developer community. It has a modular architecture and there exists a vast variety of third party extension. Being such an extension, WissKI profits from a stable core system (security updates!) and also from these community contributions, providing all sorts of functionality. </p>
            <p>As Drupal itself, WissKI is published as open source and can be downloaded from the project web site (wiss-ki.eu) or from github.</p>
            <p>Although WissKI in its core is domain-agnostic, it is designed to best fit the needs of object centered documentation and research as it is typical for many memory institutions, but also for research projects from art history, biodiversity, architecture, epigraphy, etc. As such it naturally goes together with the CIDOC CRM, an ontology designed for the documentation of cultural heritage. It is used by several academic and memory institutions in Germany in national and international research projects; it is used for such diverse purposes as research environment, curated collection management, virtual exhibition, or in courses and seminars.</p>
            <p>The tutorial aims at all researchers, archivists and curators who are interested in object documentation, in particular its semantic disclosure integrating data from (database and content management systems) form-based input and plain text fields. Furthermore it addresses people interested in applications of the CIDOC CRM.</p>
            <p>This half-day tutorial</p>
            <list type=""unordered"">
                <item>gives a short introduction to the (technical) approach of WissKI,</item>
                <item>presents current use cases and modes of use,</item>
                <item>shows how to install, configure, and use WissKI, and</item>
                <item>includes a hands-on for semantic modelling and data acquisition with WissKI and CIDOC CRM.</item>
            </list>
        </body>
    </text>",xml,This text is republished here with permission from the original rights holder.,,annotations;cidoc crm;knowledge modelling;virtual research environment;wisski,English,information architecture;interdisciplinary collaboration;knowledge representation;ontologies;semantic web;text analysis
3907,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Negotiating Sustainability: The Grant Services “Menu” at UVic Libraries,https://dh2017.adho.org/abstracts/231/231.pdf,Lisa Goddard;Christine Walde,"paper, specified ""short paper""","Brief summary
This paper provides a brief overview of library best practices for digital curation, with particular attention to the areas that highlight disciplinary tensions between library science and the humanities. The authors introduce the University of Victoria’s grant service “menu” for digital preservation and hosting services, and outline some of the most promising models for balancing creativity with sustainability in DH project design. We will suggest roles for libraries, researchers, administrators, and funders in helping to create technical and social conditions that nurture sustainable research projects in the digital humanities and beyond.

Abstract

Knowledge building is an iterative process that refines and extends previous research. Through citation, we acknowledge our debt to scholars and theorists whose work enables our own. The ephemeral nature of the digital world threatens to destabilize a centuries long system of scholarly communication and knowledge sharing. In a print ecosystem many immutable copies of an object are distributed globally and are curated by network of organizations. In the digital world a single copy of an object is served from a central location. Digital content is thus susceptible to manipulation, corruption, and erasure. The key to analog preservation is to ensure that artefacts remain the same. Digital preservation, in contrast, requires “active management” comprising constant changes, patches, and updates. Objects become quickly obsolete as the environments around them change.

Funding agencies are putting increased pressure on researchers to include sustainability plans in funding applications (NEH 2016, SSHRC 2016). Researchers often turn to the University Library to provide preservation solutions for digital projects without fully understanding the technical, policy, and funding implications of these requests. Libraries have made significant strides in planning for the long-term preservation of the many thousands of digital objects in our collections. Digitization projects adhere to strict standards for resolution, colour management, and file formats (FADGI 2016). Digital asset management systems like Hydra/Fedora provide a single place to store objects along with descriptive and administrative metadata that helps to determine the preservation actions that should be taken against each object (Goddard, 2016). Those actions include auditing and bit-checking of file systems to ensure against data loss, format migrations as media and file types become obsolete, replication of objects across different technology stacks and jurisdictions, and discovery interfaces that ensure continued discoverability and access. Libraries are building national networks that will allow us to replicate data across multiple jurisdictions to mitigate against disasters both natural and human (DPN 2016, Canadiana 2016, CARL 2016). Despite concerted efforts, only a handful of library repositories have so far met the stringent conditions that are necessary for certification as a Trusted Digital Repository, which requires technical and policy elements including plans for long term staffing and funding, and contingency plans in the event of organizational failure (CRL, 2015). Ultimately, libraries still can’t make guarantees about preservation for digital objects in our own collections, even those that are subject to internationally recognized best practices. This problem is compounded when DH research projects fail to adhere to adequate quality standards for objects (e.g. images, texts, video, maps, mark-up) and overlook established metadata models and vocabularies.

To this point we have outlined the challenges of curating fairly static digital files, but most DH projects are far more than the sum of their digital objects. Many DH research projects are complex software stacks with many layers of tools, objects, code and dependencies. If a project is built on Drupal, for example, librarians will have to not only maintain all of the unique objects and code produced by the project, but they are also committed to maintaining a specific version of a rapidly evolving software platform -- a version that will likely be obsolete before the project concludes. Drupal is, at the very least, well documented and widely deployed. Many DH projects also include custom-built tools, the inner workings of which are known only to a handful of people on the research team. The complex technology profiles of contemporary DH projects require ongoing active management including patching, tending, and rebuilding over time (Burpee, 2015). While the library may have sufficient resources to steward one or two unique project environments, this approach cannot scale to hundreds or even thousands of projects over time. In the current technical and funding environment is simply not possible for libraries to provide high-level curation for the enormous variety of funded digital projects that are produced by researchers within their organizations.

Libraries alone will not solve the problem of sustainability in DH projects. A fundamental characteristic of sustainability is that it must be established as a key design principle from the outset. It is almost impossible to retroactively render a project sustainable without rebuilding from the ground up. Initial choices about technologies, data models, formats, and documentation will influence the likelihood that a project will still be accessible in a decade. One complicating factor is that sustainability is largely at odds with a researcher’s freedom of choice when it comes to decisions about platforms, tools, and data models. Truly sustainable DH projects will require a level of standardization that is far from the current norm in DH project development, and which is unlikely to be unequivocally embraced by humanists. Research is an experimental process, and technological constraints can stifle creativity and independence of action. Models, by their very nature, seek to simplify, while the humanist tradition revels in nuance and complexity (McCarty, 2005; Quamen, 2013).

Leslie Johnston from the Library of Congress suggests that libraries can pursue two models for preserving complex DH projects. The first approach is to “preserve the content but forgo the look and feel. This is often extremely unpopular.” The second is to “preserve the content and the look and feel exactly as they were implemented. This is often close to impossible.” (Johnson, 2013) The tension between these two models is where libraries, researchers, and funders need to more clearly outline our assumptions and expectations.

The University of Victoria Libraries have developed a suite of preservation services for grant funded projects in order to plainly articulate our competencies, assets, and constraints (Goddard and Walde, 2017). This document acts as kind of a “menu” of services from which researchers can select as they develop their grant applications. These include the use of our Hydra/Fedora4 digital asset management system, metadata expertise that extends to consultations around interoperability and linked data, web hosting and discovery, exhibit building software, copyright consultation, open access publishing, research data management, and digitization services. We provide template paragraphs related to sustainability, preservation, open access, and knowledge mobilization that researchers can easily repurpose for any given funding proposal. We include a break down of the in-kind value of each of these services, along with any costs that will be charged back, so that researchers can easily estimate the value of the institutional commitment. We hope that this approach will enable critical conversations about sustainability to happen during the grant writing process, rather than towards the end of funding cycles as has been too often the case in the past. In order to offer our “gold standard” preservation services libraries will have to be involved in early conversations about technology preferences and data models. We certainly don’t assume that all decisions will be dictated by curation needs, but rather that our consultation will enable researchers to make cleareyed decisions about the impact of their choices on sustainability.

The preservation “menu” is an appealing model for researchers, as it enables them to quickly understand the variety of services and in-kind contributions that the library can offer in order to strengthen a funding application. There are also advantages to the library. By tying preservation services to grant funded projects we can avail of a rigorous review process that helps us to direct library resources towards research that has been deemed valuable by a network of disciplinary experts. It provides an easy formula for calculating in-kind contributions for letters of support, and to some extent standardizes the process of writing those letters. It helps to promote librarians as desirable co-applicants and collaborators on funding applications. It underscores to administrators the library’s value as a university research support. This model also provides mechanisms whereby grant funds can flow back into the development of new features for the library’s digital asset management and publishing platforms.

Susan Brown notes that “successful technologies rely on social resources.” (Brown, 2016) Part of our challenge is to muster support from researchers, librarians, administrators, and funders to create optimal conditions for long-term digital curation. The conversation about long-term preservation will be an ongoing negotiation that bridges different disciplinary perspectives, and balances ideals with resource constraints. Just as the traditional model of scholarly print publishing has shaped the means of scholarly production through the last two centuries, these conversations will ultimately will help to shape the future of humanities research platforms, resources, and methodologies.

Bibliography
Brown, S. (2016). “Tensions and tenets of socialized scholarship.” Digital Scholarship in the Humanities, 31(2): 283-300. http://doi.org/10.1093/llc/fqu063

Burpee, K. J. (2015). “Outside the Four Corners: Exploring Non-Traditional Scholarly Communication.” Scholarly and Research Communication, 6(2). http://src-online.ca/index.php/src/article/view/224/417.

Canadian Association of Research Libraries (CARL)

(2016). Compute Canada and the Canadian Association

of Research Libraries Join Forces to Build a National

Research Data Platform. Ottawa, Ontario: CARL.

https://www.computecanada.ca/research/compute-

canada-and-the-canadian-association-of-research-

libraries-join-forces-to-build-a-national-research-data-

platform/

Canadiana (2016). Preservation Policy and Strategy. Ottawa,    Ontario.

http://www.canadiana.ca/preservation-policy-strategy

Center for Research Libraries (CRL) (2015). Certification and Assessment of Digital Repositories. Chicago, IL. https://www.crl.edu/archiving-preservation/digital-archives/certification-assessment

Digital Preservation Network (DPN) (2016). Node Requirements.    http: //dpn.org/dpn-

admin/resources/dpnnodereqmarch2016.pdf

Federal Agencies Digitization Guidelines Initiative Working Group (FADGI) (2016). FADGI Technical Guidelines for Digitizing Cultural Heritage Materials. Washington,    DC.

http://www.digitizationguidelines.gov/guidelines/FAD GI_Still_Image_Tech_Guidelines_2016.pdf

Goddard, L. (2016). “The Read-Write Library.” Scholarly and    Research    Communication.    7(2).

http://dx.doi.org/10.22230/src.2016v7n2/3a255

Goddard, L. and Walde, C. (2017). Hosting and Preservation Services for Grant-Funded Research Projects. Victoria, BC: University of Victoria. http://www.uvic.ca/library/about/ul/UVicLibraries_Gr antServices_Feb2017.pdf

Johnston, L. (2013). “Digital Humanities and Digital Preservation.”    The    Signal.

https://blogs.loc.gov/thesignal/2013/04/digital-humanities-and-digital-preservation/

Kretzschmar, W. and Potter, W. (2010). “Library collaboration with large digital humanities projects.” Literary and Linguistics Computing, 25(4): 439-45. http://dx.doi.org/10.1093/llc/fqq022

Marcum, D. (2016). “Due diligence and stewardship in a time of change and uncertainty.” Ithaka S+R Issue Brief. https://doi.org/10.18665/sr.278232

McCarty, W. (2005). “Chapter 1: Modelling.” Humanities Computing. London, UK: Palgrave, pp. 20-72.

Muñoz, T. and Flanders, J. (2014). “An Introduction to Humanities Data Curation.” Digital Humanities Data Curation    Guide.

http://guide.dhcuration.org/contents/intro/

National Endowment for the Humanities (NEH) (2017). Data Management Plans for NEH Office of Digital Humanities    Proposals    and    Awards.

https://www.neh.gov/files/grants/data_management_ plans_2017.pdf

Quamen, H. (2013). “The Limits of Modelling: Data Culture and the Humanities.” Scholarly and Research Communication,    3(4).    http://src-

online.ca/index.php/src/article/view/69.

Social Sciences and Humanities Research Council of Canada (SSHRC) (2016). Tri-Agency Statement of Principles on Digital Data Management. Ottawa, Ontario. http://www.science.gc.ca/default.asp?lang=En&n=83F 7624E-1

Vandegrift, M. and Varner, S. (2013). “Evolving in Common: Creating Mutually Supportive Relationships Between Libraries and the Digital Humanities.” Journal of Library Administration, 53(1): 67-78.",txt,Creative Commons Attribution 4.0 International,,curation;digital preservation;grant services;scholarly communication;sustainability,English,"archives, repositories, sustainability and preservation;computer science;digitisation - theory and practice;interdisciplinary collaboration;library & information science;project design, organization, management;standards and interoperability"
3914,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Livingstone Online: Access Beyond Openness,https://dh2017.adho.org/abstracts/241/241.pdf,Megan Elizabeth Ward;Adrian S. Wisnicki,"paper, specified ""long paper""","The study of nineteenth-century Africa is troubled by issues of access on two fronts. First, explorers’ unedited field notes - the closest thing we have to a “‘raw’ record” - are rarely available, and, if they are, they are often crumbling, illegible, or located in far-flung archives (Bridges 1998). Second, even when such sources are available in later published forms, they present ethical and ideological problems. Written largely by European explorers and heavily edited, the published texts often exclude the voices of the very populations to which they attempt to provide access. Livingstone Online, a digital project dedicated to the written and visual legacy of nineteenth-century explorer David Livingstone (1813-73), works to counter these issues through its site design, transcription processes, and use of spectral imaging technology.

In order to do so, however, we have had to reconsider our understanding of access, both technologically and ideologically. As a publicly-funded project, we adhere to a high standard of transparency. Yet, as an archive of a contentious figure of imperial exploration, we are also responsive to the recent critiques of access - both of open access as privileging imperial knowledge expansion (Christen 2012; Risam 2017) and of the digital humanities as excluding consideration of race (Gallon 2016). To navigate this conflict, we strive to provide access that is not simply based on openness.

Instead, our project offers an understanding of access that moves in two directions temporally: striving to repair the past by being ethical in our digital treatment and remediation of historical materials, while also acting in a future-oriented fashion in developing and implementing our transparency policies, data standards, and code of collaboration in order to engage a variety of audiences, including those often excluded from DH practice. In this way, our project attempts to create a digital platform for culturally sensitive materials, while our documentation procedures seek to reveal every step of our decision-making process to critical review. Reparative

Livingstone Online, now in its twelfth year (2004 present), is a digital museum and library that draws on recent scholarship and international collaboration to restore one of the British Empire's most iconic figures to his global contexts. Our digital collection of highresolution manuscript images and critically-edited transcriptions - 11,000 images and 700 transcriptions by 2017 - is among the largest on the internet related to any single historical British visitor to Africa. Our site publishes important research on Livingstone's legacy and explores the many ways his ideas have circulated over time. Uniquely, we also takes its visitors far behind the scenes of our work - documenting step-bystep the international collaboration among archives, scholars, scientists, librarians, computer programmers, and other specialists that has made our project possible.

Our use of spectral imaging to uncover the material history of Livingstone’s manuscripts gives us important insights into the conditions under which Livingstone and other imperial explorers wrote - from unacknowledged contributors to the many environments through which the manuscripts circulated. In foregrounding these dimensions, we are also creating a new approach to using spectral imaging in cultural heritage projects because spectral imaging has primarily been used to unearth layers of text, rather than to examine the broader circumstances of imperial record-making and the preservation of imperial records over time. This new use of spectral imaging also constitutes an ethics of access, which is framed by critical essays that explore Livingstone’s uncredited information sources. Livingstone Online here puts forward an idea of access as uncovering the hidden hands and voices of the past.

For instance, in our study of Livingstone’s 1871 Field Diary, we have collaborated with spectral imaging scientists to develop pseudocolor (false color) images to differentiate passages that Livingstone originally wrote from those he added latter and those added by other hands. Likewise, the development of animated spectral images has enabled a chronological reconstruction of events in the life of the diary. By contrast, recourse to images made by principal component analysis (PCA) has uncovered stains on pages otherwise invisible to the naked eye and has introduced us to dimensions of manuscript history otherwise not even suspected to exist.

In addition, we frame these spectral images with paratextual tools that value equally the different kinds of information that Livingstone records. For example, few or no other record remains of many of the villages or the African and Arab individuals Livingstone mentions. As a result, our integrated glossary offers unique, otherwise unavailable geographical information that circulated during Livingstone's time in Central Africa and enumerates the names of people that might otherwise be lost to history. The glossary and other critical materials also provide insight into the complex social dynamics that operated in areas where Livingstone traveled. Overall, Livingstone Online offers a version of access in which the freely available manuscript pages are only a starting point; spectral imaging technology combined with critical building helps construct a reparative ethos of access.


Figure 1. A processed spectral image of a two-page spread from the 10 March 1870 'Retrospect' (Livingstone 1870a:[3]). The kaleidoscopic colors foreground and differentiate the wide range of substances that have left traces on the manuscript's pages over its 145-year history. Copyright National Library of Scotland. Creative Commons Attribution-NonCommercial 3.0 Unported

Future-Oriented
Alongside such reparative work, we also strive to design a project that looks toward the future. Livingstone Online makes our project documents available to an almost unprecedented degree in order to make our publicly-funded research fully accountable, to illuminate our work practices, and to support future digital projects. Our extensive downloadable primary materials (including 12,000 images, 3000 metadata records, and hundreds of transcriptions) are supplemented by freely available project materials, images, and working documents -the things often hidden behind the public face of digital projects. We have curated access to over 600 project documents, including planning documents, spectral image processing information, and essay notes, in order to illuminate the long-term history of our project work. Likewise, access to our grant narratives and working documents de-mystifies funding processes and international, interdisciplinary collaboration in order to support the work of other scholars, especially junior or independent scholars and those new to DH.

In addition, our site is technically accessible in a range of ways. We have built the site with sustainable, community-supported, open-source technologies such as Drupal for our front end and Fedora for our back end, which means that others can access our underlying code (which is fully available from Github) to reuse and modify it for their own projects. To promote use of our site more broadly, we've also worked to make the site inviting to scholars and general users alike, using intuitive, visually-driven site design. Through our site design, open-source code, and transparent documentation, we hope to foster user-led interpretation over passive reception of authorized knowledge.

LIVINGSTONE ONLINE

illuminating imperial exploration


Figures 2,3. Livingstone Online's six section pages, two of which are pictured here, each rely on a diverse range of historical illustrations and contemporary images to complicate the notion of a definitive Livingstone.

As part of this effort, our site is also fully mobile accessible, including for complex functions such as the review and study of archival manuscripts and transcriptions. This opens up Livingstone’s documents and our critical materials to the parts of the world where he worked and travelled; for many people on the African continent, for instance, mobile technology is the main access point to the internet. Likewise, just under a quarter of our collaborating archives are in Africa, and we are actively working on developing additional relationships with African-based archives, in the interests of not only bringing new Livingstone materials into our site, but also to encourage collaboration with African-based scholars and general audiences.

In these ways, we hope to initiate a conversation about the biases and assumptions inherent in the ways that technological advances shape our preservation of the past. We also hope to develop a nuanced practice of access that is embedded in our site design, spectral imaging processing, and transparent documentation, as well as made explicit in our critical materials. Thinking of access beyond openness means creating more historically-minded digital collections that also look to future knowledge creation by an array of populations, not all of them academic or based in the west.

Bibliography
Bridges, R. (1998). “Explorers’ Texts and the Problem of Reactions by Non-Literate Peoples: Some Nineteenth-Century East African Examples.” Studies in Travel Writing 2: 65-84.

Christen, K. (2012). “Does Information Really Want to be Free? Indigenous Knowledge Systems and the Question of Openness.” International Journal of Communication. Web.

Gallon, K. (2016). “Making a Case for the Black Digital Humanities.” In Debates in the Digital Humanities, eds. Lauren F. Klein and Matthew K. Gold. Univeristy of Minnesota    Press.

http://dhdebates.gc.cuny.edu/debates/text/55

Risam, R. (2017, forthcoming). “Decolonising Digital Humanities in Theory and Practice.” Routledge Companion to Digital Humanities and Media Studies. Ed. Jentary Sayers. London: Routledge.",txt,Creative Commons Attribution 4.0 International,,digital archives,English,"anthropology;archives, repositories, sustainability and preservation;cultural and/or institutional infrastructure;digitisation, resource creation, and discovery;digitisation - theory and practice;english studies;geography;knowledge representation;project design, organization, management"
3969,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Understanding Botnet-driven Blog Spam: Motivations and Methods,https://dh2017.adho.org/abstracts/329/329.pdf,Brandon Bevans;Bruce DeBruhl;Foaad Khosmood,"paper, specified ""long paper""","Introduction

Spam, or unsolicited commercial communication, has evolved from telemarketing schemes to a highly sophisticated and profitable black-market business. Although many users are aware that email spam is prominent, they are less aware of blog spam (Thomason, 2007). Blog spam, also known as forum spam, is spam that is posted to a public or outward facing website. Blog spam can be to accomplish many tasks that email spam is used for, such as posting links to a malicious executable.

Blog spam can also serve some unique purposes. First, blog spam can influence purchasing decisions by featuring illegitimate advertisements or reviews. Second, blog spam can include content with target keywords designed to change the way a search engine identifies pages (Geerthik, 2013). Lastly, blog spam can contain link spam, which spams a URL on a victim page to increase the inserted URLs search engine ranking. Overall, blog spam weakens search engines' model of the Internet popularity distribution. Much academic and industrial effort has been spent to detect, filter, and deter spam (Dinh, 2013), (Spirin and Han, 2012).

Less effort has been placed in understanding the underlying distribution mechanisms of spambots and botnets. One foundational study in characterizing blog spam (Niu et al., 2007) provided a quantitative analysis of blog spam in 2007. This study showed that blogs in 2007 included incredible amounts of spam but does not try to identify linked behavior that would imply botnet behavior. A later study on blog spam (Stringhini, 2015) explores using IPs and usernames to detect botnets but does not characterize the behavior of these botnets. In 2011, a research team (Stone-Gross et al., 2011) infiltrated a botnet, which allowed for observations of the logistics around botnet spam campaigns. Overall, our understanding of blog spam generated by botnets is still limited.

Related Work

Various projects have attempted to identify the mechanics, characteristics, and behavior of botnets that control spam. In one important study (Shin et al., 2011), researchers fully evaluated how one of the most popular spam automation programs, XRumer, operates. Another study explored the behavior of botnets across multiple spam campaigns (Thonnard and Dacier, 2011). Others (Pitsillidis et al., 2012) examined the impact that spam datasets had on characterization results. (Lumezanu et al., 2012) explored the similarities between email spam and blog spam on Twitter. They show that over 50% of spam links from emails also appeared on Twitter.


Figure 1: Browser rendering of the ggjx honeypot

The underground ecosystem build around the botnet community has been explored (Stone-Gross et al., 2011). In a surprising result, over 95% of pharmaceuticals advertised in spam were handled by a small group of banks (Levchenko et al., 2011). Our work is similar in that we are trying to characterize the botnet ecosystem, focusing on the distribution and classification of certain spam producing botnets.

Experimental Design

In order to classify linguistic similarity and differences in botnets, we implement 3 honeypots to gather samples of blog spam. We configure our honeypots identically using the Drupal content management systems (CMS) as shown in Figure 1. Our honeypots are identical except for the content of their first post and their domain name. Ggjx.org is fashion themed, npcagent.com is sports themed, and gjams.com is pharmaceutical themed. We combine the data collected from Drupal with the Apache server logs (Apache, 2016) to allow for content analysis of data collected over 42 days. To allow botnets time to discover the honeypots, we activate the honeypots at least 6-weeks before data collection.

We generate three tables of content for each honeypot (Bevans and Khosmood, 2016). In the user table, we record the information the spambot enters while registering and user login statistics that we summarize in Table 1. This includes the user id, username, password, date of registration, registration IP, and number of logins. In the content table, we record the content of spam posts and comments which we summarize in Table 2. This includes the blog node id, the author's unique id, the date posted, the number of hits, type of post, title of the post, text of the post, links in the post, language of the post, and a taxonomy of the post from IBM's Alchemy API.

Honey pot

Quantity

Mean Logins/User

# of Countries

ggjx

62992

1.066

83

gjams

28230

1.102

40

npcagent

34332

1.05

53

Table 1: User table characteristics for three honeypots

Honeypot

Quantity

Avg. Hits

Avg. Links

English Posts

ggjx

2279

28.237

2.356

1962

gjams

2225

18.178

0.311

2137

npcagent

1430

29.043

1.823

1409

Table 2: Characteristics for the content tables

Honeypot

ggjx

gjams

npcagent

# Of Entities

3430

1790

1566

# of Users

62992

28230

34332

Mean Users/Entity

18.365

15.771

21.923

Max Users/Entity

37589

14249

23577

cr of Users/Entity

666.128

359.619

611.157

# of IPs

5291

3092

2120

Mean IPs/Entity

1.543

1.727

1.354

Maximum IPs/entity

118

135

60

<r of IP Quantity

4.277

5.551

2.406

Mean Posts/Entity

.664

1.243

.907

Max Posts/Entity

163

484

664

a of Posts/Entity

5.319

14.448

17.256

% of Entities Who Posted

15.2

12.4

13.5

Table 3: Characteristics of entities

Lastly, in the access table, we include data and meta-data from the Apache logs. This includes the user id, the access IP, the URL, the HTTP request type, the node ID, and an action keyword describing the type of access.

Our honeypots received a total of 1.1 million requests for ggjx, 481 thousand requests for gjams, and 591 thousand requests for npcagent.

Entity Reduction

It is widely accepted that spambot networks, or botnets, are responsible for most spam. Therefore, we algorithmically reduce spam instances into unique entities representing botnets. For each entity, we define 4 attributes: entity id, associated IPs, usernames, and associated user ids. To construct entities we scan through the users and assign each one to an entity as follows.

1. For a user, if an entity exists which contains its username or IP, the user is added to the entity.

2. If more than one entity matches the above criteria, all matching entities are merged.

3. If no entity matches the above criteria, a new entity is created.

We summarize the entity characteristics in Table 3. The maximum number of users in one entity is almost 38 thousand for ggjx with over 100 unique IP addresses. These results confirm what is expected - the vast majority of bots interacting with our honeypots are part of large botnets. This also allows us to perform content analysis exploring what linguistic qualities differentiate botnets.

Feature

Description

Indicates

Effective

Bag or Words

Set of words with count

Lexical content

Yes

Alchemy

Document taxonomy

Taxonomy

Yes

Link

URL core domain names

URL similarity

Variable

Vocab

Vocab complexity

Vocabulary complexity

No

Part-of-speech

A BoW of parts-of-speech

Simple syntax

No

Table 4: NLP feature sets we consider for our content analysis and their effectiveness at differentiating botnets

Content Analysis

To better understand botnets, we use natural language processing (Collobert and Weston, 2008) for analyzing the linguistic content of entities. For our analysis, we consider various feature sets as proxies for linguistic characteristics as summarized in Table 4. We use a Maximum Entropy classifier (Mega M, 2016) to test which features differentiate botnets. In order to test a feature, we train the classifier with 70% of the posts, randomly selected, from the N largest entities and test it with the remaining 30% of the posts. Our final results are the average of three runs.

The first feature set we test is Bag Of Words (BoW) which models the lexical content of posts. Put simply, each word in a document is put into a ‘bag' and the syntactic structure is discarded. For implementation details, see our technical report (Bevans, 2016). In Figure 2, we show our analysis of the BoW feature set.

When considering the top 5 contributing entities, the classification accuracy is less than 95% which implies that the lexical content of botnets varies greatly. The second feature we consider is the taxonomy provided by IBM Watson's AlchemyAPI. Alchemy's output is a list of taxonomy labels and associated confidences. For the purpose of our analysis, we discard any low or non-confident labels. In Figure 3, we show our analysis of the Alchemy Taxonomy feature set which highlights the accuracy of Alchemy's taxonomy. We note that the Alchemy Taxonomy feature set is dramatically smaller in size than the BoW feature set while still providing high performance. This indicates a full lexical analysis is not necessary but a taxonomic approach is sufficient. Our third feature is based on the links in the posts. To create the feature, we parse each post for any HTTP links and strip the link to its core domain name.

The classifier with the link feature set had varied results, as shown in Table 5, where it was reliable in differentiating ggjx entities but less reliable for the other two honeypots. These results correlate with link scarcity from Table 2.


We test the normalized vocabulary size of a post as a feature. We derive this from the number of unique words divided by the total number of words in the post. As shown in Table 5, the vocabulary size does not differentiate botnets.

We also form a feature set based on the part-of-speech (PoS) makeup of a post using the Stanford PoS Tagger. The Stanford PoS tagger returns a pair for each word in the text, the original word and corresponding PoS. We create a BoW from this response that creates an abstract representation of the document's syntax. As shown in Table 5, the PoS does not differentiate botnets.

Feature Set

Database

Accuracy (10 entities)

Accuracy (60 entities)

BoW

BoW

BoW

ggjx

gjams

npcagent

93%

92%

93%

71%

78%

83%

Alchemy

Alchemy

Alchemy

ggjx

gjams

npcagent

87%

91%

91%

80%

84%

82%

Link

Link

Link

ggjx

gjams

npcagent

89%

53%

72%

84%

37%

61%

PoS

PoS

PoS

ggjx

gjams

npcagent

32%

53%

70%

16%

39%

60%

Vocab

Vocab

Vocab

ggjx

gjams

npcagent

32%

50%

74%

17%

36%

60%

Table 5: Accuracies for various features when identifying 10 and 60 entities using the maximum entropy classifier

Conclusions

In this paper, we examine interesting characteristics of spam-generating botnets and release a novel corpus to the community. We find that hundreds of thousands of fake users are created by a small set of botnets and much fewer numbers of them actually post spam. The spam that is posted is highly correlated by subject language to the point where botnets labeled

by their network behavior are to a large degree re-discoverable using content classification (Figure 3).

While link and vocabulary analysis can be good differentiators of these botnets, it is the content labeling (provided by Alchemy) that is the best indicator. Our experiment only spans 42 days, thus it's possible the subject specialization is a feature of the campaign rather than the botnet itself.

Bibliography

Apache virtual    host.    (2016).

http://httpd.apache.org/docs/current/vhosts Accessed: 2016-08-10.

Bevans, B., and Khosmood, F. (2016). Forum Spam Corpus. http://users.csc.calpoly.edu/~foaad/bfbevans Accessed: 2017-04-01.

Bevans, B. (2016). “Categorizing Forum Spam.” Master's Theses at Cal Poly Digital Commons. http: //digitalcom-mons.calpoly.edu/theses/1623 Accessed: 2017-04-01.

Collobert, R., and Weston, J. (2008). “A unified architecture for natural language processing: Deep neural networks with multitask learning.” Proceedings of the 25th International Conference on Machine Learning, ACM: 160-67.

Dinh, S. et al. (2015). “Spam campaign detection, analysis, and investigation.” Digital Investigation, (12) S12-S21.

Geerthik, S. (2013). “Survey on internet spam: Classification and analysis.” International Journal of Computer Technology and Applications, 4(3): 384.

Levchenko, K. et al. (2011). “Click trajectories: End-to-end analysis of the spam value chain.” Symposium on Security and Privacy, IEEE. 431-446.

Lumezanu, C. and Feamster, N. (2012). “Observing common spam in twitter and email.” Proceedings of the 2012 ACM conference on Internet measurement, ACM. 461466.

Mega M. (2016). “Mega model optimization package.” https://www.umiacs.umd.edu/~hal/megam/, Accessed: 2016-08-10.

Shin, Y., Gupta, M., and Myers, S. A. (2011). “The nuts and bolts of a forum spam automator.” LEET.

Spirin, N., and Han, J. (2012). “Survey on web spam detection: Principles and algorithms.”

ACM SIGKDD Explorations Newsletter, 13(2): 50-64.

Stone-Gross, B., et al. “The underground economy of spam: A botmaster's perspective of coordinating large-scale spam campaigns.” LEET, 11: 4.

Stringhini, G. (2015). “Evilcohort: Detecting communities of malicious accounts on online services.” 24th USENIX Security Symposium (USENIX Security 15), 563-578.

Thomason, A. (2007). “Blog spam: A review.” CEAS, Citeseer.

Thonnard O. and Dacier, M. (2011). “A strategic analysis of spam botnets operations.” Proceedings of the 8th Annual

Collaboration, Electronic messaging, Anti-Abuse and Spam Conference, ACM, 162-171.

Niu, Y. et al. (2007). “A quantitative study of forum spamming using context-based analysis.” NDSS.

Pitsillidis, A. et al. (2012). “Taster's choice: A comparative analysis of spam feeds.”

Proceedings of the 2012 ACM conference on Internet measurement, ACM. 427-440.",txt,Creative Commons Attribution 4.0 International,,blog spam;botnets;spam,English,computer science;content analysis;corpora and corpus activities;data mining / text mining;data modeling and architecture including hypothesis-driven modeling;library & information science;natural language processing
4029,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Digital Annotation for Social and Collaborative Learning in the Humanities Classroom,https://dh2017.adho.org/abstracts/415/415.pdf,Brian Johnsrud;Daniel Marle Bush;Michael Widner,"paper, specified ""short paper""","Reading, writing, and discussion are the most common—and, most would agree, the most valuable—components of a university-level humanities seminar. In humanities courses, all three activities can be guided and supported with a variety of digital and analog tools. Digital texts can create novel opportunities for teaching and learning, particularly when students’ reading activity is made visible to other members of the course. In this short paper, we will introduce Lacuna, a web-based platform which hosts digital course materials to be read and annotated socially and collaboratively. We report these findings in relation to an observational case study of four different humanities courses at California community colleges using Lacuna.

Lacuna is the result of a collaborative and iterative effort at Stanford to design a platform that supports the practices of critical reading and dialogue in humanities courses. On Lacuna, which uses Drupal to manage content, course syllabus materials are digitized and uploaded to the platform. These materials can be organized by topic, class date, and other metadata such as medium (text, video, or audio). When students and instructors open up materials, they can digitally annotate selections from any text using a pop-up javascript annotator.

Annotation on Lacuna is a social as well as an individual practice, leveraging the participatory possibilities of web-based technologies. Lacuna users can choose to share annotations with one another and hover over highlighted passages to reveal others’ comments or questions. Social annotation makes explicit and visible for students the broad array of annotation practices within an interpretive community such as a classroom and helps students cocreate interpretations of texts. Students’ annotation activity on Lacuna is also made visible through a separate instructor dashboard, which helps instructors track engagement throughout the course. Finally, annotations can be connected across texts using the “Sewing Kit” in order to support intertextual analyses.

After introducing the features of the platform, we present a case study of the use of Lacuna in four California community-college classes. As part of a U.S. Department of Education Title VI grant, we worked with faculty from Foothill College, De Anza College, and College of San Mateo to incorporate Lacuna into their lesson plans and classroom pedagogy. Drawing on ethnographic methods, we describe how the faculty members used the platform’s affordances to integrate students’ online activity into course planning and seminar discussions and activities. We also explore students’ experience of social annotation and social reading, insights gathered from surveys, interviews, and classroom observation.

In our case study, we find that student annotations and writing on Lacuna give instructors more insight into students’ perspectives on texts and course materials. The visibility of shared annotations encourages students to take on a more active role as peer instructors and peer learners. Our short talk will close with a discussion of the new responsibilities, workflows, and demands on self-reflection introduced by these altered relationships between course participants. People at our talk will learn about the benefits and challenges encountered in using Lacuna, which are likely to be shared by individuals using other learning technologies with similar goals and features. We will also consider future directions for the enhancement of teaching and learning through the use of social reading and digital annotation.",txt,Creative Commons Attribution 4.0 International,,annotation;collaborative;learning;pedagogy;social,English,"design;english studies;interface & user experience design/publishing & delivery systems/user studies/user needs;linking and annotation;literary studies;teaching, pedagogy, and curriculum"
4131,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,"Research Computing’s Demand for Humanists, and Vice Versa",https://dh2017.adho.org/abstracts/564/564.pdf,Quinn Dombrowski;Tassie Gniady;Megan Meredith-Lobay;Jeffrey Tharsen;Lee Zickel,panel / roundtable,"Over the last five years, research computing has undergone a shift from focusing on running infrastructure for highly technical researchers, primarily in the sciences, to supporting medium- to large-scale computational needs across a wide range of disciplines, where practitioners fall across the spectrum of technical proficiency.

This shift in approach has opened up new opportunities, both for scholars whose research questions are facilitated by computationally intensive algorithms (e.g. photogrammetry, natural language processing, OCR at scale), and for humanists in support positions that require translation between technical and non-technical audiences. This panel will discuss opportunities for research and career development through the perspectives of research IT staff whose backgrounds in both humanistic inquiry and computational and digital methodologies enable them to engage in outreach to, training of, and support for humanities researchers. It will also provide practical suggestions for humanists who could benefit from research computing resources but find it difficult to navigate the expectations of research IT organizations.

The communication, writing, and teaching skills cultivated through an advanced degree in the humanities align with employment trends in research IT groups. While some familiarity and comfort with computation is generally a requirement for working in research IT, advanced programming or system administration skills are crucial for a minority of positions in these groups. As the mandate of research computing groups has expanded, it has become increasingly clear that successful research computing programs require documentation that is comprehensible to a non-technical audience, hands-on workshops for non-specialists, and staff capable of understanding the fundamentals of researchers’ work and identifying effective approaches to meeting their computation needs. To that end, in 2014 the US National Science Foundation funded the Advanced Cyberinfrastructure - Research and Education Facilitators (ACI-REF) program, which developed a cohort of computation “facilitators” across a number of universities who could ""maximize the impact of investment in research computing"" by “assisting] researchers in leveraging ACI resources for themselves” and sharing solutions among participating institutions (NSF 2014). This approach to providing support has been highly influential among campus research IT organizations, and related workshops such as the ACI-REF virtual residency (featuring plenaries such as “Effective Communication: How to talk to researchers about their research” and “Writing Grant Proposals”) have drawn large crowds (Neeman et al. 2016).

Similarly, the Extreme Science and Engineering Discovery Environment (XSEDE), funded by the National Science Foundation, has begun specifically reaching out to disciplines that have been typically under-represented in the high performance computing arena, including those in the humanities. XSEDE has a program called Extended Collaborative Support Services (ECSS) that partners humanities scholars and others from under-represented areas with computing professionals who can help them achieve their desired research objectives (Wilkins-Diehr et al. 2016). This has been both necessary and fruitful particularly in areas that are focused on data analytics (such as video analysis, image analysis, network analysis, etc.), as opposed to the more traditional simulation and modeling done by scientists and engineers. XSEDE’s Novel and Innovative Projects group includes a specialist in digital humanities, as well as specialists in pertinent related areas such as big data analytics.

Along the same lines, in January 2014, Compute Canada hired a full-time Digital Humanities coordinator to lead a national team supporting researchers wanting to engage with national advanced research computing (ARC) resources. Compute Canada, a national non-profit organization incorporated in 2012, plans and oversees panCanadian ARC resources used for big data analysis, visualization, data storage, software, portals and platforms for research computing serving Canadian academic and research institutes. The national support team for Compute Canada now consists of the full-time DH coordinator, and a large geographically dispersed team that meets bi-weekly to coordinate national initiatives like training at the Digital Humanities Summer Institute and national competitions such as the recent partnership between Compute Canada and the Canadian Social Sciences and Humanities Research Council. The DH team also meets locally with humanities researchers at their own institutions to help them leverage national infrastructure, and shares experiences and advice back to the national team to help in developing tools, services, and training opportunities that will benefit the national DH community (Simpson 2015).

One of the interesting ramifications of these changes in IT staffing trends is the addition of humanists to IT-based groups in high performance computing, cyberinfrastructure, visualization, and data architectures--humanists who are called upon to maintain both a deep understanding of computational systems, as well as track the needs of scholars in the humanities, which tend to be quite different from researchers in the “hard” or social sciences. Often, they are the lone humanist in a group dominated by engineers and computer scientists. There is a clear need and desire to expand the use of computational resources into ""non-traditional"" (i.e., non-hard-sciences) disciplines, both to justify an institutionwide investment in research computing, and as a way of building institutional capacity for supporting digital humanities scholarship (as described in the forthcoming 2017 ECAR/CNI white paper on institutional digital humanities support). Nonetheless, institutions are struggling with translating their services into comprehensible and relevant offerings for humanities researchers. Finding effective means of supporting these researchers within the traditional model of the research IT group has also been a challenge, given the ways that it differs from library-based support models with which scholars are more familiar. Panelists will reflect on projects they have worked on that successfully bridged the humanities-research computing divide, to the benefit of both groups.

As one example, at Indiana University, a workflow for teaching text analysis with R has been developed that uses web-based Shiny scripts to introduce an algorithm, highly annotated RNotebooks explaining each line of code, lightly annotated RScripts allowing for remixing and adaptation, and, finally, RScripts to leverage multicore environments. The scripts use data both from literature and Twitter, and these tutorials consistently draw the highest attendance in the series DH for Humanists, held throughout the semester. Individual research projects using more sophisticated algorithms such as NER and LDA have also grown out of this project.

As another example, at the University of Chicago, the recently-developed Visual Text Explorer provides a new type of framework for reading texts along with a range of user-customizable analytics, allowing for simultaneous close and distant reading. Other humanities research computing projects include web-based data-driven animated interactive mapping systems, tools for comparative sequence analysis across literary corpora, and automated aggregators of ranges of specific secondary data sources to inform the reading of specific texts/types of texts, all of which require no knowledge of computer programming by the user but nonetheless leverage research computing resources.

At UC Berkeley, 3D modeling work by Near Eastern Studies scholars that uses photogrammetry software running on the high-performance compute cluster has been helpful for testing new Graphics Processing Unit (GPU) nodes in the cluster.

Finally, panelists will provide recommendations for how humanities scholars can translate their research projects in ways that will make them more comprehensible and compelling for institutional research IT groups that may not have a humanist on their support staff.

Panel participants
Quinn Dombrowski is the Digital Humanities Coordinator in Research IT at UC Berkeley. Research IT supports research data management, museum informatics, and computationally intensive research across all domains. She is the author of Drupal for Humanists and has an MA in Slavic linguistics and an MLIS from the University of Illinois.

Tassie Gniady is the manager of the Cyberinfrastructure for Digital Humanities Group at Indiana University. The CyberDH group focuses on workflows for text analysis and photogrammetry. She also teaches an Introduction to Digital Humanities course in the Information and Library Science School. Tassie has a Ph.D in early modern literature from UC-Santa Barbara and an MIS from Indiana University.

Megan Meredith-Lobay is the Digital Humanities and Social Sciences Scientific Analyst for the University of British Columbia's Advanced Research Computing Department. She is also part of WestGrid and Compute Canada, the Canadian HPC national infrastructure platform. Megan has a PhD from the University of Cambridge Department of Archaeology in which she explored the early Christian archaeology of Argyll, Scotland using GIS and early online archaeological databases.

force"". In XSEDE16 Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale. ACM: New York. DOI: 10.1145/2949550.2949584

Simpson, J. (2015). “Building Support for Digital Humanities”. Compute Canada blog. https://www.com-putecanada.ca/blog/building-support-for-digital-hu-

manities/

Wilkins-Diehr, N., et al. (2016)“An Overview of the XSEDE Extended Collaborative Support Program” in High Performance Computer Applications, ed. Isidoro Gitler and Jaime Klapp. Springer Link:    2016.

http://link.springer.com/chapter/10.1007%2F978-3-

319-32243-8 1

Lisa M. Snyde is the director of Campus Research Initiatives for UCLA's Office of Information Technology, and manager of the GIS, Visualization, and 3D Modeling group for the Institute for Digital Research and Education. She has a Ph.D. in Architecture and teaches Virtual Reality and 3D Modeling in UCLA's digital humanities program.

Jeffrey Tharsen is Computational Scientist for the Digital Humanities at the University of Chicago where he is the lead technical domain expert for digital and computational approaches to humanistic inquiry. Jeffrey has a Ph.D. from the University of Chicago's East Asian Languages & Civilizations department, specializing in the fields of premodern Chinese philology, phonology, poetics and paleography.

Bibliography
National Science Foundation. (2014) “Advanced Cyber-

infrastructre - Research and Educational Facilitation: Campus-Based Computational Research Support.” https://www.nsf.gov/awardsearch/show-

Award?AWD ID=1341935

Neeman, H., et al. (2016) ""The Advanced Cyberinfrastructure Research and Education Facilitators Virtual Residency: Toward a National Cyberinfrastructure Work-",txt,Creative Commons Attribution 4.0 International,,careers;computation;infrastructure;research computing,English,"archaeology;computer science;corpora and corpus activities;cultural and/or institutional infrastructure;data mining / text mining;english studies;interdisciplinary collaboration;near eastern studies;project design, organization, management;visualization"
6473,2018 - Mexico City,Mexico City,Puentes/Bridges,2018,ADHO,ADHO;EHD,El Colegio de México;Universidad Nacional Autónoma de México (UNAM) (National Autonomous University of Mexico),Mexico City,,Mexico,https://dh2018.adho.org/,Perspectivas Digitales y a Gran Escala en el Estudio de Revistas Culturales de los Espacios Hispánico y Lusófono,,Ventsislav Ikoff;Laura Fólica;Diana Roig Sanz;Hanno Ehrlicher;Teresa Herzgsell;Claudia Cedeño;Rocío Ortuño;Joana Malta;Pedro Lisboa,panel / roundtable,"<text>
        
            <p>Este panel se propone avanzar, desde un punto de vista metodológico, en el análisis a gran escala de la revista como institución cultural, una aproximación que puede cuestionar centros de producción literaria y revelar dinámicas de relación hasta ahora desconocidas entre las mal denominadas literaturas periféricas y los centros culturales hegemónicos. Para ello, los coordinadores de este panel proponemos cuatro presentaciones que abordan el estudio de revistas culturales históricas del ámbito hispánico y lusófono a través de diversos estudios de caso que utilizan herramientas digitales y combinan intereses disciplinares en los campos de la historia de las ideas, la historia cultural, los estudios de traducción, y la literatura comparada desde una perspectiva transnacional. El panel se propone dar muestra del estado actual del estudio de revistas culturales en los espacios hispánico y lusófono a través del uso de herramientas digitales que permitan avanzar en la discusión metodológica.</p>
            <p>A este respecto, las propuestas de comunicación que se presentan se enmarcan en proyectos de investigación en curso vinculados a distintas universidades de Bélgica, Alemania, Portugal y España (Universiteit Antwerpen, Universität Tübingen, Universidade Nova de Lisboa y Universitat Oberta de Catalunya, respectivamente). Estos proyectos privilegian a la revista cultural como objeto de estudio y emplean distintas herramientas y metodologías digitales, dando sobradas muestras de la riqueza de perspectivas analíticas dentro del campo de las Humanidades Digitales (digitalización de materiales impresos y POS-tagging, construcción de repositorios electrónicos y de portales de investigación, bases de datos relacionales, geolocalización y visualización). Los autores de las distintas presentaciones comparten un compromiso similar en la colaboración científica entre pares, gracias a la publicación en abierto de los datos recogidos en sus investigaciones (
                <hi rend=""italic"">open source</hi>). Las comunicaciones presentarán los respectivos proyectos en curso, ejemplificando con estudios de caso que de ellos se derivan.
            </p>
            <p>En concreto, las comunicaciones del panel abordarán los siguientes objetos:</p>
            <list type=""ordered"">
                <item>La creación de un repositorio de textos a partir de revistas en las Filipinas entre 1850 y 1945, dentro del marco del proyecto “Digitization of Philippine Rare Periodicals and Training in DH”, con el propósito de facilitar el futuro estudio de textos históricos a través de herramientas digitales. Por medio de un análisis textual computacional, esta comunicación ejemplificará la utilidad de este repositorio con un estudio sobre la actitud que adopta la sociedad de habla filipina respecto de otros países en tres momentos concretos del siglo 
                    <hi rend=""smallcaps"">xx</hi>.
                </item>
                <item>La presentación del entorno digital “Revistas culturales 2.0” al servicio de investigadores de revistas culturales históricas en lengua española. En base a este portal, la comunicación presentará un estudio de redes sociales entre los autores, revistas y géneros literarios con el objetivo de centrarse en textos programáticos (editoriales, prólogos o manifiestos).</item>
                <item>La presentación de la base de datos de “Revistas de Ideias e Cultura” portuguesas del siglo 
                    <hi rend=""smallcaps"">xx</hi> que combina aproximaciones a partir de la historia de las ideas, la biblioteconomía y la ciencia de la información. La comunicación abordará las redes de recepción en revistas portuguesas en base a las obras y los nombres citados en ellas.
                </item>
                <item>La identificación y análisis de las traducciones literarias publicadas en revistas hispánicas en el primer tercio del siglo 
                    <hi rend=""smallcaps"">xx</hi> con el objetivo de descubrir publicaciones hasta ahora desconocidas y revelar las relaciones literarias y editoriales entre distintos órganos de la prensa cultural hispánica a escala transnacional. Este estudio se realizará a partir de los datos recogidos en el VRE “MapModern” sobre revistas clave españolas e hispanoamericanas, mediadores culturales, y su participación en eventos y organizaciones culturales internacionales.
                    <pb></pb>
                </item>
            </list>
            <div type=""div1"" rend=""DH-Heading1"">
                Philippines at the crossroads: enhancing research on Philippine periodicals and finding transnational attitudes in them
                <p>
                    <hi rend=""bold"">Rocío Ortuño Casanova</hi>, Universiteit Antwerpen
                </p>
                <p>Key Words: Philippines, online repository, OMEKA, IIIF</p>
                <p>The Philippines has been historically the centre of intercontinental, cultural, and economic relations: between Asia and Europe (Spain) both, in the time of the Spanish invasion (1565-1898) and nowadays 
                    <ref target=""https://www.casaasia.es/triangulacion/eng/MONTOBBIO.pdf"">(Montobbio, 2004: 11, 13</ref>); between America and Asia since the Manila galleon (1565-1815) 
                    <ref target=""https://books.google.be/books/about/The_Age_of_Trade.html?id=75D0oAEACAAJ&amp;redir_esc=y"">(Giráldez 2015)</ref>
                    <hi rend=""Hyperlink"">;</hi> and during the US invasion of the country (1902-1941,1946) (Kramer 398-407, 
                    <ref target=""https://books.google.be/books?id=wDQ5RfVzUFAC&amp;dq=after+postcolonialism+remapping+philippines+United+States+confrontations&amp;hl=es&amp;source=gbs_navlinks_s"">San Juan 2000</ref>). However, the scarce research performed so far on the Philippines, and the difficulty of access to textual materials from the country have became two major problems for the study of these relations, in which the Philippines constitutes a blind spot. In order to address these problems, the 
                    <ref target=""https://www.uantwerpen.be/en/rg/digitalhumanities/"">AC/DC research group</ref> of the University of Antwerp is developing a 
                    <ref target=""https://www.uantwerpen.be/en/rg/digitalhumanities/about/projects/vlir-uos/"">project</ref> in partnership with the University of the Philippines and funded by 
                    <ref target=""http://www.vliruos.be/"">VLIR-UOS</ref> to create an online repository of periodical publications in the Philippines, and to offer training in DH to potential users of this repository.
                </p>
                <p>This talk is structured into two parts. The first one will provide an overview of the digitization scene in the Philippines, and will present the VLIRUOS TEAM project 
                    <ref target=""https://www.uantwerpen.be/en/research-groups/digitalhumanities/about/projects/vlir-uos/"">“Strengthening Digital Research at the University of the Philippines System: Digitization of Philippine rare newspapers and magazines (1850-1945), and training in Digital Humanities”</ref>. The second part will offer an example of what kind of research results we expect to achieve with it.
                </p>
                <p>The project has the initial objectives of (1) making written documentation available online for perusal of researchers both, from the Philippines and abroad. (2) Increasing academic research on humanities in the Philippines by the diffusion of DH methodologies. Therefore, two actions will be implemented:</p>
                <list type=""ordered"">
                    <item>The creation of an online repository of Philippine periodicals published between 1850 and 1945 and hosted at the University of the Philippines. Although the University of Santo Tomás is also uploading their 
                        <ref target=""http://digilib.ust.edu.ph/rare-perio.html"">rare periodicals collection</ref>, and there are other incipient projects for digitization in the Philippines, this repository will differentiate itself by considering three aspects:
                    </item>
                    <item>
                        <hi rend=""bold"">A social aim</hi>: how can this repository be useful to a wide Filipino public? 
                    </item>
                    <item>
                        <hi rend=""bold"" xml:space=""preserve"">Becoming useful to a range of researchers: </hi>how can we process the texts and what metadata are necessary to facilitate research for scholars from different disciplines such as linguistics, history or literature? 
                    </item>
                    <item>
                        <hi rend=""bold"">Facing the challenges of the Philippine context</hi> such as 
                        <ref target=""http://www.philstar.com/technology/2016/10/10/1631885/philippine-internet-improving-still-slowest-world"">slow internet</ref> or multilingualism in periodical publications.
                    </item>
                    <item>Organization of training session in DH and implementation of projects in four campuses of the University of the Philippines.</item>
                </list>
                <p>One of the main objectives of the project is producing interdisciplinary research on the Philippines, based on the digitized materials, using digital tools. In this talk, one example of the kind of research results that we expect to achieve will be provided. We will show an analysis of adjectives related to Spain, China and the US in 1918 (end of World War I), 1930 (after the Crack) and 1936 (between the declaration of the Philippines as a Commonwealth state and the beginning of the Spanish Civil war) in the Philippine cultural magazine 
                    <hi rend=""italic"">Excelsior</hi>, obtained with POS tagging of the text. It aims to find the attitude of the Philippine speaking society towards other countries at the beginning of 20
                    <hi rend=""superscript"">th</hi> century with computational 
                    <ref target=""https://aclweb.org/anthology/W/W12/W12-2514.pdf"">text analysis</ref> (Computer linguistics). The data obtained allows to reach conclusions on historical and literary trends
                    <hi rend=""italic"">.</hi>
                </p>
                <p>References:</p>
                <p>
                    <hi rend=""bold"">Giráldez, A.</hi> (2015). 
                    <hi rend=""italic"">The Age of Trade: The Manila Galleons and the Dawn of the Global Economy</hi>. Lanham, Maryland: Rowman &amp; Littlefield.
                </p>
                <p>
                    <hi rend=""bold"">Kramer, P.</hi> (2006). 
                    <hi rend=""italic"">The Blood of Government: Race, Empire, the United States, and the Philippines</hi>. Quezon City: Ateneo de Manila University Press.
                </p>
                <p>
                    <hi rend=""bold"">Montobbio, M.</hi> (2004). 
                    <hi rend=""italic"">Triangulando la triangulación: España/Europa-América Latina-Asia Pacífico</hi>. Barcelona: Cidob/ Casa Asia.
                </p>
                <p>
                    <hi rend=""bold"">Roque, A.</hi> (2012). Towards a computational approach to literary text analysis. 
                    <hi rend=""italic"">Workshop on Computational Linguistics for Literature</hi>. Montréal, Canada: Association for Computational Linguistics, June 8, 2012, pp. 97–104.
                </p>
                <p>
                    <hi rend=""bold"">San Juan, E.</hi> (2000). 
                    <hi rend=""italic"">After Postcolonialism: Remapping Philippines-United States Confrontations</hi>. Lanham, Maryland: Rowman &amp; Littlefield.
                </p>
                <p>
                    <pb></pb>
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                
                    <hi rend=""italic"">Revistas culturales 2.0</hi> – Portal e investigación
                
                <p>
                    <hi rend=""bold color(1A1A1A)"">Teresa Herzgsell y Claudia Cedeño</hi>
                    <hi rend=""color(1A1A1A)"">, Universität Tübingen</hi>
                </p>
                <p>
                    <hi rend=""italic color(1A1A1A)"">Revistas culturales 2.0</hi>
                    <hi rend=""color(1A1A1A)"" xml:space=""preserve""> es un portal creado en 2014 y desde entonces en uso. Está diseñado como un entorno virtual para investigadores de revistas culturales históricas en lengua española. Técnicamente, el portal se basa en el gestor de contenidos Drupal 8.0, en el que se han implementado funcionalidades específicas. Así se permite por una parte a los usuarios registrados etiquetar los materiales digitalizados (provenientes de los fondos de revistas culturales latinoamericanos del Instituto Iberoamericano de Berlín) mediante formularios prestablecidos. Y por otra parte se posibilita el intercambio de datos entre el entorno virtual y bibliotecas, y otros grupos de investigación. Las funciones que el portal ofrece actualmente son: </hi>
                    <hi rend=""italic color(1A1A1A)"">Blog</hi>
                    <hi rend=""color(1A1A1A)"" xml:space=""preserve"">, </hi>
                    <hi rend=""italic color(1A1A1A)"">Biblioteca Virtual</hi>
                    <hi rend=""color(1A1A1A)"" xml:space=""preserve"">, </hi>
                    <hi rend=""italic color(1A1A1A)"">Red de Participantes</hi>
                    <hi rend=""color(1A1A1A)"" xml:space=""preserve"">, </hi>
                    <hi rend=""italic color(1A1A1A)"">Publicaciones</hi>
                    <hi rend=""color(1A1A1A)"" xml:space=""preserve"">, </hi>
                    <hi rend=""italic color(1A1A1A)"">Bibliografía</hi>
                    <hi rend=""color(1A1A1A)"" xml:space=""preserve""> y </hi>
                    <hi rend=""italic color(1A1A1A)"">Enlaces comentados</hi>
                    <hi rend=""color(1A1A1A)"">. En conjunto tienen el objetivo de construir puentes entre las bibliotecas y sus colecciones digitales, grupos de investigación y el público general interesado. Tenemos paralelamente dos tareas básicas: la de orientar sobre proyectos actuales de digitalización de revistas culturales hispánicas, tanto de España como de América Latina, y otra, no menos importante, la de impulsar el uso de herramientas y tecnologías de las humanidades digitales en la investigación.</hi>
                </p>
                <p>
                    <hi rend=""color(1A1A1A)"" xml:space=""preserve"">Fruto de este último objetivo es la línea de investigación actual titulada </hi>
                    <hi rend=""italic color(1A1A1A)"">Del modernismo a las vanguardias: procesos de modernización y redes transnacionales en revistas culturales de la modernidad</hi>
                    <hi rend=""color(1A1A1A)"" xml:space=""preserve"">, proyecto grupal financiado desde 2017 por la Fundación Alemana de Investigación Científica (DFG). Este se conforma por dos subproyectos estrechamente entrelazados entre sí, con enfoque sobre el modernismo y las vanguardias, respectivamente. En ellos la revista cultural es considerada como una red que pone en contacto no solamente diferentes actores (colaboradores que pueden ser escritores, pintores, directores, etc.), sino también diferentes géneros textuales. </hi>El desarrollo de tales géneros está marcado tanto por las dinámicas intrínsecas del campo literario, como de factores externos como son el valor económico de la literatura en cuanto mercancía que se quiere vender a un público. Para poner en práctica este enfoque que conceptualiza la revista como red usamos métodos cuantitativos de análisis y visualización de redes basados en datos, métodos de muy reciente aplicación en el campo de la investigación de revistas (estudios pioneros en este sentido son So y Long, 2013 y Murphy et al., 2014).
                </p>
                <p>
                    <hi rend=""color(1A1A1A)"" xml:space=""preserve"">Sobre la base de metadatos hemerográficos estructurados, se analizan con </hi>
                    <hi rend=""italic color(1A1A1A)"">Gephi</hi>
                    <hi rend=""color(1A1A1A)"" xml:space=""preserve""> revistas como redes bimodales (distribución autor-género y autor-revista). Sin embargo, la visualización en nuestro proyecto de investigación no es el objetivo final, sino un paso intermedio que permite reconocer patrones formales que nos dirigen a la hora de realizar después lecturas intensivas, enfocadas especialmente en textos programáticos como editoriales, prólogos, manifiestos, etc. En conjunto, pues, nuestra metodología es mixta y combina enfoques cuantitativos y cualitativos, lecturas distantes con cercanas.</hi>
                </p>
                <p>
                    <hi rend=""bold color(1A1A1A)"">Bibliografía:</hi>
                </p>
                <p>
                    <hi rend=""bold"">Murphy</hi>
                    <hi rend=""bold smallcaps"">,</hi>
                    <hi rend=""bold"" xml:space=""preserve""> J. S. et al.</hi> (2014). Visualizing Periodical Networks. 
                    <hi rend=""italic"">The Journal of Modern Periodical Studies</hi>, V(1) (Special Issue).
                </p>
                <p>
                    <hi rend=""bold"">So, R. J. y Long, H.</hi> (2013). Network Analysis and the Sociology of Modernism. 
                    <hi rend=""italic"">Boundary 2</hi>, 40: 147-82.
                </p>
                <p>
                    <hi rend=""bold color(1A1A1A)"">Enlaces</hi>
                    <hi rend=""color(1A1A1A)"">:</hi>
                </p>
                <p>
                    <ref target=""https://www.revistas-culturales.de/"">
                        <hi rend=""color(103CC0)"">https://www.revistas-culturales.de/</hi>
                    </ref>.
                </p>
                <p>
                    <ref target=""http://gepris.dfg.de/gepris/projekt/327964298?language=en"">http://gepris.dfg.de/gepris/projekt/327964298?language=en</ref>
                    <pb></pb>
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Portuguese magazines of ideas and culture in the early decades of the 20
                    <hi rend=""superscript"">th</hi> century: reception networks within different political and artistic movements
                
                <p>
                    <hi rend=""bold"">Joana Malta and Pedro Lisboa</hi>, Universidade Nova de Lisboa
                </p>
                <p>For almost two decades now, the Seminário Livre de História das Ideias (Free Seminar of History of Ideas) research group has been working on building a comprehensive and extensive database containing some of the most important 20
                    <hi rend=""superscript"">th</hi> century Portuguese periodicals. The Magazines of Ideas and Culture project has adopted a multidisciplinary approach from the beginning, making use of knowledge from fields such as history of ideas, library science, and information science, with the aim of building a relational database that contains exhaustive information on authorship, quoted authors and works, subjects, concepts, and geographical names for all articles printed in these publications. For methodological purposes, any autonomous printed piece, whether an essay, a poem, an aphorism, a news story, etc., is considered an article. Final users of the database are thus presented with a corpus of texts and contextualising metadata that structure the more or less explicit programmatic and doctrinarian aspects of these magazines, from a perspective grounded in the fields of history of ideas and conceptual history.
                </p>
                <p>Not only do we propose to present the platform and the project, but also to show some of the possible outcomes obtained by using this information. Working mainly with data on quoted names and works, we bring to light common reception networks from magazines published by artists and intellectuals with diverse origins such as the republican Renascença Portuguesa, the anarchist movement, or the Modernist literary currents. We will be using information on all quoted names and works from every article published in the following groups of magazines: 
                    <hi rend=""italic"">Nova Silva</hi> (1907), 
                    <hi rend=""italic"">A Águia</hi> (1910-1932), and 
                    <hi rend=""italic"">A Vida Portuguesa</hi> (1912-1915), all related to the Renascença Portuguesa movement; 
                    <hi rend=""italic"">Atlantida</hi> (1915-1920), published both in Portugal and in Brazil; 
                    <hi rend=""italic"">A Sementeira</hi> (1908-1919), 
                    <hi rend=""italic"">Germinal</hi> (1916-1917), 
                    <hi rend=""italic"">Suplemento Literário Ilustrado do Jornal A Batalha</hi> (1923-1927), and 
                    <hi rend=""italic"">Renovação</hi> (1925-1926), from the anarchist camp; and 
                    <hi rend=""italic"">Orpheu</hi> (1915), 
                    <hi rend=""italic"">Eh Real!</hi> (1915), 
                    <hi rend=""italic"">Exílio</hi> (1916), 
                    <hi rend=""italic"">Centauro</hi> (1916), 
                    <hi rend=""italic"">Sphinx</hi> (1917), 
                    <hi rend=""italic"">A Tradição</hi> (1917) and 
                    <hi rend=""italic"">Portugal Futurista</hi> (1917), magazines that were connected, in one form or another, to the Modernist movement. These publications had very different underlying programmatic and doctrinarian backgrounds, as well as a very asymmetric presence within the context of Portuguese magazines, both with periodicals that existed for extended periods (
                    <hi rend=""italic"">e.g.</hi>, 
                    <hi rend=""italic"">A Águia</hi> was published from 1910 to 1932) and those with very short timespans (
                    <hi rend=""italic"">e.g.</hi>, a single issue of 
                    <hi rend=""italic"">Portugal Futurista</hi> was published). We identify differences and similarities, both profound and subtle, between the intellectual and cultural frameworks underlying the selected group of magazines and the movements they belong to, as a means to gain better understanding of the ideological foundations of Portuguese political, cultural, artistic, and intellectual movements of the first decades of the 20
                    <hi rend=""superscript"">th</hi> century.
                </p>
                <p>References:</p>
                <p>
                    <hi rend=""bold"">Andrade, L. C. de</hi> (2003). Introdução: quatro notas breves. In Andrade, L. C. (ed), 
                    <hi rend=""italic"">Revistas, Ideias e Doutrinas. Leituras do Pensamento Contemporâneo</hi>. Lisbon: Livros Horizonte, pp. 11-18.
                </p>
                <p>
                    <hi rend=""bold"">Andrade, L. C. de</hi> (1999). O Substantivo “intelectuais”. 
                    <hi rend=""italic"">Cadernos de Cultura</hi>, 2: 23-41.
                </p>
                <p>
                    <hi rend=""bold"">Andrade, L. C. de</hi> (2016). Revistas de Ideias e Cultura, 
                    <ref target=""http://ric.slhi.pt/A_Aguia/um_voo_singular_e_longo"">http://ric.slhi.pt/A_Aguia/um_voo_singular_e_longo</ref> (accessed 22 November 2017).
                </p>
                <p>
                    <hi rend=""bold"">Carrington, P. J. et al.</hi> (eds) (2005). 
                    <hi rend=""italic"">Models and Methods in Social Network Analysis</hi>. Cambridge: Cambridge University Press.
                </p>
                <p>
                    <hi rend=""bold"">Castro, Z. O. de (1996).</hi> Da história das ideias à história das ideias políticas. 
                    <hi rend=""italic"">Cultura: Revista de História e Teoria das Ideias</hi>, VIII (2): 11-21.
                </p>
                <p>
                    <hi rend=""bold"">Charle, C.</hi> (2004). 
                    <hi rend=""italic"">Le Siècle de la Presse (1830-1939)</hi>. Paris: Éditions du Seuil.
                </p>
                <p>
                    <hi rend=""bold"" xml:space=""preserve"">Freire, J. </hi>(1981). “A Sementeira”, do arsenalista Hilário Marques. 
                    <hi rend=""italic"">Análise Social</hi>, XVII (67-68): 767-826.
                </p>
                <p>
                    <hi rend=""bold"">Frigessi, D.</hi> (ed) (1979). 
                    <hi rend=""italic"">La Cultura Italiana del '900 Attraverso le Riviste</hi>. Turin: Giulio Einaudi.
                </p>
                <p>
                    <hi rend=""bold"">Guimarães, L. et al.</hi> (2013). 
                    <hi rend=""italic"">Atlântida. A invenção da comunidade luso-brasileira</hi>. Rio de Janeiro: Contracapa.
                </p>
                <p>
                    <hi rend=""bold"">Lisboa, P. (2015).</hi> Edição electrónica de revistas históricas. O caso de A Águia. In Rollo, M. F. and Amaro, A. R. (eds), 
                    <hi rend=""italic"">República e Republicanismo</hi>. Coimbra: Caleidoscópio, pp. 133-145.
                </p>
                <p>
                    <hi rend=""bold"" xml:space=""preserve"">Martins, A. L. </hi>(ed) (2012). 
                    <hi rend=""italic"">História da Imprensa no Brasil</hi>. São Paulo: Contexto.
                </p>
                <p>
                    <hi rend=""bold"" xml:space=""preserve"">Pluet-Despatin, J. et al. </hi>(eds) (2002). 
                    <hi rend=""italic"">La Belle Époque des Revues</hi>. Paris: IMEC.
                </p>
                <p>
                    <hi rend=""bold"">Santos, A. R. dos</hi> (1990). 
                    <hi rend=""italic"">A Renascença Portuguesa: um movimento cultural portuense</hi>. Porto: Fundação Eng. António de Almeida.
                </p>
                <p>
                    <hi rend=""bold"">Scott, J.</hi> (2000). 
                    <hi rend=""italic"">Social Network Analysis: a handbook</hi>. London: Sage.
                </p>
                <p>
                    <hi rend=""bold"">Seminário Livre de História das Ideias, Revistas de Ideias e Cultura</hi>, 
                    <ref target=""http://ric.slhi.pt/"">http://ric.slhi.pt/</ref>
                </p>
                <p>(accessed 22 November 2017).</p>
                <p>
                    <hi rend=""bold"">Tebbel, J. and Zuckerman, M. E.</hi> (1991). 
                    <hi rend=""italic"">The Magazine in America, 1741-1990</hi>. New York: Oxford University Press.
                </p>
                <p>
                    <hi rend=""bold"">Wasserman, S. and Faust, K.</hi> (1994). 
                    <hi rend=""italic"">Social Network Analysis: methods and applications</hi>. Cambridge: Cambridge University Press.
                    <pb></pb>
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                La traducción en revistas literarias hispánicas: una reflexión metodológica a partir del empleo de herramientas digitales
                <p rend=""Normal1"">
                    <hi rend=""bold background(white)"">Diana Roig Sanz, Laura Fólica y Ventsislav Ikoff</hi>
                    <hi rend=""background(white)"">, Universitat Oberta de Catalunya</hi>
                </p>
                <p rend=""Normal1"">
                    <hi rend=""background(white)"">El estudio de las publicaciones periódicas ha contado con una tradición académica generalmente acotada a los límites del espacio nacional; sin embargo, el giro transnacional que han experimentado las Humanidades y las Ciencias Sociales en los últimos años y la aparición de nuevas herramientas informáticas han permitido nuevos estudios con un marco interpretativo transnacional y a gran escala (Tolonen,</hi> 2016; Schelstraete y 
                    <hi rend=""background(white)"">Von Remoortel, 2017) que ha revitalizado la bibliografía existente sobre el estudio de la prensa cultural.</hi>
                </p>
                <p rend=""Normal1"">Ante este estimulante panorama, la reflexión sobre cómo analizar las traducciones diseminadas en las páginas periódicas no goza todavía del mismo interés y los investigadores se siguen abocando sobre todo a los aspectos temáticos, semánticos o semióticos de las publicaciones. No obstante, las traducciones publicadas en la prensa periódica constituyen un objeto de extrema y original riqueza a la hora de estudiar el proceso de recepción y transferencia de literaturas extranjeras, la difusión de autores y obras, la creación de redes transnacionales, así como el establecimiento o cuestionamiento de cánones literarios. Por otra parte, las herramientas digitales y el uso de los macrodatos permiten explorar estos aspectos desde una perspectiva espacial y temporal más amplia e inclusiva, que promueve la incorporación de voces hasta ahora desconocidas y de relaciones insospechadas, así como el cuestionamiento de distinciones estancas como la de ""centro"" y ""periferia"", no sólo en el campo cultural hispánico sino en el espacio internacional.</p>
                <p rend=""Normal1"">
                    <hi rend=""background(white)"" xml:space=""preserve"">Esta comunicación se propone reflexionar sobre el uso de herramientas digitales y metodologías basadas en </hi>
                    <hi rend=""italic background(white)"">big data</hi>
                    <hi rend=""background(white)"" xml:space=""preserve"">, con el objetivo de rastrear y analizar las traducciones literarias publicadas en una selección de revistas culturales hispánicas de la primera mitad del siglo </hi>
                    <hi rend=""smallcaps background(white)"">xx</hi>
                    <hi rend=""background(white)"">.</hi>
                </p>
                <p>
                    <hi rend=""background(white)"" xml:space=""preserve"">Este análisis a gran escala se realizará a partir de los datos recopilados en una base de datos relacional, </hi>basada en el entorno digital de exploración 
                    <ref target=""http://nodegoat.net/"">
                        <hi rend=""background(white)"">Nodegoat</hi>
                    </ref>
                    <hi rend=""background(white)"" xml:space=""preserve""> y creada para el proyecto </hi>
                    <ref target=""https://mapmodern.wordpress.com/mapping-modernity/"">
                        <hi rend=""background(white)"">MapModern</hi>
                    </ref>
                    <hi rend=""background(white)"" xml:space=""preserve"">, Esta base nos permitirá trabajar con distintas publicaciones y geografías del ámbito hispánico: por ejemplo, las revistas españolas </hi>
                    <hi rend=""italic background(white)"">La Gaceta literaria</hi>
                    <hi rend=""background(white)"" xml:space=""preserve""> y </hi>
                    <hi rend=""italic background(white)"">Revista de Occidente</hi>
                    <hi rend=""background(white)"" xml:space=""preserve"">, las argentinas </hi>
                    <hi rend=""italic background(white)"">Sur</hi>
                    <hi rend=""background(white)"" xml:space=""preserve""> y </hi>
                    <hi rend=""italic background(white)"">Proa</hi>
                    <hi rend=""background(white)"" xml:space=""preserve"">, la mexicana </hi>
                    <hi rend=""italic background(white)"">Los Contemporáneos</hi>
                    <hi rend=""background(white)"" xml:space=""preserve"">, la chilena </hi>
                    <hi rend=""italic background(white)"">Ercilla</hi>
                    <hi rend=""background(white)"">, entre otras.</hi>
                </p>
                <p>En este sentido, entre los problemas metodológicos que advertimos en el estudio de las traducciones publicadas en prensa periódica cabe señalar el límite del recorte o escala, la ausencia de datos o disparidad en sus grados de exhaustividad, la representatividad de los mismos, los errores generados a partir de la falta de coherencia entre índices y el contenido de las publicaciones, los problemas relativos a la digitalización de los materiales con tecnologías automáticas como OCR, la posible distorsión generada por las visualizaciones.</p>
                <p>
                    <hi rend=""background(white)"" xml:space=""preserve"">En definitiva, la reflexión metodológica sobre el estudio de las traducciones presentes en publicaciones periódicas nos permitirá arrojar luz a nuevas relaciones culturales entre estos órganos de prensa cultural en una escala hispánica muy amplia y en la diacronía del inicio del siglo </hi>
                    <hi rend=""smallcaps background(white)"">xx</hi>
                    <hi rend=""background(white)"">, nos permitirá descubrir mediadores/traductores, así como traducciones y originales hasta ahora no estudiados, promoviendo, de este modo, una apertura de los estudios literarios y culturales en espacios menos dominantes.</hi>
                </p>
                <p>Bibliografía:</p>
                <p>
                    <hi rend=""bold"">Fólica, L.</hi> (2010). La traducción literaria en el periodismo cultural: representaciones de autores, traductores y lengua. 
                    <hi rend=""italic"">Avatares de la comunicación y la cultura</hi>, 1: 122-143
                    <hi rend=""italic"">.</hi>
                </p>
                <p>
                    <hi rend=""bold"">Jockers, M.</hi> (2013). 
                    <hi rend=""italic"">Macroanalysis: Digital Methods and Literary History</hi>. Springfield: University of Illinois.
                </p>
                <p>
                    <hi rend=""bold"">Lafleur, H., et al.</hi> (2006). 
                    <hi rend=""italic"">Las revistas literarias argentinas (1893-1967)</hi>. Buenos Aires: El 8vo Loco.
                </p>
                <p>
                    <hi rend=""bold"">Sanz Roig, D.</hi> (2016). Hacia una nueva historia literaria: redes, mediadores culturales y humanidades digitales. 
                    <hi rend=""italic"">Puentes de Crítica</hi>: 40-49.
                </p>
                <p>
                    <hi rend=""bold"">Sanz Roig, D. y Meylaerts R.</hi> (2016). Edmond Vandercammen, médiateur culturel: le monde hispanique et le réseau du 
                    <hi rend=""italic"">Journal des Poètes</hi>. 
                    <hi rend=""italic"">Lettres Romanes</hi>, 70 (3-4): 405-433.
                </p>
                <p>
                    <hi rend=""bold"">Meylaerts, R., Sanz Roig D., Gonne, M. y Lobbes, T.</hi> (2016). Cultural Mediators in Cultural History: What do we learn from studying mediators’ complex transfer activities in interwar Belgium. In. Brems, E., Réthelyi, O. y Van Kalmthout, T. (eds), 
                    <hi rend=""italic"">The Circulation of Dutch literature</hi>. Leuven: Leuven University Press.
                </p>
                <p>
                    <hi rend=""bold"">Sanz Roig, D. y Meylaerts R.</hi> (en prensa). Paul Vanderborght and 
                    <hi rend=""italic"">La Lanterne sourde</hi>: networks and cultural mediation with the Spanish and Latin-American critics and translators. In D’haen, T. y Vandesbosch, D. (eds). 
                    <hi rend=""italic"">Literary Transnationalism(s)</hi>. Leiden/Boston: Brill.
                </p>
                <p>
                    <hi rend=""bold"">Schelstraete, J., y Van Remoortel, M.</hi> (2017). Towards a sustainable and collaborative data model for periodical studies. 
                    <hi rend=""italic"">Media History</hi>.
                </p>
                <p>
                    <hi rend=""bold"">Tolonen, M.</hi> (2016). Printing in a Periphery: a Quantitative Study of Finnish Knowledge Production, 1640-1828. 
                    <hi rend=""italic"">Digital Humanities 2016: Conference Abstracts</hi>. Jagiellonian University y Pedagogical University, Cracovia, 11–16 julio 2016.
                </p>
            </div>
        
    </text>",xml,Creative Commons Attribution 4.0 International,,hispanic and lusophone;network research;online repository;periodical publications;research environments,Spanish,"archives, repositories, sustainability and preservation;cultural and/or institutional infrastructure;cultural studies;literary studies;networks, relationships, graphs;spanish;spanish and spanish american studies;visualization"
6548,2018 - Mexico City,Mexico City,Puentes/Bridges,2018,ADHO,ADHO;EHD,El Colegio de México;Universidad Nacional Autónoma de México (UNAM) (National Autonomous University of Mexico),Mexico City,,Mexico,https://dh2018.adho.org/,REED London and the Promise of Critical Infrastructure,,Diane Katherine Jakacki;Susan Irene Brown;James Cummings;Kimberly Martin,"paper, specified ""short paper""","<text>
        
            <p>Alan​ ​Liu​ ​has​ ​called​ ​upon​ ​digital​ ​humanists​ ​to​ ​think​ ​more​ ​critically​ ​about​ ​infrastructure​ ​-​ ​the​ ​“social cum​ ​technological​ ​milieu​ ​that​ ​at​ ​once​ ​enables​ ​the​ ​fulfillment​ ​of​ ​human​ ​experience​ ​and​ ​enforces constraints​ ​on​ ​that​ ​experience” (Liu, 2017).​ ​Liu’s​ ​invitation​ ​comes​ ​at​ ​the​ ​moment​ ​when​ ​researchers​ ​involved​ ​in large-scale,​ ​long-term​ ​projects​ ​are​ ​shifting​ ​focus​ ​from​ ​remediation​ ​and​ ​the​ ​creation​ ​of​ ​digital incunabula​ ​to​ ​transmediation​ ​and​ ​the​ ​development​ ​of​ ​systems​ ​that​ ​support​ ​sustained​ ​discourse across​ ​ever-morphing​ ​digital​ ​networks,​ ​when​ ​we​ ​are​ ​recognizing​ ​the​ ​potential​ ​for​ ​“dynamism​ ​of​ ​the base​ ​or​ ​serialized​ ​form​ ​of​ ​the​ ​text—the​ ​state​ ​in​ ​which​ ​it​ ​is​ ​stored—as​ ​opposed​ ​to​ ​dynamic​ ​modes​ ​of presentation” (Brown, 2016: 288)​. ​REED​ ​London​ ​is​ ​one​ ​such​ ​project​ ​with​ ​a​ ​polyvalent​ ​dataset​ ​that​ ​spans​ ​over​ ​500 years’​ ​worth​ ​of​ ​archival​ records, ​embracing​ ​from​ ​the​ ​start​ ​the​ ​need​ ​to​ ​establish​ ​a​ ​stable, ​responsive production​ ​and​ ​presentation​ ​environment​ ​primed​ ​for​ ​use​ ​by​ ​a​ ​wide​ ​range​ ​of​ ​scholarly​ ​audiences. Thus​ ​we​ ​find​ ​that​ ​we​ ​are​ ​immediately​ ​testing​ ​those​ ​infrastructural​ constraints. ​In​ ​this​ ​paper, members​ ​of​ ​the​ ​REED​ ​London​ ​project​ ​team​ ​will​ ​address​ ​the​ ​challenges​ ​we​ ​face​ ​as​ ​we​ ​develop​ ​and implement​ ​a​ ​framework​ ​that​ ​trains​ ​us​ ​to​ ​think​ ​about​ ​our​ ​collected​ ​data​ ​in​ ​relation​ ​to​ ​much​ ​larger networks​ ​of​ ​disparate​ ​resources​ ​and​ ​user​ ​needs.</p>
            <p>REED​ ​London​ ​develops​ ​from​ ​a​ ​partnership​ ​between​ ​the​ ​Records​ ​of​ ​Early​ ​English​ ​Drama​ ​(REED) and​ ​the​ ​Canadian​ ​Writing​ ​Research​ ​Collaboratory​ ​(CWRC). ​Together​ ​we​ ​are​ ​establishing​ ​an​ ​openly accessible​ ​online​ ​scholarly​ ​and​ ​pedagogical​ ​resource​ ​of​ ​London-centric​ documentary, editorial, and bibliographic​ ​materials​ ​related​ ​to​ ​performance,​ ​theatre,​ ​and​ ​music​ ​spanning​ ​the​ ​period​ ​1100-1642. With​ ​support​ ​from​ ​the​ ​Andrew​ ​W.​ ​Mellon​ ​Foundation​ ​and​ ​a​ ​CANARIE​ ​Research​ ​Software​ ​Program grant,​ ​a​ ​team​ ​of​ ​researchers​ ​in​ ​the​ ​digital​ ​humanities​ ​and​ ​performance​ ​history​ ​from​ ​the​ ​U.S., Canada,​ ​and​ ​the​ ​U.K.​ ​are​ ​building​ ​a​ ​stable,​ ​extensible​ ​editorial​ ​production​ ​and​ ​publication environment​ ​that​ ​will​ ​create​ ​new​ ​possibilities​ ​for​ ​scholarly​ ​presentation​ ​of​ ​archival​ ​materials​ ​gathered from​ ​legal,​ ​ecclesiastical,​ ​civic,​ ​political,​ ​and​ ​personal​ ​archival​ ​sources​ ​in​ ​and​ ​around​ ​London.​ ​The REED​ ​London​ ​project​ ​combines​ ​materials​ ​from​ ​three​ ​printed​ ​REED​ ​collections​ (
                <hi rend=""italic"">Inns</hi> ​
                <hi rend=""italic"">of</hi>​ ​
                <hi rend=""italic"">Court</hi>, 
                <hi rend=""italic"">Ecclesiastical</hi>​ ​
                <hi rend=""italic"">London</hi>​,​ ​and​ 
                <hi rend=""italic"">Civic</hi>​ ​
                <hi rend=""italic"">London</hi>​ ​
                <hi rend=""italic"">to</hi>​ 
                <hi rend=""italic"">1558</hi>)​,​ ​the​ ​prosopographical​ ​material​ ​from​ ​REED’s 
                <hi rend=""italic"">Patrons</hi>​ ​
                <hi rend=""italic"">&amp;</hi>​ ​
                <hi rend=""italic"">Performances</hi>​ ​
                <hi rend=""italic"">(P&amp;P)</hi>​,​ ​the​ ​bibliographical​ ​materials​ ​of​ ​the​​ 
                <hi rend=""italic"">Early</hi>​ ​
                <hi rend=""italic"">Modern</hi>​ ​
                <hi rend=""italic"">London</hi>​
                <hi rend=""italic"">Theatres (EMLoT)</hi>​ ​​database,​ ​and​ ​in-progress​ ​and​ ​planned​ ​digital​ ​collections​ ​focusing​ ​on​ ​London​ ​area performance​ ​spaces,​ ​most​ ​notably​ ​the​ ​Globe,​ ​Rose,​ ​and​ ​Curtain​ ​theatres​ ​and​ ​Civic​ ​London 1559-1642.
            </p>
            <p>REED​ ​is​ ​an​ ​internationally​ ​renowned​ ​scholarly​ ​project​ ​that​ ​has​ ​worked​ ​to​ ​locate,​ ​transcribe,​ ​and​ ​edit evidence​ ​of​ ​drama,​ ​secular​ ​music,​ ​and​ ​other​ ​communal​ ​entertainment​ ​in​ ​Britain​ ​from​ ​the​ ​Middle Ages​ ​until​ ​1642.​ ​Since​ ​1979​ ​REED​ ​has​ ​published​ ​twenty-seven​ ​printed​ ​collections​ ​of​ ​transcribed records​ ​plus​ ​contextual​ ​materials.​ ​REED​ ​has​ ​long​ ​recognized​ ​the​ ​importance​ ​of​ ​online​ ​access​ ​to​ ​its resources,​​first​​ with 
                <hi rend=""italic"">P&amp;P</hi>​​​ and​​​ 
                <hi rend=""italic"">EMLoT</hi>​, ​​and​​ more​​ recently​​ with ​​the ​​born-digital​​ collection​ 
                <hi rend=""italic"">Staffordshire</hi>​. REED​ ​has​ ​wrestled​ ​with​ ​the​ ​balance​ ​between​ ​what​ ​was​ ​once​ ​considered​ ​its​ ​“core”​ ​print​ ​publication activities​ ​and​ ​“adjunct”​ ​digital​ ​efforts,​ ​in​ ​the​ ​process​ ​migrating​ ​its​ ​data​ ​across​ ​a​ ​succession​ ​of programs​ ​and​ ​formats​ ​from​ ​Basic​ ​and​ ​dBASE​ ​to​ ​TEI​ ​P5​ ​XML​ ​and​ ​MySQL (Hagen,​ ​MacLean,​ ​and​ ​Pasin, 2014).​ ​REED​ ​has​ ​developed​ ​its digital​​ resources ​​in​​ ways ​​that​​ complicate ​​integration (
                <hi rend=""italic"">P</hi>​
                <hi rend=""italic"">&amp;P</hi> ​​​exists ​​in​​ a ​​Drupal​​instance;​​​ 
                <hi rend=""italic"">EMLoT</hi> ​​​was built​ ​in​ ​a​ ​version​ ​of​ ​Django​ ​that​ ​is​ ​now​ ​out-of-date;​ ​
                <hi rend=""italic"">REED​ ​Staffordshire</hi>​ ​was​ ​lightly​ ​tagged​ ​in​ ​TEI​ ​and relies​ ​on​ ​EATSML for entity management, an​ ​XML​ ​format​ ​used​ ​by​ ​the​ ​Entity​ ​Authority​ ​Tool​ ​Set​ ​(EATS)​ ​for​ ​serialisation​ ​of​ ​its​ ​data).​ ​The​ ​components​ ​of​ ​REED​ ​London​ ​must​ ​therefore​ ​first be​ ​made​ ​intra-operable​ ​before​ ​they​ ​can​ ​become​ ​interoperable (Jakacki, 2016).​ ​The​ ​partnership​ ​with​ ​CWRC supports​ ​broader​ ​adoption​ ​of​ ​standards​ ​for​ ​TEI​ ​text​ ​markup,​ ​RDF​ ​metadata​ ​specifications,​ ​and named​​ entity ​​aggregation,​​ most ​​immediately​​ with ​​the ​​ingestion​​ of ​
                <hi rend=""italic"">EMLoT</hi> ​​​and ​​the​​ printed ​​​
                <hi rend=""italic"">Inns</hi> ​​
                <hi rend=""italic"">of Court</hi>​​ ​collection.
            </p>
            <p>CWRC​ ​is​ ​an​ ​online​ ​infrastructure​ ​project​ ​designed​ ​to​ ​enable​ ​unprecedented​ ​avenues​ ​for​ ​studying the​ ​words​ ​that​ ​most​ ​move​ ​people​ ​in​ ​and​ ​about​ ​Canada.​ ​Built​ ​with​ ​funding​ ​from​ ​the​ ​Canada Foundation​ ​for​ ​Innovation,​ ​the​ ​CWRC​ ​platform​ ​supports​ ​best​ ​practices​ ​in​ ​the​ ​production​ ​of​ ​online collections,​ ​editions,​ ​born-digital​ ​essays,​ ​anthologies,​ ​collections,​ ​monographs,​ ​articles,​ ​or bibliographies,​ ​and​ ​supports​ ​the​ ​inclusion​ ​of​ ​visual,​ ​audio,​ ​and​ ​video​ ​sources (​About​ ​CWRC/CSÉC).​ ​It​ ​supports collaboration​ ​through​ ​the​ ​use​ ​of​ ​interoperable​ ​data​ ​formats​ ​and​ ​interlinking​ ​of​ ​materials,​ ​and​ ​for teams​ ​like​ ​REED​ ​London​ ​provides​ ​invaluable​ ​tools​ ​for​ ​communicating,​ ​tracking​ ​activity,​ ​and workflow.​ ​We​ ​envision​ ​that​ ​as​ ​the​ ​partnership​ ​develops​ ​and​ ​as​ ​REED​ ​London​ ​advances​ ​through production​ ​toward​ ​publication​ ​we​ ​will​ ​take​ ​full​ ​advantage​ ​of​ ​CWRC’s​ ​functionality.​ ​From​ ​the​ ​start​ ​we have​ ​worked​ ​directly​ ​in​ ​CWRC’s​ ​unique​ ​editor,​ ​CWRC-Writer,​ ​which​ ​allows​ ​us​ ​to​ ​edit​ ​REED​ ​London records,​ ​essays,​ ​and​ ​bibliographical​ ​material​ ​using​ ​more​ ​diplomatic​ ​and​ ​critical​ ​TEI​ ​P5​ ​XML​ ​markup and​ ​at​ ​the​ ​same​ ​time​ ​creating​ ​semantic​ ​web​ ​annotations​ ​with​ ​RDF​ ​to​ ​identify,​ ​manage,​ ​and​ ​interlink entities​ ​contained​ ​within.​ ​The​ ​platform​ ​is​ ​also​ ​helping​ ​us​ ​to​ ​develop​ ​a​ ​better​ ​editorial​ ​workflow through​ ​management​ ​of​ ​access​ ​to​ ​data​ ​and​ ​editing​ ​by​ ​role,​ ​team​ ​communications,​ ​tracking​ ​and reporting​ ​of​ ​team​ ​activities.</p>
            <p>To​ ​ensure​ ​REED​ ​London’s​ ​stability​ ​and​ ​sustainability​ ​while​ ​extending​ ​its​ ​content​ ​and​ ​value​ ​to​ ​new generations​ ​of​ scholars​ ​the​ ​project​ ​is​ ​being​ ​built​ ​within​ ​the​ ​CWRC​ environment. ​The​ ​scope​ ​of​ ​REED London​ ​would​ ​not​ ​be​ ​possible​ ​without​ ​the​ ​sophisticated,​ ​integrated​ ​platform​ ​that​ ​CWRC​ ​provides. The​ ​focus​ ​of​ ​our​ ​first​ ​year​ ​is​ ​the​ ​design​ ​and​ ​construction​ ​of​ ​a​ ​collaborative​ ​online​ ​production​ ​and publication​ ​environment.​ ​Extending​ ​from​ ​CWRC’s​ ​existing​ ​integrated​ ​content​ ​management​ ​and preservation​ ​system,​ ​the​ ​enhanced​ ​environment​ ​will​ ​accommodate​ ​the​ ​range​ ​of​ ​record​ ​texts, editorial​ ​and​ ​bibliographical​ ​content​ ​from​ ​the​ ​source​ ​materials,​ ​while​ ​a​ ​customized​ ​browser-based CWRC-Writer​ ​platform​ ​will​ ​support​ ​the​ ​team’s​ ​goal​ ​of​ ​developing​ ​online​ ​editorial​ ​collaboration​ ​and review.​ ​The​ ​resulting​ ​streamlined​ ​production​ ​and​ ​publication​ ​environment​ ​will​ ​yield​ ​multi-faceted user-centered​ ​editions,​ ​meaning​ ​that​ ​agile​ ​component​ ​archival​ ​and​ ​editorial​ ​parts​ ​can​ ​cohere according​ ​to​ ​various​ ​criteria​ ​in​ ​response​ ​to​ ​scholars’​ ​research​ ​and​ ​teaching​ ​needs.​ ​In​ ​this​ ​way​ ​we are​ ​establishing​ ​a​ ​platform​ ​that​ ​produces​ ​new​ ​forms​ ​of​ ​“edition”​ ​that​ ​combine​ ​customized​ ​textual​ ​and contextual​ ​materials,​ ​exportable​ ​customized​ ​datasets​ ​and​ ​dynamic​ ​data​ ​visualizations.​ ​It​ ​also​ ​means that​ ​we​ ​will​ ​be​ ​able​ ​to​ ​realize​ ​the​ ​promise​ ​of​ ​extending​ ​the​ ​value​ ​of​ ​these​ ​materials​ ​to​ ​colleagues​ ​in fields​ ​beyond​ ​performance​ ​history,​ ​including​ ​political,​ ​religious,​ ​and​ ​cultural​ ​studies,​ ​and​ ​linguistics. </p>
            <p>The​ ​partnership​ ​between​ ​CWRC​ ​and​ ​REED​ ​allows​ ​us​ ​to​ ​explore​ ​the​ ​potential​ ​for​ ​new​ ​research applications​ ​associated​ ​with​ ​prosopography,​ ​networks,​ ​and​ ​deep​ ​contextualization.​ ​REED​ ​London’s wealth​ ​of​ ​references​ ​to​ ​very​ ​itinerant​ ​individuals​ ​across​ ​contemporaneous​ ​records​ ​means​ ​that​ ​we will​ ​be​ ​able​ ​to​ ​discern​ ​patterns​ ​through​ ​linking,​ ​analysis,​ ​and​ ​visualization.​ ​We​ ​will​ ​leverage​ ​REED’s named​ ​entities​ ​for​ ​linking​ ​people,​ ​places,​ ​events,​ ​and​ ​organizations.​ ​Our​ ​team​ ​has​ ​healthy​ ​debates about​ ​the​ ​problematic​ ​present​ ​of​ ​linked​ ​data.​ ​Brown​ ​has​ ​stated​ ​that,​ ​“linking​ ​up​ ​with​ ​other​ ​data means​ ​connecting​ ​one​ ​ontology​ ​to​ ​another,​ ​and​ ​this​ ​brings​ ​with​ ​it​ ​a​ ​pressure​ ​toward​ ​generalization rather​ ​than​ ​specificity” (Brown,​ ​Simpson,​ ​et.​ ​al.,​ ​2015).​ ​Cummings​ ​has​ ​posited​ ​that​ ​“being​ ​able​ ​to​ ​seamlessly​ ​integrate​ ​highly complex​ ​and​ ​changing​ ​digital​ ​structures​ ​from​ ​a​ ​variety​ ​of​ ​heterogeneous​ ​sources​ ​through interoperable​ ​methods​ ​without​ ​either​ ​significant​ ​conditions​ ​or​ ​intermediary​ ​agents​ ​is​ ​a​ ​deluded fantasy” (Cummings​ ​2014).​ ​Still,​ ​as​ ​a​ ​group​ ​we​ ​hope​ ​that​ ​by​ ​publishing​ ​our​ ​ontologies​ ​as​ ​a​ ​means​ ​of​ ​relating​ ​these entities​ ​as​ ​linked​ ​open​ ​data,​ ​we​ ​will​ ​be​ ​able​ ​to​ ​contribute​ ​to​ ​larger​ ​dialogues​ ​about​ ​class​ ​and​ ​society in​ ​Britain​ ​-​ ​certainly​ ​over​ ​the​ ​500​ ​years​ ​covered​ ​by​ ​REED​ ​London,​ ​but​ ​also​ ​about​ ​the​ ​development​ ​of Britain​ ​and​ ​Europe.​ ​CWRC​ ​content​ ​will​ ​be​ ​aggregated​ ​by​ ​the​ ​Advanced​ ​Research​ ​Consortium (ARC),​ ​and​ ​REED​ ​London​ ​will​ ​benefit​ ​from​ ​that​ ​aggregation,​ ​as​ ​we​ ​anticipate​ ​that​ ​people​ ​who​ ​figure in​ ​the​ ​REED​ ​London​ ​corpus,​ ​such​ ​as​ ​Elizabeth​ ​I,​ ​Francis​ ​Bacon,​ ​and​ ​Inigo​ ​Jones​ ​will​ ​be discoverable​ ​by​ ​scholars​ ​searching​ ​for​ ​these​ ​known​ ​figures​ ​across​ ​other​ ​linked​ ​resources.​ ​Perhaps more​ ​important,​ ​REED​ ​London​ ​records​ ​include​ ​extended​ ​references​ ​to​ ​thousands​ ​of​ ​Londoners​ ​who were​ ​in​ ​some​ ​way​ ​connected​ ​to​ ​performance,​ ​but​ ​who​ ​were​ ​not​ ​defined​ ​by​ ​that​ ​connection:​ ​civic officials,​ ​guild​ ​members,​ ​lawyers,​ ​clerks,​ ​priests,​ ​etc.​ ​The​ ​work​ ​of​ ​this​ ​project​ ​thus​ ​holds​ ​as​ ​yet unrealized​ ​value​ ​for​ ​a​ ​much​ ​broader​ ​understanding​ ​of​ ​British​ ​historical​ ​subjects.</p>
            <p>Working​ ​within​ ​CWRC’s​ ​platform​ ​and​ ​optimizing​ ​CWRC-Writer​ ​has​ ​allowed​ ​the​ ​core​ ​REED​ ​London team​ ​to​ ​move​ ​efficiently​ ​to​ ​an​ ​advanced​ ​planning​ ​phase.​ ​By​ ​the​ ​end​ ​of​ ​2017​ ​we​ ​will​ ​have designed​ ​templates​ ​for​ ​all​ ​record​ ​formats​ ​from​​ 
                <hi rend=""italic"">Inns of Court</hi>​ ​​and​ ​mapped​ ​database​ ​fields​ ​from 
                <hi rend=""italic"">EMLoT</hi>​​ ​to​ ​align​ ​with​ ​the​ ​record​ ​parts​ ​from​ ​the​ ​print​ ​collections.​ ​We​ ​will​ ​have​ ​harvested​ ​a​ ​preliminary “white​ ​list”​ ​of​ ​named​ ​entities​ ​(people,​ ​places,​ ​organizations)​ ​from​ ​all​ ​three​ ​print​ ​​​collection​ ​indexes, P&amp;P,​ ​and​ ​Staffordshire.​ ​Because​ ​of​ ​this​ ​efficient​ ​onramp​ ​we​ ​will​ ​be​ ​able​ ​to​ ​focus​ ​in​ ​the​ ​first​ ​half​ ​of 2018​ ​on​ ​ingesting​ ​data,​ ​records,​ ​and​ ​contextual​ ​materials​ ​from​ ​Inns​ ​of​ ​Court​ ​and​ ​EMLoT.​ ​We​ ​will test​ ​the​ ​REED-specific​ ​entity​ ​list​ ​on​ ​ingested​ ​materials.​ ​We​ ​will​ ​also​ ​begin​ ​to​ ​user-test​ ​the​ ​editorial workflow​ ​system​ ​with​ ​the​ ​larger​ ​project​ ​team​ ​of​ ​REED​ ​editors​ ​and​ ​staff.​ ​By​ ​June​ ​2018​ ​we​ ​will​ ​have begun​ ​semantic​ ​tagging​ ​and​ ​experimentation​ ​with​ ​the​ ​CWRC​ ​HuViz​ ​semantic​ ​web​ ​visualization​ ​tool. At​ ​the​ ​DH​ ​2018​ ​conference​ ​we​ ​will​ ​report​ ​on​ ​further​ ​customization​ ​of​ ​the​ ​CWRC​ interface,​ our​ ​plans for​ ​data​ ​discovery​ ​and​ ​research​ ​collaboration,​ ​and​ ​present​ ​preliminary​ ​plans​ ​for​ ​user-responsive editions​ ​and​ ​data​ ​linkage.
            </p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"">Brown, ​S.</hi> (2016). ​Tensions​ ​and​ ​Tenets​ ​of​ ​Socialized​ ​Scholarship. 
                        <hi rend=""italic"">D</hi>​​
                        <hi rend=""italic"">igital</hi>​ ​
                        <hi rend=""italic"">Scholarship</hi>​ ​
                        <hi rend=""italic"">in</hi>​ ​
                        <hi rend=""italic"">the Humanities</hi>​, ​31 (2): 283-300.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Brown, S.​, ​Simpson, J.,</hi> ​
                        <hi rend=""bold"">CWRC​ ​Project​ ​Team, and ​Inke​ ​Project​ ​Team.</hi> (2015) ​An​ ​Entity​ ​By​ ​Any​ ​Other Name:​ ​Linked​ ​Open​ ​Data​ ​as​ ​a​ ​Basis​ ​for​ ​a​ ​Decentered,​ ​Dynamic​ ​Scholarly​ ​Publishing​ ​Ecology. 
                        <hi rend=""italic"">Scholarly</hi>​ ​
                        <hi rend=""italic"">and</hi>​ ​
                        <hi rend=""italic"">Research</hi>​ ​
                        <hi rend=""italic"">Communication</hi>​​ ​6 (2). 
                        <ref target=""http://src-online.ca/index.php/src/article/view/212/409"">http://src-online.ca/index.php/src/article/view/212/409</ref>.
                    </bibl>
                    <bibl>
                        <hi rend=""italic"">Canadian Writing Research Collaboratory</hi> ​project​ ​website.​ ​​
                        <ref target=""http://www.cwrc.ca/en/"">http://www.cwrc.ca/en/</ref>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Cummings,​ ​J.</hi> (2014).​ ​The​ ​Compromises​ ​and​ ​Flexibility​ ​of​ ​TEI​ ​Customisation.​ In​ ​Mills, C., ​ ​Pidd, M.​ ​and​ Ward, E.​ (eds), 
                        <hi rend=""italic"">Proceedings of the Digital Humanities Congress 2012</hi>.​ ​
                    </bibl>
                    <bibl>CWRC:​ About​ ​CWRC/CSÉC​ ​webpage.​ ​​
                        <ref target=""http://www.cwrc.ca/about/#whatis"">http://www.cwrc.ca/about/#whatis</ref>
                    </bibl>
                    <bibl>CWRC​ ​Humanities​ ​Visualizer​ ​webpage. ​​
                        <ref target=""http://www.cwrc.ca/uncategorized/huviz-tool/"">http://www.cwrc.ca/uncategorized/huviz-tool/</ref>
                    </bibl>
                    <bibl>
                        <hi rend=""italic"">Early Modern London Theatres</hi>​​ ​website.​ ​​
                        <ref target=""http://www.emlot.kcl.ac.uk"">http://www.emlot.kcl.ac.uk</ref>
                    </bibl>
                    <bibl>Entity Authority Tool Set​ ​(EATS)​ ​website. ​​
                        <ref target=""https://eats.readthedocs.io/en/latest/index.html"">https://eats.readthedocs.io/en/latest/index.html</ref>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Hagen, T.,​ ​MacLean, S.,​ ​and​ ​Pasin, M.​</hi> (​2014).​ ​Moving​ ​Early​ ​Modern​ ​Theatre​ ​Online: the Records​ ​of​ ​Early​ ​English​ ​Drama​ ​introduces​ ​the​ ​Early​ ​Modern​ ​London​ ​Theatres. http://static.michelepasin.org/public_articles/2014-REED_McLean-Pasin.pdf
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Jakacki, D.</hi> (2017)​ ​REED​ ​London:​ ​Humanistic​ ​Roots,​ ​Humanistic​ ​Futures.​ ​Paper​ ​given​ ​at​ ​MLA​ ​2017. 
                        <ref target=""http://dx.doi.org/10.17613/M67794"">http://dx.doi.org/10.17613/M67794</ref>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Jakacki, D.</hi>​ (2016) REED​ ​and​ ​the​ ​Prospect​ ​of​ ​Networked​ ​Data. Paper​ ​given​ ​at​ ​the​ ​Conference​ ​of​ ​the​ ​Canadian Society​ ​for​ ​Renaissance​ ​Studies.​http://dx.doi.org/10.17613/M6CK59
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Liu, A.</hi> (2017)​ ​Toward​ ​Critical​ ​Infrastructure​ ​Studies"",​ ​paper​ ​given​ ​at​ ​the​ ​University​ ​of​ ​Connecticut.​ ​​
                        <ref target=""https://www.youtube.com/watch?v=2ojrtVx7iCw"">https://www.youtube.com/watch?v=2ojrtVx7iCw</ref>
                    </bibl>
                    <bibl>Records​ ​of​ ​Early​ ​English​ ​Drama​ ​project​ ​website. h​​ttp://reed.utoronto.ca 
                        <lb></lb>
                    </bibl>
                    <bibl>
                        <hi rend=""italic"" xml:space=""preserve"">REED​ Patrons and Performances </hi>website. https://reed.library.utoronto.ca
                        <lb></lb>
                    </bibl>
                    <bibl>
                        <hi rend=""italic"">REED</hi>​ 
                        <hi rend=""italic"">Staffordshire</hi>​​ ​Collection​ ​website. ​​https://ereed.library.utoronto.ca/collections/staff/
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,Creative Commons Attribution 4.0 International,,archives;lod;london;performance history;tei,English,"archives, repositories, sustainability and preservation;cultural and/or institutional infrastructure;cultural studies;digital ecologies and critical infrastructure studies;english;english studies;historical studies;ontologies"
9296,2020 - Ottawa,Ottawa,carrefours / intersections,2020,ADHO,ADHO,Carleton University;Université d'Ottawa (University of Ottawa),Ottawa,Ontario,Canada,https://dh2020.adho.org/,Ten years recovering the memory of republican exile with citizen collaboration. The results of E-xiliad@s Project: a perspective from the Digital Humanities and the Digital Public History.,,Lidia Bocanegra Barbecho,poster / demo / art installation,"The Spanish republican exile was the result of the Republican defeat in 1939 by the Francoist army, led by the general Francisco Franco. Nearly half a million-people had to go into mass exile during the months of January and February, through the French border crossings. Many other exiles did so, months later, from Alicante to the North African coasts. These places of destination were, in most cases, places of passage to successive destination countries in Europe and, especially, in Latin America. The international nature of this historical event means that there is currently a large number of personal files scattered in different places around the world. In order to recover these stories, the e-xiliad@s project was conceived in 2009, with a Digital Humanities and Public History perspective: www.exiliadosrepublicanos.info. It is an crowdsourcing project that, through a multilanguage digital platform, retrieves unpublished documents about the anonymous exiled. From the research point of view, the privileged target audience is composed by relatives and friends of the exiles and those interested in the subject. This initiative funded twice (2009 and 2011) by the General Directorate of Migrations of the Spanish Ministry of Employment and Social Security, uses a methodology created ad hoc to obtaining data based on public participation from citizen science. That is, the content is generated on-line by the public at an international level and coordinated by a scientific specialist. For almost a decade, this crowdsourcing project has been developing an online public engagement strategy for public participation based on open data, supported by a custom digital platform and its digital social networks, with more than 1.500 followers. At this stage, the project recovered around five hundred unpublished archives among photographs, memories, official documents, letters and interviews, that comes associated with about two hundred completed exile records. The vast majority of these data are public, thanks to the informed consent of the author.Figure 1. Examples of documents and photos provided by relatives of exiles registered on the project platform and collaborators.At technological level, e-xiliad@s has been built using Drupal 6 LTS (Long Term Support) with a MySQL database. The technological solution answer the need of the internal survey form that has been created using a specific data model connected with a strategic communication plan, that serves to get data online via user’ confidence with the project and to stimulate his/her family memoir.Figure 2. Detail of the internal form to be filledAs lesson learned, the need to maintain a technology that is getting older without official support (Drupal); the management of a project social network community continuously growing; and the spamming problem due to the project popularity within the republican exile community (more than 11.000 spamming users) are questions that we are trying to solve. Despite this, E-xiliad@s acts as a digital identity place for those connected to this topic and as a space that informs society, with scientific rigor, from the field of Digital Public History. Currently there are six large online social network communities that move much of the information on the subject of Republican exile and the Spanish Civil War, among which is the e-xiliad@s project with its social networks (Facebook: @exiliados.republicanos and Twitter: @exiliadas. They play a significant role at the national and international level regarding the recovery of historical memory, due to the quantity and quality of the information it offers. Graph analysis that represents the first and second level connections of the social networks of the E-xiliad@s project, which is represented in the community in green.",txt,This text is republished here with permission from the original rights holder.,,Citizen science;Crowdsourcing;Digital Public History;Open Data;Spanish Republican exile,English,20th century;contemporary;crowdsourcing;english;europe;history;humanities computing;north america;public humanities collaborations and methods;south america
9500,2020 - Ottawa,Ottawa,carrefours / intersections,2020,ADHO,ADHO,Carleton University;Université d'Ottawa (University of Ottawa),Ottawa,Ontario,Canada,https://dh2020.adho.org/,Enhancing Community through Open DH Website Design,,Rennie Mapp;Christian Howard-Sukhil,lightning talk,"Our lightning talk offers solutions to some shortcomings in communication about DH projects and undertakings on university campuses, particularly through the development of institutional DH websites. By an “institutional DH website,” we mean a community website, hosted by a given university or institution, that is explicitly devoted to the advancement, support, and promotion of DH work collectively. A number of previous attempts to establish institutional DH websites have failed, and there is a growing need to understand how we can sustainably create and maintain such sites in a way that meets the diverse needs of DH scholars. To this end, we offer an alternative approach for creating such communal sites that is designed for specific communities. More than merely providing a definition of DH and a set of resources for those interested in the field, institutional DH websites can beneficially act as community hubs for DH practitioners by showcasing live projects and encouraging interdisciplinary collaboration. While there is no one-size-fits-all solution, an open development process can help scholars and DH staff who face long-standing DH challenges around methodological innovation, data reproducibility, reinvention of the wheel, and the balance of technical and humanistic priorities. In particular, we offer a user-focused development process for DH websites, which emphasizes the identification and enhancement of human networks and communities of practice. At the most basic level, user-focused design starts with a needs assessment of the website’s primary audience and is refined through attention to typical user needs and exemplary uses throughout the project’s lifecycle in order to maintain an active user community. This stands in contrast to the design of institutional DH sites as a means of cataloguing the services or offerings at a specific institution. At the structural level, user-focused design for institutional DH sites foregrounds open access and accessibility by thinking about these concerns throughout the design process (rather than as a last-minute add-on). The two speakers have designed institutional DH websites at the University of Virginia and Bucknell University, each employing a different platform (Drupal and WordPress); nonetheless, both sites promote similar design philosophies. The talk will model how institutions can create similar sites designed for their own communities with an eye toward developing appropriate use cases and sustainability practices.",txt,This text is republished here with permission from the original rights holder.,,design;website development,English,"contemporary;design studies;english;informatics;interface design, development, and analysis;north america;project design, organization, management"
9654,2020 - Ottawa,Ottawa,carrefours / intersections,2020,ADHO,ADHO,Carleton University;Université d'Ottawa (University of Ottawa),Ottawa,Ontario,Canada,https://dh2020.adho.org/,The Historic Graves crowdsourcing project: 10 years transcribing Irish history,,Maurizio Toscano;John Tierney,poster / demo / art installation,"Irish diaspora is how we refer to the historical process of migration from Ireland, recorded since the Early Middle Ages, but particularly evident since the XVIII century. By the 21st century, according to figures in the Irish Emigration Patterns and Citizens Abroad report published in 2017 by the Irish Department of Foreign Affairs and Trade, an estimated 70 million people worldwide claimed some Irish descent, that largely maintain a connection with Irish cultural identity and heritage.The online project Historic Graves (https://historicgraves.com/) capitalized on this global phenomenon, putting together a worldwide community of more than 15,000 users, collaborating in generating a nationwide genealogical dataset. The project began in 2010, from an idea of John Tierney developed by Maurizio Toscano, both still at the forefront of the initiative. It started and still runs as a community focused grassroots heritage project, where local community groups living in Ireland are trained in low-cost high-tech field survey of historic graveyards and recording of their own oral histories. In almost a decade, historicgraves.com published online more than 800 graveyards, recording at least a photo and the location of 99,235 graves and collaboratively transcribing the details of 199,851 people, and counting.Community co-production happens within a freely available online platform, created for the transcription of memorial epitaphs. Training workshops are offered to local communities interested in contributing to surveying and transcribing historic graveyards. The combination of online interaction with local workshops and meetings works best in terms of ensuring meaningful participation. Data gathering and management procedures have proved to be essential on two fronts: data collected in the field is normally available online in short time; on the website, memorials’ transcription is unmediated and immediately available to share. Instant publication proved to be highly engaging for the volunteer groups involved: they see immediate results for their work and are willing to share them with family and friends living abroad. Each local community can then download individual datasets of their own records, as tabular Open Data. The global community of users takes responsibility for quality control and completeness.In 2018, the project has received a Heritage Council grant, that supported the upgrading of the online platform (backend and frontend), and it has been part of the initiative European Year of Cultural Heritage (#EuropeForCulture). From a technical point of view, data, code and layout have been upgraded from Drupal 6 to Drupal 7, with a plan to move to BackdropCMS or Drupal 9 in late 2021, when both Drupal 7 and Drupal 8 will reach ""End of Life"".The Historic Graves project is fundamentally a Public History project, originated from local communities and that constantly relies on them to grow, solve ethical issues and stay sustainable. Technologies, and the Web in particular, have played a big role, both enabling remote transcriptions and stimulating participation, although they never been the focus of the initiative. Additionally, in line with public digital humanities principles, the project has always been open to participation, well beyond the academy, growing as a group of networked and engaged communities, not just as a repository for content.",txt,This text is republished here with permission from the original rights holder.,,Collaborative platforms;Crowdsourcing;cultural heritage;Ireland;Public History,English,18th century;19th century;20th century;crowdsourcing;english;europe;global;history;public humanities collaborations and methods
9759,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,Hyper Audio Linking – Generating Hybrids of Text and Video Content for Digital Publishing,,Agnes Blaha;Andreas Leo Findeisen,workshop / tutorial,"<text>
        
            <div>
                <p>The Digital Humanities as a field of research and publication face both new opportunities and challenges due to the increasing number of media sources relevant to researchers and students, no matter their age and origin. Aggregator platforms like YouTube or Vimeo let formerly unknown materials surface that are relevant for new research in the humanities. Meanwhile, large public and private collections held by historical organizations, broadcasting corporations and NGOs, or individuals like artists, scientists or politicians remain to be indexed, investigated and re-published (Sommer, 2016). At the same time, researchers who are experimenting with new, creative ways to combine curating, scholarship and presentation draw attention to the immersive and narrative potential of sound-based media (see e.g. Barber, 2017; Murray and Wiercinski, 2014; Cohen, Rakerd, Rehberger &amp; Boyd 2012).</p>
                <p>Some of the use cases relevant to the field of digital humanities that will probably become even more frequent over the next years include:</p>
                <list type=""unordered"">
                    <item>Support for oral history researchers who need to transcribe large amounts of recordings</item>
                    <item>Using transcripts for lectures and online learning tools, both for students and the public at large</item>
                    <item>Enhancing the usability of digital A/V archives via full text search</item>
                    <item>Facilitating community contribution and citizen science projects</item>
                    <item>Furthering the accessibility of A/V content (e.g. for deaf people)</item>
                    <item>Intertwining multimedia material and text for research papers published in online journals</item>
                </list>
                <p>Alternating between more theory-focused discussion and hands-on experience, we will different methods to integrate transcripts into multimedia formats that are useable for research and publications. After a brief introduction into the principles and current possibilities as well as the limitations of computerized speech to text transcription, particularly in comparison with tools for manual transcription such as WebAnno and OCTRA, we will introduce participants to a small number of different speech-to-text software packages that can be used to semi-automatically transcribe A/V content, while discussing their respective pros and cons.</p>
                <p>We will then demonstrate how to use the open source package ffmpeg to extract and convert various types of A/V recordings to formats suitable for further processing and try out automatic speech to text conversion with OH portal, an online transcription tool developed and maintained by the phonetics research team at the university of Munich (
                    <ref target=""https://www.phonetik.uni-muenchen.de/apps/oh-portal/"">https://www.phonetik.uni-muenchen.de/apps/oh-portal/</ref>) and Watson, a powerful, trainable natural language processing tool developed by IBM. 
                </p>
                <p>For many in the DH community, creating a good transcript is just the start. Presenting research to fellow academics and the public at large, garnering interest for archives and projects, creating lively and easy to use learning material - all while preserving the non-verbal aspects of raw sources - are often just as important. This is probably even more true for novel, hybrid formats which have been theorized to be result of an amalgamation of analog and digital publishing (Ludovico, 2012).</p>
                <p>As an exemplary solution, we will introduce Hyper Audio Linking, a technique for the presentation of digital content that allows to link transcripts to pre-defined jump marks in video or audio recordings via JavaScript. HAL augmentation deals with diverse content aspects of a source while letting the original untouched. It can be used for all kinds of sources, be it contemporary recordings of lectures and panel discussions, interviews, or historical footage, as well as some art genres like theatre plays, films, or audio dramas. By defining sections or “chapters” and linking them to timestamps, the transcript can be used to jump between those parts in the original media, allowing recipients to switch between reading or viewing/listening mode. The resulting multi-media hybrid can be further augmented by defining formatting options for different types of content and by including various metadata, images, annotations, keywords, and the like. The result is an elegant and easy to use frontend interface that allows for full-text search and easy navigation within A/V content.</p>
                <p>A simplified workflow of an oral history project that uses HAL both as support for researchers and to publish research results and multimedia documents can be seen in figure 1. Importantly, HAL-augmented files can not only be used for the final step of publishing and presenting results. Depending on the configuration of jump marks, they can also support members of the research team in navigating recordings or in tracing observations, selected utterances and topics over multiple sources. Meanwhile, the method is flexible enough to be introduced on the fly at any point in time. If desired, its use may also be restricted to a certain part of collected sources, e.g. only those that are selected for public access.</p>
                <figure>
                    <graphic n=""1001"" width=""15.993180555555556cm"" height=""8.653638888888889cm"" url=""Pictures/f8f7bd6e2ad601e031f88e63e7d5676f.jpeg"" rend=""inline""></graphic>
                </figure>
                <p>Figure 1: Sample workflow with HAL integrated in an oral history project</p>
                <p>Using an existing interface to a database of videos on history and politics of digital culture, participants will experiment with different ways to use HAL links within transcripts. We will discuss and compare our Drupal-based implementation with other existing tools such as ELAN, an annotation tool for A/V content developed at the Max Planck Institute for Psycholinguistics in Nijmegen. We will also talk about some of the key differences between software designed to work as a standalone tool for researchers versus a method developed for usage either as the interface of an online archive or as an enrichment or additional feature of web publishing formats.</p>
                <p>To round off the workshop, we will start to build a web page featuring A/V content, a transcript, and additional material such as photographs and annotations from scratch. Using just a text editor and a few simple lines of Html and JavaScript code, participants will learn to apply HAL augmentation to their own publications. We will use a pre-configured installation of the open source content management system Drupal that can be customized to fit different use cases, including curated online collections, project documentation, event pages etc.</p>
                <p>For this second practice part of the workshop, participants should ideally bring their own A/V sources and start working on their personal project. Those who don’t have a specific use case in mind (yet) will be able to choose among different sources provided by the workshop team instead.</p>
                <p>The last workshop unit will be a discussion dealing with the editorial planning for publication. Whether you are an individual producer, a research team or an online cluster, you will have to make certain decisions in how to manage the publication: Who is the audience that will be primarily addressed? Which specific needs have to be taken into account with regards to usability, accessibility and technological literacy? How much (if any) pre-existing knowledge on the topic can be assumed, and how much introductory information should be included?</p>
                <div>
                    Target audience and requirements
                    <p>Previous workshops and presentations of Hyper-Audio-Linking have been held at BASICS, transmediale Berlin, at the Ludwig-Boltzmann-Institute for Media.Art.Research, Linz, and most recently as part of the EADH conference Galway 2018. Based on these earlier workshops on similar topics, we expect to work with a group of between ten and fifteen participants, which seems ideal for a more hands-on experience.</p>
                    <p>There are no skill requirements for participation, all introductions into software usage and programming will be suitable for beginners but can be easily adapted to participants with more advanced levels of pre-existing knowledge.</p>
                    <p>Participants should bring their own laptop. Participants who use a laptop provided by their employer should make sure to either have all software pre-installed or know that they are authorized to install software on their computer.</p>
                    <p>All required software is either open access or free to use, there will be no additional costs.</p>
                </div>
                <div>
                    Pre-conference support and provision of material
                    <p>Participants who would like to familiarize themselves with HAL in advance will be given access to stubs of prepared A/V content on our platform Transforming Freedom.org four weeks before the conference start. </p>
                    <p>Download links for other software packages and instruction material on how to install and use the respective software will be made available to the participants about two weeks in advance of the workshop. </p>
                </div>
                <div>
                    Intended length and format
                    <p>The duration of the workshop will be a half day, that is, approximately four hours including breaks. As far as the format is concerned, we will alternate between theory-focused presentation, group discussion, and practical tasks that let participants try out different techniques supported by the instructors.</p>
                </div>
            </div>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"">Barber, J. F.</hi> (2017). Radio Nouspace: Sound, Radio, Digital Humanities. 
                        <hi rend=""italic"">Digital Studies/Le champ numérique, 7(1), 1.</hi> DOI: 
                        <ref target=""http://doi.org/10.16995/dscn.275"">http://doi.org/10.16995/dscn.275</ref> (accessed 05 May 2019).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Cohen, S., Rakerd, B., Rehberger, D., &amp; Boyd, D. A.</hi> (2012). Oral history in the digital age: the imperative for rethinking best practices based on a survey of the field(s). Retrieved from 
                        <ref target=""http://ohda.matrix.msu.edu/2012/07/ohda-survey/"">http://ohda.matrix.msu.edu/2012/07/ohda-survey/</ref>. (accessed 04 May 2019).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ludovico, A.</hi> (2012). Post-Digital Print: The Mutation of Publishing since 1894. 
                        <hi rend=""italic"">Onomatopee 77</hi>.
                    </bibl>
                    <bibl style=""text-align:left;"">
                        <hi rend=""bold"">Murray, A. and Wiercinski, J.</hi> (2014). A Design Methodology for Sound-based Web Archives. 
                        <hi rend=""italic"">Digital Humanities Quarterly</hi> 8.2, 
                        <ref target=""http://digitalhumanities.org/dhq/vol/8/2/000173/000173.html"">http://digitalhumanities.org/dhq/vol/8/2/000173/000173.html</ref> (accessed 05 May 2019).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sommer, B.</hi> (2016). 
                        <hi rend=""italic"">Practicing Oral History in Historical Organizations</hi>. London: Routledge.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,digital publishing;hybrid publishing;speech to text;transcripts,English,"audio, video, multimedia;communication and media studies;crowdsourcing;digital archives and digital libraries;digital humanities (history, theory and methodology);english;scholarly editing"
9783,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,A Database of Islamic Scientific Manuscripts — Challenges of Past and Future,,Robert Casties,"paper, specified ""short paper""","<text>
        
            <div type=""div1"" rend=""DH-Heading1"">
                The ISMI project
                <p>The Islamic Scientific Manuscript Initiative (ISMI) project was founded in 2005 to make accessible information on all Islamic manuscripts in the exact sciences (astronomy, mathematics, optics, mathematical geography, music, mechanics, and related disciplines), whether in Arabic, Persian, Turkish, or other languages from the 9
                    <hi rend=""sup"">th</hi> to the 19
                    <hi rend=""sup"">th</hi> century
                    <note xml:id=""ftn1"" place=""foot"" n="""">ISMI website: 
                        <ptr target=""https://ismi.mpiwg-berlin.mpg.de/""></ptr>
                    </note> The ISMI project limits itself to “scientific” manuscripts but it tries to encompass all such manuscripts worldwide regardless of their current location and it tries to record as much information about these manuscripts as available, including reader and ownership marks, annotations and illustrations, making it possible to learn more about structures and practices of knowledge in the islamicate world (Ragep et al., 2008).
                </p>
                <div type=""div2"" rend=""DH-Heading2"">
                    The database
                    <p>The database of the ISMI project is a cooperation project by the Max Planck Institute for the History of Science and the Institute of Islamic Studies at McGill University in Montreal. The database has been built up over more than ten years starting from an early personal database project by the involved scholars, extended by corrected information from catalog works like MAMS (Matvievskaya et al., 
                        <emph>1983</emph>
                        <emph>)</emph> and personal research by the scholars in the project and outside. It currently contains information about over 4700 texts in 15000 witnesses in 8000 codices and 2500 persons and an accompanying secondary bibliography of 2700 titles and it is constantly being extended.
                    </p>
                    <p>The database development started in 2006 with a new data model based on the idea of a network of flexible objects and relations. Objects can have arbitrary attributes and the relations between objects are also like objects and can have attributes.</p>
                    <p>
                        <figure>
                            <graphic url=""Pictures/b78eb750f60ee5a3e17f52f8c1f2067b.png""></graphic>
                            Part of current ISMI data model showing relations between text, witness, person and codex objects.
                        </figure>
                    </p>
                    <p>The basic objects in the data model are for example the TEXT which is abstract, the WITNESS which is a concrete material manuscript and the PERSON (real or imaginary). These objects are connected by relations like 
                        <hi rend=""italic"">is_exemplar_of</hi> which connects a text and its witnesses and 
                        <hi rend=""italic"">was_created_by</hi> which connects a text and a person as its author (see Figure 1). The same person can at the same time also be connected to other witnesses as a copyist or as a dedicatee. This very flexible data model was regularly modified and extended to accommodate changes and refinements that were developed in close cooperation with the scholars entering the data as their understanding of the source material and the technical possibilities of the system changed. Examples of theses unique additions are the possibility to record misattributions of authorship and misidentifications of witnesses in existing literature as well as documented reading events and changes of ownership.
                    </p>
                    <p>This concept of a network of objects with flexible relations, an attribute-graph, exists today in database products like Neo4J
                        <note xml:id=""ftn2"" place=""foot"" n="""">Neo4J: 
                            <ptr target=""https://neo4j.com/""></ptr>
                        </note> but those were not available in 2006 which led to the development of a custom database called ""OpenMind"". The database software is Open Source, written in Java, uses a conventional SQL database backend and a Web-based frontend.
                    </p>
                    <p>A first version of a public website presenting a limited set of 130 codices by the Staatsbibliothek Berlin with digitalizations was published in 2015.</p>
                </div>
                <div type=""div2"" rend=""DH-Heading2"">
                    Towards new standards
                    <p>The current database system OpenMind was a custom development which was necessary at the time of its creation but has not aged well and burdens the future development of the project with limited flexibility and high maintenance for software development. The data model was also not created based on existing ontologies due to a lack of usable tools at the time. Both features were acceptable during the development of the project but they pose a problem to the continued maintenance of the project and the reusability of its data.</p>
                    <p>Currently both software and data are migrated to new standard tools in two phases:</p>
                    <p>In the first phase data is still entered in the legacy OpenMind backend but there is a new public web frontend based on the Drupal CMS that is fed by an XML export from the legacy backend. The XML data is also fed into a Neo4J graph database for additional queries and visualisations. This is the architecture for the beta launch in September 2018 and the public launch end of November 2018.</p>
                    <p>In the second phase the data model will be migrated to the CIDOC-CRM
                        <note xml:id=""ftn0"" place=""foot"" n="""">CIDOC-CRM: 
                            <ptr target=""http://www.cidoc-crm.org/""></ptr>
                        </note> reference ontology using the FRBRoo
                        <note xml:id=""ftn6"" place=""foot"" n="""">FRBRoo: 
                            <ptr target=""https://www.ifla.org/publications/node/11240""></ptr>
                        </note> model and other extensions. All data is converted to RDF following the new data model and a frontend based on the ResearchSpace
                        <note xml:id=""ftn7"" place=""foot"" n="""">ResearchSpace: 
                            <ptr target=""https://www.researchspace.org/""></ptr>
                        </note> software and a triple store backend is created for data entry and specialized queries and visualisations. This process is currently under way.
                    </p>
                </div>
                <div type=""div2"" rend=""DH-Heading2"">
                    The new ISMI website
                    <p>The new public website presents data on 650 persons (selected chronologically following MAMS), 2300 texts, 6900 witnesses and related objects, representing authors from before 1350CE. The website will be public starting end of November 2018. Additional data publications are in preparation.</p>
                    <p>The new web frontend provides browsable lists of all major object types (persons, texts/works, witnesses, codices, places,…) as well as a general search and searches for specific object types. All objects on the pages are linked which makes it easy to get from a person to all their works and their witnesses as well as to the commentaries on the titles and their supercommentaries.</p>
                    <p>The search has a simple normalization for Arabic and a special normalization for romanized Arabic and is specially tuned to be very forgiving for differences in spelling especially for Arabic names. Feedback for the search and navigation during the beta test phase was very positive.</p>
                    <p>The website also shows currently 104 freely available digitized codices using the IIIF
                        <note xml:id=""ftn8"" place=""foot"" n="""">IIIF: 
                            <ptr target=""https://iiif.io/""></ptr>
                        </note> image standard and the Diva.js
                        <note xml:id=""ftn9"" place=""foot"" n="""">Diva.js: 
                            <ptr target=""https://ddmal.github.io/diva.js/""></ptr>
                        </note> viewer (see Figure 2). Most of the codices were scanned by the MPIWG in a cooperation with the Staatsbibliothek Berlin but some exemplars from the Gallica project of the Bibliothéque Nationale de France and the Qatar Digital Library are also present to demonstrate the potential of public IIIF image sources in an area that has been plagued in the past with proprietary data silos and restrictive access conditions making global electronic manuscript databases nearly impossible. We hope to expand the amount of scanned codices in the future.
                    </p>
                    <p>
                        <figure>
                            <graphic url=""Pictures/9ce4f599b657c0a3210f783f9ee5cedc.png""></graphic>
                            Display of scanned manuscript (Codex Petermann I 671, Staatsbibliothek Berlin)
                        </figure>
                    </p>
                    <p>The experimental “ISMI Lab” section of the site offers access to the “Query Builder” tool which allows to construct custom queries to the database based on objects, attributes and relations and a full Neo4J graph database console with access to all published data (see Figure 3). These additional tools are very powerful but require some technical expertise and familiarity with the ISMI data model. There is some documentation but this section is more of an experimental offer to also get in contact with interested scholars in the hope that interesting queries and research questions can be exchanged and new, easier to use, tools can be developed in the future.</p>
                    <p>
                        <figure>
                            <graphic url=""Pictures/5f4a31f9768fd43c7e54231948dff640.png""></graphic>
                            Experimental Neo4J console showing partial graph of commentary relations.
                        </figure>
                    </p>
                </div>
                <div type=""div2"" rend=""DH-Heading2"">
                    A never ending project?
                    <p>The history of the project in the last ten years has shown the difficulties of developing and maintaining a project of this complexity – organisationally, in terms of hardware, software, and scholarly support. We think this project shows the potential for a unifying manuscript database that is not limited to singular collections and presents the continually updated and expanded current knowledge of scholars in the field. We hope that scholars in the future will not have to figure out errors in decades-old printed catalogues individually again and again but that they can participate in a common database and share and enhance their individual findings. The collaborative phase of the ISMI database is only beginning and we would like to start the discussion now. We think we have laid the technical foundations to make the database maintainable and adaptable and the data shareable and linkable but the long term value of a shared resource lies in its users and its contributors.</p>
                </div>
            </div>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>Ragep, Jamil F., and Sally P. Ragep. 
                        <hi rend=""italic"">The Islamic Scientific Manuscript Initiative (ISMI) Towards a Sociology of the Exact Sciences in Islam.</hi> In A Shared Legacy: Islamic Science East and West. Homage to Professor J. M. Millàs Vallicrosa, edited by Emilia Calvo, Mercè Comes, Roser Puig, and Monica Rius, 15–21. Barcelona: University of Barcelona, 2008. 
                        <ptr target=""https://www.rasi.mcgill.ca/ISMI_SharedLegacy.pdf""></ptr>
                    </bibl>
                    <bibl>G. P. Matvievskaya and B. A. Rosenfeld, 
                        <hi rend=""italic"">Matematiki i astronomi musulmanskogo srednevekovya i ikh trudi</hi> (VIII-XVII vv.) [Mathematicians and Astronomers of the Muslim Middle Ages and Their Works (VIII-XVII centuries)], 3 vols. (Moscow: Nauka, 1983), later extended as 
                        <emph>Boris A. Rosenfeld and </emph>
                        <emph>Ekmeleddin İhsanoğlu</emph>
                        <emph>, Mathematicians, astronomers and other scholars of Islamic civilization and their works (7th-19th c.)</emph>
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,database;history of science;islamicate;manuscripts,English,"archives, repositories, sustainability and preservation;databases & dbms;data models and formal languages;english;history of science;manuscripts description and representation;near eastern studies"
9990,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,Project Endings: Early Impressions From Our Recent Survey On Project Longevity In DH,,Stewart Arneil;Martin Holmes;Greg Newton,"paper, specified ""long paper""","<text>
        
            <p>Despite the thousands of digital projects launched during the past 20 years, experts warn of a new “digital dark age” (Cerf, 2015; Davis, 2016) as our ability to produce digital information continues to outpace our capacity to preserve and access that knowledge for the long term, even (or especially) when using content management systems (Montoya, 2016).</p>
            <p>
                <hi rend=""italic"">Project Endings</hi> is a collaboration between the Humanities Faculty and the University Library which aims to provide practical solutions to issues attendant on ending a project and archiving the digital products of research, including not only data but also interactive applications and web-based publications. 
                <hi rend=""italic"">Project Endings</hi> endeavours to align the aims of faculty researchers producing projects and the archivists who will eventually be responsible for curating their work.
            </p>
            <p>The project divides digital projects into five primary components: data, products, processing, documentation, and release management. We aim at longevity primarily for data and products, but believe that this goal requires careful attention to processing, documentation and release management. We are developing preservation principles for all of these factors, using practice-based methods (Holmes, 2017; Arneil and Holmes, 2017; Holmes and Takeda, 2018), diagnostic tools (Holmes and Takeda, 2017), and scholarly research as listed in the project bibliography at 
                <ptr target=""https://hcmc.uvic.ca/endings/EndingsListofReferences2015.pdf""></ptr>.
            </p>
            <p>The project conducted a survey on the 
                <ref target=""https://www.limesurvey.org/"">LimeSurvey</ref> platform consisting of 30 questions 
                <ptr target=""https://hcmc.uvic.ca/endings/survey.html""></ptr> to discover how project leaders dealt with the issues of long-term sustainability for each of the five primary components. We promoted the survey to Canadian and international professional communities and received 128 responses. 25 detailed interviews were run with a sample of the respondents to get more information on the issues raised by the survey results.
            </p>
            <p>Results of the survey show that concerns about longevity for digital humanities projects are not exaggerated. 57% of survey respondents did not consider an endpoint for their project, despite the fact that project management principles include declarations of goals, timelines, and milestones (Zanduis and Stellingwerf, 2013). In the light of this, perhaps it is not surprising that 54% did not have long-term preservation plans. These findings suggest that many researchers do not distinguish between products generated to exploit the features of the processing environment and products generated to survive after active work on the project ends or independent of development work in the project. Furthermore, only 32% considered “benchmarks for assessing progress” and 41% included precise timelines in their plans.</p>
            <p>In a group of projects that were for the most part (74%) less than 10 years old and 58% still in progress, 22% reported that project outputs stopped working due to software obsolescence. This is in a field of projects in which 74% started with born-digital data. If a failure occurs during the active life of the project it might be repairable, but repair is much less likely if the project has ended.</p>
            <p>The value of using a standardized data model is not universally recognized, with 14% of survey respondents not using one at all and 26% making up their own. Although a home-made data model is by definition not standardized, it may still be viable for a long time if well documented. 60% claimed to have a clearly documented data model, but 90% of those that had documentation considered it to be partial or inadequate, so it appears that a project’s data model is well documented in only about 50% of cases.</p>
            <p>HTML is the most popular standard output for DH projects (68% of respondents used it), despite the continued popularity of PDF (45%), XML (38%), and various binary media formats (&gt;65%). Javascript is considered by many (30%) to be a major technology in their project. HTML and Javascript are robust long-term (Holmes, 2017), but if they are produced in a project only on-the-fly by a content management system (CMS) or database, then the longevity of the output is dependent on that of the CMS or database. 34% of the respondents used WordPress or Drupal, 31% used PHP/SQL databases, 38% used XML/XSLT/XQuery systems, and 41% used “other” software services and libraries. Some projects used more than one of these.</p>
            <p>Lack of ongoing funding was cited by 38% of respondents as the main obstacle to long term preservation. Perhaps more surprisingly, 33% of respondents rated either lack of expertise or bad technology choices as their main obstacle, which may explain the results reported above regarding software obsolescence. Early results from the interviews suggest that CMS and other software libraries and services are the likeliest sources of software failure over time. We hope that further analysis of the interviews will tell us whether a more expert assessment of software and output choices would have mitigated the issue of lack of ongoing funding.</p>
            <p>While a reassuringly high 42% of respondents reported that university services were responsible for long-term maintenance of the project’s work, an alarming 45% reported that this responsibility fell to the Principal Investigator or nobody, demonstrating either significant vulnerability or great confidence.</p>
            <p>Our survey results suggest that there is a limited use of project management (“What is PRINCE2?”, 2018; Sedlmayer et al., 2015) and software lifespan principles in DH projects. Results further suggest that there is a need for an improved understanding by researchers of specific attributes of a project which are likely to facilitate long-term viability of the project data, outputs and documentation at minimal cost for those charged with preservation. Blurring the lines between data, processing, outputs and the management of those components over time can result in vulnerabilities for long term preservability which may not be apparent until it is too late.</p>
            <p>With all of this in mind 
                <hi rend=""italic"">Project Endings</hi> is working on a suite of recommendations that will provide guidance on project structure and management with long term viability as the goal. We are offering an online interactive questionnaire that assesses the long-term viability of each component in a project and provides recommendations for improving the prospects for long-term survival. Behind each question is the empirical evidence provided by survey/interview participants as well as the combined experience of the 
                <hi rend=""italic"">Project Endings</hi> team. The questionnaire is intended primarily to be a thought-provoking activity for project leaders and principal investigators. An early draft of the questionnaire is available at 
                <ptr target=""https://hcmc.uvic.ca/endings/questionnaire.htm""></ptr>.
            </p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"">Arneil, S. and Holmes, M.</hi> (2017). Archiving form and function: preserving a 2003 digital project. Brighton, U.K.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Cerf, V.</hi> (2015). Google’s Vint Cerf warns of ‘digital Dark Age’ 
                        <ptr target=""http://www.bbc.com/news/science-environment-31450389""></ptr>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Davis, R. C.</hi> (2016). Die Hard: The Impossible, Absolutely Essential Task of Saving the Web for Scholars. Skidmore College, Saratoga Springs, U.S.A. 
                        <ptr target=""https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=1077&amp;context=jj_pubs""></ptr>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Holmes, M.</hi> (2017). Selecting Technologies for Long-Term Survival. Victoria, Canada 
                        <ptr target=""https://github.com/projectEndings/Endings/raw/master/presentations/SHARP_2017/mdh_sharp_2017.pdf""></ptr>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Holmes, M. and Takeda, J.</hi> (2017). Beyond Validation: Using Programmed Diagnostics to Learn About, Monitor, and Successfully Complete Your DH Project. Montreal, Canada 
                        <ptr target=""https://dh2017.adho.org/abstracts/140/140.pdf""></ptr>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Holmes, M. and Takeda, J.</hi> (2018). Why do I need four search engines?. Tokyo, Japan 
                        <ptr target=""https://conf2018.jadh.org/files/Proceedings_JADH2018.pdf#page=58""></ptr>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Montoya, R. D.</hi> (2016). Advocating for Sustainability: Scaling-Down Library Digital Infrastructure. 
                        <hi rend=""italic"">Journal of Library Administration</hi>, 
                        <hi rend=""bold"">2016</hi>(56:5): 603–20 doi:10.1080/01930826.2016.1186969.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sedlmayer, M., Coesmans, P., Fuster, M., Schreiner, J. G., Gonçalves, M., Huynink, S., Jaques, T., et al. (eds).</hi> (2015). 
                        <hi rend=""italic"">Individual Competence Baseline for Project, Programme &amp; Portfolio Management.</hi> International Project Management Association 
                        <ptr target=""http://products.ipma.world/wp-content/uploads/2016/03/IPMA_ICB_4_0_WEB.pdf#page=27""></ptr>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Zanduis, A. and Stellingwerf, R.</hi> (2013). 
                        <hi rend=""italic"">ISO21500: Guidance on Project Management – a Pocket Guide.</hi> Van Haring Publishing 
                        <ptr target=""https://www.vanharen.net/Samplefiles/9789087538095SMPL.pdf#page=21""></ptr>.
                    </bibl>
                    <bibl>What is PRINCE2? 
                        <hi rend=""italic"">Projects in Controlled Environments</hi>
                        <ptr target=""https://www.prince2.com/uk/what-is-prince2""></ptr>.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,best practices;project planning;sustainability,English,"archives, repositories, sustainability and preservation;digital archives and digital libraries;digital humanities (history, theory and methodology);english;project design, organization, management;standards and interoperability"
10011,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,DH Text Submission GuidelinesA Digital Enquiry On The Italian Reception Of The English Novel In The Periodical Press Of The Long Eighteenth Century,,Andrea Penso,"paper, specified ""short paper""","<text>
        
            <p>This poster aims at showing the first results of the ongoing collaborative research project 
                <hi rend=""italic"">The reception of the English novel in the Italian literary press between 1700 and 1830: a transcultural enquiry into the early shaping of the modern Italian literary and cultural identity</hi>. The project aims at investigating the reception of English novels in the Italian literary press during the Long Eighteenth Century (1700-1830). The analysis focuses on an existing corpus of data relative to the publication, dissemination, translations, critical reviews, and editorial advertisements of English novels in Italian literary newspapers and journals of the time. The main purpose of the project is to uncover how the English novels were introduced to the Italian readership through literary journalism with the application of digital methodologies of investigation. One of the project goals is in fact to create a methodological paradigm that may be extended to the study of the reception of English novels in the literary journalism of other nations. 
            </p>
            <p>The present paper therefore has three primary objects:</p>
            <p>a) To show to the public the first research output: an open access, bilingual, and annotated digital repository, which consists of a Drupal-based software for corpora, and represents an immediate way to develop the research. The first step of the project has been the cataloguing, analysis, and digitization of the corpus of reviews. This preliminarily created digital database allows the subsequent computational, textual and critical surveys. </p>
            <p>b) To show how the TEI has been applied to the analysis of the corpus. The text encoding of the reviews, conducted following the TEI standards, makes possible to point out the elements that are original and innovative with respect to the foreign reviews of the time which the Italian press copied from, often adapting the contents. In fact, the encoding of the reviews allows also to understand and visualise their “genealogical dimension” (i.e. the comparative analysis of reviews that were taken from French or English periodicals and made their way into the Italian press). These sources have already been identified, and the comparison between the encoded versions will allow to understand the extent of the influence French and English journalism had on the Italian press, and to outline the specific Italian input. The TEI will therefore make possible to focus on the re-interpretations of Italian reviewers who drew on the Italian literary tradition but challenged its subjects, genres and linguistic structures. Ultimately, the TEI will also be applied to integrate the stylometric analysis and the gathering of Geospatial information: the short presentation will allow me to show a case study and to explain how the mark-up process proves to be a fundamental part of the methodology for the analysis of the corpus. </p>
            <p>c) To illustrate the two main lines of approach that will be applied in order to digitally explore the corpus. The first consists of a stylistic and linguistic analysis of the reviews, which will be pursued equalizing and comparing stylistic and lexical constellations belonging to different discursive practices from a number of periodicals and journalist. Digital stylometry, word frequency and statistical analyses tools such as 
                <hi rend=""italic"">R</hi>, 
                <hi rend=""italic"">MiniTab</hi> and 
                <hi rend=""italic"">Intelligent Archive</hi> will be used during this phase. The study of the readers’ response to the contents, spread by the novels via the reviews, is deeply connected to the stylistic analysis of the reviews. In fact, the outlining of the reviews’ stylistic features is crucial to understanding in which ways the contents were revealed to the public, and how the audience was influenced in the perception of the moral values and the social messages of the novels. The second line of approach will concern the spatial analysis of the data, which will be mapped thanks to 
                <hi rend=""italic"" xml:space=""preserve"">GIS </hi>digital tools integrated with Geo-criticism. The spatial analysis allows the visualization of popular reading trends in 18th and early 19th century Italy.
            </p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Altick, Richard D. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"" xml:space=""preserve"">The English Common Reader: A Social History of the Mass Reading Public, 1800-1900. </hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Columbus: Ohio State UP, 1998. </hi>
                    </bibl>
                    <bibl>
                        <hi rend=""HTML_Cite"">
                            <hi rend=""italic"">Baines, Paul</hi>. The Long 18
                            <hi rend=""superscript"">th</hi> century. 
                            <hi rend=""italic"">London</hi>
                            <hi rend=""italic"" xml:space=""preserve"">: </hi>
                            <hi rend=""italic"">Arnold</hi>, 
                            <hi rend=""italic"">2004.</hi>
                        </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Balay, Robert. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Early Periodical Indexes: Bibliographies and Indexes of Literature Published in Periodicals before 1900</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Lanham, MD: Scarecrow, 2000. </hi>
                    </bibl>
                    <bibl>Beller, Manfred and Leersen, Joep, 
                        <hi rend=""italic"">Imagology. The cultural construction and literary representations of national characters. A critical survey</hi>, Amsterdam-Atlanta: Rodopi, 2007.
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Benedict, Barbara M. “Readers, Writers, Reviewers, and the Professionalization of Literature.” </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Cambridge Companion to Literature, 1740-1830</hi>
                        <hi style=""font-size:11pt"">. Ed. by T. Keymer and J. Mee. Oxford: Oxford U. Press, 2004.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Basker, James. “Criticism and the Rise of Periodical Literature.” </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">The Cambridge History of Literary Criticism</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Vol. 4: </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">The Eighteenth Century</hi>
                        <hi style=""font-size:11pt"">. Ed. by H. B. Nisbetand C. Rawson. Cambridge: Cambridge UP, 1997. 316-32</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Berengo, Marino. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">I giornali veneziani del Settecento</hi>
                        <hi style=""font-size:11pt"">. Milan: Feltrinelli, 1962.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Colombo, Rosa Maria. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Settecento senza amore: studi sulla narrative inglese</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Rome: Bulzoni, 1983. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">_____. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Lo Spectator e i giornali veneziani del Settecento</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Bari: Adriatica editrice, 1966. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">De Stefanis Ciccone, Stefania, ed. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">La stampa periodica Milanese nella prima metà dell’Ottocento. Testi e concordanze</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Pisa: Giardini, 1983. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">D’Alia, Fiorella. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">La donna nel romanzo italiano del Settecento</hi>
                        <hi style=""font-size:11pt"">. Rome: Fratelli Palombi, 1990.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Di Fino, Sharon Marie. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">The Intellectual Development of German Women in Selected Periodicals from 1725 to 1784</hi>
                        <hi style=""font-size:11pt"">. New York: Peter Lang, 1990.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Donoghue, Frank. “Colonizing Readers: Review Criticism and the Formation of a Reading Public.” </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">The Consumption of Culture, 1600-1800: Image, Object, Text</hi>
                        <hi style=""font-size:11pt"">. Ed. by A. Bermingham and J. Brewer. New York: Routledge, 1995. 54-74.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Farinelli, Giuseppe. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Storia del giornalismo italiano: dalle origini ai giorni nostri</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Turin: UTET, 1997. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Forster, Antonia. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Index to Book Reviews in England, 1749-1774</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Carbondale: Southern Illinois University, 1990. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">_____. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Index to book reviews in England, 1775-1800</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. London: British Library, 1997. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Franchini, Silvia, Simonetta Soldani, eds. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Donne e giornalismo: Percorsi e presenze di una storia di genere</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Milan: Franco Angeli, 2004. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Infelise, Mario. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">L’editoria veneziana nel Settecento</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Milan: Franco Angeli, 1989. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Klancher, Jon P. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">The Making of English Reading Audiences, 1790-1832</hi>
                        <hi style=""font-size:11pt"">. Madison: University of Wisconsin Press, 1987.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">MacMurran, Mary H. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">The Spread of Novels: Translation and Prose Fiction in the Eighteenth Century</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Princeton: Princeton University Press, 2010. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Magnani, Giovanni. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Giornalismo e attività letteraria dell’Ottocento</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Florence: Bulgarini, 1974. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Mangione, Daniela. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Prima di Manzoni: autore e lettore nel romanzo del Settecento</hi>
                        <hi style=""font-size:11pt"">. Rome: Salerno editrice, 2012.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Marchesi, Giambattista. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Studi e ricerche intorno ai nostri romanzi e romanzieri del Settecento coll’aggiunta di una bibliografia dei romanzieri editi in Italia in quel secolo</hi>
                        <hi style=""font-size:11pt"">. Bergamo: Istituto italiano d'arti grafiche, 1903.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Mayo, Robert. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">The English Novel in the Magazines, 1740-1815. With a Catalogue of 1375 Magazine Novels and Novellettes</hi>
                        <hi style=""font-size:11pt"">. Evanston: Northwestern University Press, 1962.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Moretti, Franco. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Atlante del romanzo europeo. 1800-1900</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Torino: Einaudi, 1997. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Murialdi, Paolo. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Storia del giornalismo italiano</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Bologna: Il Mulino, 2000. </hi>
                    </bibl>
                    <bibl>
                        <hi rend=""HTML_Cite"">
                            <hi rend=""italic"">O’Gorman, Frank</hi>. The Long Eighteenth Century: British Political and Social History 1688–1832 (The Arnold History of Britain Series). 
                            <hi rend=""italic"">Hodder Arnold, 1997.</hi>
                        </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Pearson, Jacqueline. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Women's Reading in Britain, 1750-1835: A Dangerous Recreation</hi>
                        <hi style=""font-size:11pt"">. New York: Cambridge University Press, 1999.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Piccioni, Luigi. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Il giornalismo letterario in Italia</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. Turin-Rome: Loescher, 1984. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Reiman, Donald H. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">The Romantics Reviewed; Contemporary Reviews of British Romantic Writers</hi>
                        <hi style=""font-size:11pt"">. New York: Garland, 1972.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Rivers, Isabel. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Books and their readers in eighteenth-century England: new essays</hi>
                        <hi style=""font-size:11pt"">. London and New York: Leicester University Press, 2001.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Rose, Jonathan. “Rereading the English Common Reader: A Preface to a History of Audiences.” </hi>
                        <hi rend=""italic"" style=""font-size:11pt"" xml:space=""preserve"">Journal of the History of Ideas </hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">53 (1992). 47-70. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Sgard, Jean, ed. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Dictionnaire des journaux (1600-1789)</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. 2 Vols. Oxford: Voltaire Foundation; Paris: Universitas, 1991. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">_____. ed. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Dictionnaire des journalistes 1600-1789</hi>
                        <hi style=""font-size:11pt"">. Rev. ed. 2 Vols. Oxford: Voltaire Foundation, 1999.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Siskin, Clifford. “Eighteenth-century Periodicals and the Romantic Rise of the Novel.” </hi>
                        <hi rend=""italic"" style=""font-size:11pt"" xml:space=""preserve"">Studies in the Novel </hi>
                        <hi style=""font-size:11pt"">26.2 (Summer 1994): 26-42.</hi>
                    </bibl>
                    <bibl>Spiering, Menno (ed.), 
                        <hi rend=""italic"">Nation building and writing literary history</hi>, Amsterdam-Atlanta: Rodopi, 1999.
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Streeter, Harold Wade. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">The Eighteenth-century English Novel in French Translation: A Bibliographical Study</hi>
                        <hi style=""font-size:11pt"">. New York: Blom, 1970.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Ward, William S. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">Literary Reviews in British Periodicals 1788-1826: A Bibliography</hi>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">. New York: Garland Publishing, 1977. </hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"" xml:space=""preserve"">Westphal, Bertrand. </hi>
                        <hi rend=""italic"" style=""font-size:11pt"">La Géocritique. Réel, Fiction, Espace</hi>
                        <hi style=""font-size:11pt"">, («Paradoxe»), Paris: Éditions de Minuit, 2007.</hi>
                    </bibl>
                    <bibl>
                        <hi style=""font-size:11pt"">Zambon, Maria Rosa. Bibliographie du roman français en Italie au 18e siècle. Florence: Sansoni, 1962.</hi>
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,database;journalism;reception studies,English,"corpus and text analysis;digital archives and digital libraries;digital humanities (history, theory and methodology);digital research infrastructures and virtual research environments;english;italian studies;text encoding and markup languages"
10018,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,Paving the Way to Linked Open Data: Evaluating the Path to LOD for the Census of Antique Works of Art and Architecture Known in the Renaissance,,Oliver Pohl;Andrea Notroff,poster / demo / art installation,"<text>
        
            <p>The 
                <hi rend=""italic"">Census of Antique Artworks and Architecture Known in the Renaissance</hi> (henceforth Census) identifies and collects antique monuments and related Renaissance documents in a database, such as works of architecture, statues, frescoes, sarcophagi, paintings, drawings, sketches, manuscripts and more. Established in 1983, data has continually been added to the database. Since then, the fundamentals of the underlying relational data model of the Census did not have to be changed. Its main focus is to help researchers in art history expand their understanding about the relation between works of art produced in the Antiquity and their reception and perception in the Renaissance.
            </p>
            <p>Although the data model is robust, the research environment using the Census database does not meet current user expectations like a modern and responsive user interface and search capabilities that are easy to understand. Moreover, the site does not make use of best practices established in the Digital Humanities community, such as providing a RESTful API or making use of Linked Open Data (LOD) technologies. Another issue the Census project is currently facing is the fact that the website runs on a proprietary digital asset management system (easydb 4) which handles data entry, retrieval front- and back-end. The support for easydb 4 will be running out shortly. In order to address the issues of a) openness, b) usability and c) maintainability, the we are currently currently evaluating how to port its data and research supporting functionalities in the coming two years into an open source-based system with LOD capabilities that also provides a modern user experience.</p>
            <p>In the beginning of the evaluation process, the Census project looked at solutions of other projects in the domain history that seem to fit the requirements mentioned above. While researching and speaking to other members of the Digital Humanities and art history community, we identified the following software solutions as possible contenders for the future of the Census project:</p>
            <table rend=""frame"" xml:id=""Tabelle1"">
                <row>
                    <cell rend=""justify"">
                        <hi rend=""bold"">Software</hi>
                    </cell>
                    <cell rend=""justify"">
                        <hi rend=""bold"">Description</hi>
                    </cell>
                    <cell rend=""justify"">
                        <hi rend=""bold"">Developer / Maintainer</hi>
                    </cell>
                </row>
                <row>
                    <cell rend=""start"">conedaKor
                        <note xml:id=""ftn1"" place=""foot"" n="""">
                            <ptr target=""https://github.com/coneda/kor""></ptr>
                        </note>
                    </cell>
                    <cell rend=""start"">Open source web application for storing arbitrary entity types and interconnect them.</cell>
                    <cell rend=""start"">coneda (Germany)</cell>
                </row>
                <row>
                    <cell rend=""start"">Omeka-S
                        <note xml:id=""ftn2"" place=""foot"" n="""">
                            <ptr target=""https://omeka.org/s/""></ptr>
                        </note>
                    </cell>
                    <cell rend=""start"">Open source web publishing platform and content management system for cultural heritage collections with LOD in mind</cell>
                    <cell rend=""start"">Roy Rosenzweig Center for History and New Media (USA); George Mason University (USA)</cell>
                </row>
                <row>
                    <cell rend=""start"">researchSpace
                        <note xml:id=""ftn3"" place=""foot"" n="""">
                            <ptr target=""https://www.researchspace.org/""></ptr>
                        </note>
                    </cell>
                    <cell rend=""start"">(Partly) open source Semantic Web environment for research and collaboration</cell>
                    <cell rend=""start"">The British Museum (UK); Metaphacts (Germany)</cell>
                </row>
                <row>
                    <cell rend=""start"">WissKI
                        <note xml:id=""ftn4"" place=""foot"" n="""">
                            <ptr target=""http://wiss-ki.eu/""></ptr>
                        </note>
                    </cell>
                    <cell rend=""start"">Drupal extension for annotating arbitrary data using LOD Data in a CMS-based research environment</cell>
                    <cell rend=""start"">Germanisches Nationalmuseum (Germany)</cell>
                </row>
                <row>
                    <cell rend=""start"">
                        <hi rend=""italic"">arches</hi>
                        <hi rend=""italic"">
                            <note xml:id=""ftn5"" place=""foot"" n="""">
                                <ptr target=""https://www.archesproject.org/""></ptr>
                            </note>
                        </hi>
                    </cell>
                    <cell rend=""start"">
                        <hi rend=""italic"">Open source data management system for (cultural) heritage data</hi>
                    </cell>
                    <cell rend=""start"">
                        <hi rend=""italic"">Getty Conservation Institute (USA)</hi>
                    </cell>
                </row>
                <row>
                    <cell rend=""start"">
                        <hi rend=""italic"">easydb 5</hi>
                        <hi rend=""italic"">
                            <note xml:id=""ftn6"" place=""foot"" n="""">
                                <ptr target=""https://www.programmfabrik.de/""></ptr>
                            </note>
                        </hi>
                    </cell>
                    <cell rend=""start"">
                        <hi rend=""italic"">Closed source successor of the current Census system with open source extensions</hi>
                    </cell>
                    <cell rend=""start"">
                        <hi rend=""italic"">Programmfabrik (Germany)</hi>
                    </cell>
                </row>
            </table>
            <p>We established a catalog of criteria to test these system against, taking inspirations from 
                <anchor type=""bookmark-start"" xml:id=""id___UnoMark__288_254202659""></anchor>
                <anchor type=""bookmark-start"" xml:id=""id___UnoMark__355_254202659""></anchor>Jackson et al. (2011)
                <ptr type=""bookmark-end"" target=""#id___UnoMark__288_254202659""></ptr>
                <ptr type=""bookmark-end"" target=""#id___UnoMark__355_254202659""></ptr>and 
                <anchor type=""bookmark-start"" xml:id=""id___UnoMark__289_254202659""></anchor>
                <anchor type=""bookmark-start"" xml:id=""id___UnoMark__354_254202659""></anchor>Knodel and Naab (2016)
                <ptr type=""bookmark-end"" target=""#id___UnoMark__289_254202659""></ptr>
                <ptr type=""bookmark-end"" target=""#id___UnoMark__354_254202659""></ptr> while also taking advice from other members of the Digital Humanities community. This list includes criteria and questions such as:
            </p>
            <list type=""unordered"">
                <item>How easy is it to re-use the current data model of the Census in the new system?</item>
                <item>Is the new system easy to understand and handle for users and developers?</item>
                <item>Does the system have built-in LOD capabilities?</item>
                <item>Can the new system be installed and deployed easily?</item>
                <item>Can you extend the new system’s front-end and back-end components without breaking upgradeability?</item>
                <item>Is the new system available as open source software and is there (commercial) support available?</item>
                <item>How big is the user community for the new software?</item>
            </list>
            <p>While Omeka-S, researchSpace, WissKI and arches are built with Semantic Web technologies in mind, conedaKOR just focuses on employing a non-RDF-based graph model. When comparing the systems regarding usability and maintainability, Omeka-S offered the best documentation, modern user interface with CMS functionalities as well a modular approach for extending its source code. researchSpace impressed us with its software architectural design by only relying on a triple store and possibilities to visualize any data using React
                <note xml:id=""ftn7"" place=""foot"" n="""">
                    <ptr target=""https://reactjs.org/""></ptr>
                </note> components. However, it turns out researchSpace is very hard to deploy and complicated to maintain on a source level.
            </p>
            <p>While testing all these systems, we noticed there would not be an easy plug-and-play solution to re-use the Census database. These system either require a specific yet generic data model and/or Semantic Web ontology. Thus, we would have to re-design the current structure of Census relational database and thereby risk losing important data and relations without even having re-implemented basic functions such as searching and data entry.</p>
            <p>Instead of settling on a holistic system that covers database interaction, front-end, back-end and Linked Open Data, we had to rethink our approach to a new software architecture for the Census: We intend to establish a modular software architecture revolving around a RESTful LOD-API. Having a well documented API, e.g. in form of an OpenAPI specification
                <note xml:id=""ftn8"" place=""foot"" n="""">
                    <ptr target=""https://github.com/OAI/OpenAPI-Specification""></ptr>
                </note>, allows us to build front-end components using that API endpoint for presentation and research, while also developing a back-end system that handles data base interactions and preparing the data for the API at the same time, In other words, having an API centric software architecture makes it programming language agnostic, making it easier to swap, extend and update front- and back-end components as well as the database if the need should arise, as long as the API still functions as specified.
            </p>
            <p>The Census project recently turned 35 years and aims to continue doing its research in the future. We conclude to not adapt the next “one size fits all” solution for the Census database, and instead focus on establishing a modular approach to remain flexible for future technologies and best practices.</p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"">Jackson, M., Crouch, S. and Baxter, R.</hi> (2011). 
                        <hi rend=""italic"">Software Evaluation: Criteria-Based Assessment</hi>. Software Sustainability Institue 
                        <ptr target=""https://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationCriteria.pdf""></ptr>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Knodel, J. and Naab, M.</hi> (2016). 
                        <hi rend=""italic"">Pragmatic Evaluation of Software Architectures</hi>. (The Fraunhofer IESE Series on Software and Systems Engineering). Springer International Publishing 
                        <ref target=""https://doi.org///www.springer.com/de/book/9783319341767"">//www.springer.com/de/book/9783319341767</ref> (accessed 27 November 2018).
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,api;evaluation;lod;rest;software architecture,English,"art history and design studies;cultural artifacts digitisation - theory;data models and formal languages;digital humanities (history, theory and methodology);digital research infrastructures and virtual research environments;english;methods and technologies;semantic web and linked data"
10470,2016 - University of Leipzig,University of Leipzig,Modellierung - Vernetzung – Visualisierung: Die Digital Humanities als fächerübergreifendes Forschungsparadigma,2016,DHd,DHd,Universität Leipzig (Leipzig University),Leipzig,,Germany,http://dhd2016.de/,WissKI – Wissenschaftliche Kommunikations-Infrastruktur,,Dorian Merz;Mark Fichtner,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <p>
            <anchor id=""id_docs-internal-guid-9368970c-6b1e-1590-eb78-232effc12222""/> Das
        DFG-finanzierte Projekt “WissKI” hat in den Jahren 2009 bis 2011 zur Entwicklung
        einer digitalen Forschungsumgebung für die Anwendung im Bereich der Digital
        Humanities geführt. Hauptaspekt der Datenerfassung und -haltung in WissKI sind die
        semantischen Zusammenhänge zwischen einzelnen Fakten und Datensätzen. Dies wird
        durch umfassende Unterstützung aktueller Semantic Web Technologien erreicht. Die
        Einordnung und Speicherung der erhobenen Daten erfolgt auf Grundlage einer
        Domänenontologie, deren Konzepte und Relationen - zu sogenannten Pfaden verbunden -
        als Vorlage für die Masken und Felder im System dienen. Auf Basis dieser Technologie
        werden solitär erscheinende Daten zu einem gemeinsamen, semantischen Netzwerk
        verbunden und damit die  unmittelbare Sichtbarkeit weiterer, tiefergehender
        Zusammenhänge ermöglicht. Hierdurch bietet sich ein Mehrwert, der in der
        Vergangenheit in flachen Hierarchien wie Datenbanktabellen gar nicht oder nur mit
        sehr hohem Aufwand erkennbar gemacht werden konnte. Das Web-basierte Systemdesign
        und der dadurch ermöglichte Zugriff über das Internet, die Anbindung von externen
        kuratierten Datenquellen (sog. Authority Files) und die Möglichkeit zur
        Bereitstellung ausgewählter Daten über gängige Online-Schnittstellen (Web-Frontend,
        SPARQL-Endpoint, ...) betonen den Semantic-Web-Gedanken hinter der Infrastruktur. </p>
         <p>Die Speicherung der Daten erfolgt  in einem Triple-Store, der die eingegebenen Fakten
          in einer Subjekt-Prädikat-Objekt-Satzform ablegt. Die Aneinanderreihung der hier
          verwendeten Prädikate zu Pfaden erfolgt im Kern des Systems, dem sogenannten
          Pathbuilder, mit dem die semantische Bedeutung der einzelnen Masken-Einträge in
          Bezug auf das beschriebene Objekt (auch Person, Ort o. Ä.) anhand der Ontologie
          festgelegt wird. Die Eingabe der Daten erfolgt über eine, mit den gängigen
          Datenbankoberflächen vergleichbare, Editier-Oberfläche. Sie ist aus Feldern
          aufgebaut, die wiederum je einem bestimmten Feldtyp zugeordnet sind. Feldtypen
          bestimmen die Ein- und Ausgabemodalitäten der Daten. So stehen zum Beispiel ein-
          oder mehrzeilige Textfelder. verschiedenartige Auswahldialoge und Möglichkeiten zur
          Bildanzeige zur Verfügung. In einigen davon kann WissKI den Anwender durch
          Auto-Vervollständigung unterstützen, indem zu angefangenen Eingaben ähnlich lautende
          Einträge aus der lokalen Datenhaltung oder den Authority Files vorgeschlagen
          werden.</p>
         <p>Weitere umfassende Möglichkeiten zur Datenaufbereitung sind das halbautomatische
            Auszeichnen und Erkennen bzw. Einordnen von Personen, Orten, Zeiten und anderen
            Entitäten aus Texten (sog. Named Entity Recognition), die Verwaltung der
            Revisionsgeschichte von Texten, das Werkzeug zur Bildannotation und die Verknüpfung
            mit der Literaturverwaltungssoftware Zotero. Dabei verzichtet die Software nicht auf
            die aus dem Bereich der Content Management Systeme bekannten Funktionalitäten wie z.
            B. die Generierung von Websites, Foren, Wikis oder auch die  detaillierte Verwaltung
            der Nutzer und ihrer Zugriffsrechte.</p>
         <p>Inzwischen ist die Software in verschiedenen Forschungsprojekten an unterschiedlichen, namhaften Institutionen  im kunst- und kulturhistorischen, sowie biologischen und technischen Bereich erfolgreich im Einsatz. Als Domänenontologie im Museums- und Sammlungsbetrieb kommen individuelle Erweiterungen des “Conceptual Reference Model” des Comité international pour la documentation  zum Einsatz (CIDOC-CRM: ISO 21127), dessen Umsetzung in der Web-Ontology-Language OWL ebenfalls vom Projekt besorgt wurde und über die Website <ref target=""http://erlangen-crm.org"">http://erlangen-crm.org</ref> frei zur Verfügung steht.</p>
         <p>Das Poster stellt nun die konsequente Weiterführung des Projektes (DFG Proj.-Nr. GR1471/9) und das damit einhergehende Update der Software vor. Neben der grundlegenden Aktualisierung der zugrundeliegenen Frameworks und Technologien (Drupal 7, php 5.5, SPARQL 1.1) sind dabei einige Erweiterungen der Oberflächenfunktionalität sowie deutliche Erleichterungen in der Bedienbarkeit vorgesehen. So können nun beliebige Feldtypen aus der “Field API” des verwendeten Content Management Systems Drupal in die Ein- und Ausgabeansichten integriert werden. Dies umfasst neben den altbewährten Textfeldern und -bereichen und Bildern (incl. Zoomviewer für sehr hochauflösende Bilder) auch interaktive Landkarten, 3D-Animationen, Zeitstrahlen und alle denkbaren Medientypen, sowohl zur direkten Ansicht als auch zum Download. Zusätzlich zu diesen bereits eingebundenen Formaten ermöglicht die offene Architektur von WissKI² auch die Einbindung anderer, gängiger Feldtypmodule, die für Drupal zur Verfügung stehen.</p>
         <p>Zu den erwähnten Erleichterungen zählt ebenso ein Update des System-Kerns, dem Pathbuilder, mit dem die Pfadschablonen durch die Domänenontologie auf einer graphischen Oberfläche ausgewählt bzw. erzeugt werden können. Daneben soll eine umfassende Bibliothek mit Musterontologien, -masken und -pfaden bereitgestellt werden, die die Einstiegshürde für Erstbenutzer minimal halten wird.</p>
         <p>Ein weiteres neu integriertes Werkzeug zur Verbesserung der Datenqualität werden automatische Inferenzmaschinen, sog. Reasoner, sein, die in der Lage sind, widersprüchliche Daten zu erkennen und somit etwaige Eingabe- oder Modellierungsfehler aufzudecken.</p>
      </body>
   </text>

",xml,Creative Commons Attribution 4.0 International,,cidoc crm;semantische web;virtuelle forschungsumgebung;wissensmodellierung;wisski,German,annotieren;artefakte;bearbeitung;bereinigung;bilder;daten;forschungsprozess;infrastruktur;interaktion;kollaboration;kommentierung;kommunikation;kontextsetzung;modellierung;netzwerkanalyse;organisation;software;sprache;text;veröffentlichung;virtuelle forschungsumgebungen;webentwicklung
10538,2016 - University of Leipzig,University of Leipzig,Modellierung - Vernetzung – Visualisierung: Die Digital Humanities als fächerübergreifendes Forschungsparadigma,2016,DHd,DHd,Universität Leipzig (Leipzig University),Leipzig,,Germany,http://dhd2016.de/,Darstellung heterogenen und dynamischen Wissens mit CIDOC CRM und WissKI,,Martin Scholz;Guenther Goerz;Sarah Wagner;Mark Fichtner,paper,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <p>Dieser Vortrag stellt die praktische Arbeit mit WissKI und die ontologische Modellierung mit dem CIDOC-CRM anhand zweier Use Cases vor. Dabei steht der dynamische Umgang mit heterogenen Daten im Fokus.</p>
         <div type=""div1"" rend=""DH-Heading1"">
            <head> Die WissKI-Software</head>
            <p>Die virtuelle Forschungsumgebung “WissKI” (Wissenschaftliche Kommunikations-Infrastruktur, http://wiss-ki.eu/) entstand aus Anforderungen an die kooperative Forschung im Bereich des Kulturerbes und seiner Dokumentation im digitalen Medium. Im Rahmen des DFG-geförderten, gleichnamigen Projekts WissKI wurde die Softwareplattform auf der Basis des Open-Source Content Management Systems Drupal (http://drupal.org) in Zusammenarbeit zwischen dem Germanischen Nationalmuseum Nürnberg (GNM), dem Zoologischen Forschungsmuseum Alexander Koenig in Bonn (ZFMK) und der Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) entwickelt. Fokus des Systems sind neben der einfachen Bereitstellung und offenen Verfügbarkeit von Quellmaterialien – strukturierten Texten, Grafiken, Bildern, Video, Audio – und Metadaten in digitaler Form, auch das interaktive und vernetzte Arbeiten auf der Basis semantischer Tiefenerschließung. Indem die typischen Eigenschaften von gängigen Content Management Systemen unberührt bleiben, verfügt das System über eine detaillierte Nutzersteuerung mit Rechteverwaltung und ist in der Lage Web-Inhalte wie Webseiten, Foren und Wikis zu verwalten und online zu präsentieren.</p>
            <p>Die Software ist als Open-Source kostenfrei unter <ref target=""http://github.com/WissKI"">http://github.com/WissKI</ref> (Görz et
          al. 2009-*) veröffentlicht und kann dementsprechend nachgenutzt und erweitert
          werden.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head> Semantische Modellierung mit dem CIDOC-CRM</head>
            <p>Bei der semantischen Modellierung kommt dem Conceptual Reference Model (CIDOC-CRM, ISO 21127) von ICOM-CIDOC als formaler Referenzontologie eine Schlüsselrolle zu. Unsere Implementation in der Web Ontology Language bildet in Verbindung mit verschiedenen Werkzeugen des Semantic Web die Grundlage des WissKI-Systems. Auch diese Softwarekomponente ist als Open-Source kostenfrei verfügbar und kann unter http://erlangen-crm.org heruntergeladen werden.</p>
            <p>Die Datenakquisition im WissKI-System erfolgt primär über karteikartenähnliche
            Formulare oder Freitextfelder, beides tradierte Formen der Datenerfassung in den
            vorliegenden Anwendungsbereichen. Die Daten werden vom System im Hintergrund
            jedoch durch das CIDOC-CRM semantisch erschlossen, d. h. neben den Daten wird
            auch ihre ontologiebezogene Bedeutung für Mensch und Maschine lesbar
            gespeichert. Hierzu bedient sich WissKI konsequent der Techniken des Semantic
            Web wie RDF und OWL-DL und zielt auf Veröffentlichung der Daten als Linked Open
            Data. Die eingegebenen Daten bilden einen Wissensgraphen, der auf einfache Weise
            weltweit und (potentiell) transdisziplinär vernetzt werden kann. Die formale
            Ontologie, die über eine hierarchische Struktur von Konzepten und Eigenschaften
            und einer standardisierten logischen Sprache ein System von Fachbegriffen
            bildet, dient der Erfassung und semantischen Erschließung der Daten. Je nach
            Fachgebiet und Sammlungsschwerpunkt kann das CIDOC-CRM als grundlegende
            Ontologie um fachspezifische Begriffe mit Anwendungsontologien erweitert werden.
            Diese flexible Art der Wissensspeicherung lässt schnell und unproblematisch
            Änderungen des Datenmodells am laufenden System zu. Durch die Einbindung einer
            standardisierten logischen Sprache und die Nutzung von Open-Source-Software
            ermöglicht WissKI eine hohe Anschlussfähigkeit, leichte Zugänglichkeit und
            Langzeit-Interpretierbarkeit der Daten.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head> Exemplarische Anwendungsszenarien</head>
            <div type=""div2"" rend=""DH-Heading2"">
               <head> EDEN</head>
               <p>Die Epigraphische Datenbank Erlangen-Nürnberg (EDEN, <ref target=""http://wisski.cs.fau.de/eden/"">Dreyer et al. 2014-*</ref>) ist eine Online-Datenbank für antike Inschriften aus Kleinasien. Momentan beschränkt sie sich auf die drei antiken griechischen Siedlungen Metropolis in Ionien, Magnesia am Mäander und Apollonia am Rhyndakos. Inhaltlich wird die Datenbank seit 2012 von Boris Dreyer (Professur für Alte Geschichte, FAU) in enger Zusammenarbeit mit Archäologen der FAU und dem Lehrstuhl für Graphische Datenverarbeitung sowie archäologischen Kollegen in der Türkei gepflegt und weiterentwickelt. Ziel ist die Schaffung eines effektiven Werkzeugs zur interdisziplinären und internationalen Forschung, das Forscher, Studenten und die breite Öffentlichkeit im Spannungsbereich zwischen Alter Geschichte und Archäologie nutzen können. Während sich Althistoriker primär mit den immateriellen Eigenschaften der Inschrift, also Textinhalt und -form, beschäftigen, ist für Archäologen der materielle Träger der Inschrift von größerem Interesse. Die Verknüpfung und gemeinsame Präsentation von Daten für beide Disziplinen bildet ein Novum in diesem Bereich. Daher waren von Anfang an web-basiertes, kollaboratives Arbeiten sowie flexible Datenhaltung wichtige Voraussetzungen der technischen Infrastruktur.</p>
               <p>Der Fokus der Datenbank ist (noch) nicht das Bereitstellen einer großen
                Anzahl an Inschriften, sondern die Angabe detaillierter Metadaten aus
                verschiedenen Disziplinen: zu den edierten Inschriften kommen zahlreiche
                Metadaten in textueller und tabellarischer Form, u. a. Funddaten,
                wissenschaftliche Kommentare, Übersetzungen und hochauflösende Bilder. Diese
                geben wertvolles Wissen für Historiker und Archäologen wieder. Entsprechend
                den Entwicklungen durch Kooperationen und neue Forschungsschwerpunkte rücken
                frühere Randinformationen schnell in den Fokus der Datenbank und werden
                sukzessive verfeinert und ergänzt. Ebenso werden weiterführende
                Informationen zu wiederkehrenden Themen wie bestimmten Herrschern, Genres
                oder Orten laufend hinzugefügt. Die Einbindung von 3D-Modellen soll die
                Datenbank mittelfristig zu virtuellen Ausgrabungsstätten erweitern.</p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head> Sammlung Rück</head>
               <p>Seit 2015 widmen sich Forscher am GNM einem einzigartigen Bestand an Musikinstrumenten. Die Sammlung Rück war die größte deutsche Sammlung historischer Musikinstrumente in Privatbesitz und wurde 1880 vom Pianisten und Lehrer Wilhelm Rück gegründet. Nach seinem Tod setzten sich seine Söhne den systematischen Ausbau der Sammlung und die Dokumentation der Entwicklung abendländischer Musikinstrumente zum Ziel. Eine seit 1929 akribisch geführte Dokumentation aller Sammlungsaktivitäten ist Ulrich Rück, dem Sohn des Sammlungsgründers, zu verdanken. Diese Korrespondenz mit über 1000 Partnern umfasst 17.200 Briefe und Postkarten, die zusammen mit 1.500 Musikinstrumenten und weiterem Material 1962 vom GNM erworben wurden. Alle Dokumente werden seither im Historischen Archiv, die Objekte selbst im Sammlungsreferat für Musikinstrumente aufbewahrt.</p>
               <p>Die Korrespondenz zum Aufbau der Sammlung Rück bietet heute eine einzigartige Quelle, Einblicke in das Handeln mit Musikinstrumenten und in die Entstehung einer Sammlung von Objekten des Kulturlebens in der Zeit zwischen Weltwirtschaftskrise und Wirtschaftswunder gewinnen zu können. Besondere Bedeutung kommt dem Schriftverkehr mit Gutachtern bezüglich Qualität und Marktwert der Instrumente zu, da hieraus ermöglicht wird,
                  einen Preisspiegel für den Handel mit historischen Musikinstrumenten der damaligen Zeit zu erstellen. Dieser soll zum Abschluss des Projekts gemeinsam mit Recherche-Ergebnissen zu den Erwerbsumständen und unmittelbaren Vorbesitzern online zugänglich gemacht werden. Weitere wichtige Aspekte der Untersuchung sind Sammlungs-, Kommunikations- und Marketingstrategien, die zu einer europaweiten Vernetzung der Sammlung führten, und die in einer Monographie dargestellt werden.
                </p>
               <p>Zentrale Herausforderung des Nachlasses Rück ist die Heterogentität der Materialien. So sind neben den Archivgütern, die inhaltlich tiefenerschlossen werden sollen, auch Objekt-, Personen- und Literaturdaten mit entsprechenden Kreuzverweisen zu erfassen. Auf der Basis dieser Verweise soll die Abfolge der Kommunikation mit den verschiedenen Kommunikationspartnern, die geschäftlichen Reisen der Familie Rück, Erwerbungen in ihren zeitlichen und räumlichen Beziehungen und der generelle Kontext zur damaligen Zeit dargestellt werden.</p>
               <p>Der Großteil der Archivmaterialien aus dem Nachlass Rück ist mit Schreibmaschine geschrieben und deshalb gut lesbar. Eine händische Transkription der Materialien ist nicht notwendig. Die Erschließung erfolgt auf Basis von hochqualitativen Digitalisaten der Briefe, die anschließend die Wissenschaftler inhaltlich zusammenfassen. In diesen Zusammenfassungen werden entsprechende Referenzierungen zu den anderen Materialien des Projektes vorgenommen. Hierfür ist eine Verlinkung und idealerweise eine Auszeichnung aus dem Fließtext heraus notwendig. Gleichzeitig wird ein System benötigt, das die verknüpften Materialien geeignet präsentieren kann. </p>
            </div>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head> Praktische Umsetzung</head>
            <div type=""div2"" rend=""DH-Heading2"">
               <head> EDEN</head>
               <p>Durch die Einbindung von Drupal bringt WissKI einerseits die nötigen Voraussetzungen für web-basiertes, kollaboratives Arbeiten mit. Der generische Aufbau ermöglicht andererseits die Anpassung an die Erfordernisse der beteiligten Disziplinen. Weiterhin erleichtert der Einsatz von Ontologien zur Datenspeicherung einen oben angesprochenen Fokuswechsel, da Daten nicht neu erfasst werden müssen, sondern lediglich die bereits bestehenden Randinformationen angereichert werden und somit der Detailgrad auf neue Anforderungen angehoben werden kann. In EDEN wurden beispielsweise die rudimentären, zunächst aufgrund von Nennungen im Text erfassten Personendaten zu Datensätzen ersten Ranges mit eigenem wissenschaftlichen Kommentar ausgebaut.Ebenso könnten die geographischen Informationen durch eine Kooperation mit Geographen ausgeweitet werden und EDEN zu einer für (kultur-)geographische Forschungen nutzbaren Quelle machen. Diese Änderungen des Datenmodells konnten mit WissKI problemlos parallel zum Einpflegen der Daten erfolgen. Somit kann die Datenbank nicht nur hinsichtlich der Datenmenge, sondern auch hinsichtlich der Fülle der Metadaten bei Bedarf stetig wachsen.</p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head> Sammlung Rück</head>
               <p>Das im Archivbereich des GNM bisher benutzte Dokumentationssystem “Faust” (http://www.land-software.de/ ) ist für die Erfassung, Erschließung und Abbildung der Projektdaten in vielerlei Hinsicht unzureichend. Zum einen würde eine Anpassung des Archivsystems auf die Bedürfnisse des Projekts Rück dazu führen, dass in allen anderen Anwendungsbereichen entsprechende Felder auftauchen. Eine inhaltliche Tiefenerschließung in der Qualität des Projekts Rück ist für die sonstigen Archivmaterialien eher untypisch, da für die Erfassung letzterer insbesondere die Quantität im Vordergrund steht.</p>
               <p>Faust hat bislang keine Komponente zur Text-Annotierung in Fließtexten, was für die semantische Tiefenerschließung der Archivmaterialien essenziell ist. Auch ist Faust nur bedingt in der Lage, die Verknüpfungsdichte an Daten darzustellen. Unter diesem Aspekt kann eines der Kernziele, das Netzwerk agierender Personen im zeitlichen und räumlichen Kontext abzubilden, kaum realisiert werden.</p>
               <p>WissKI erfüllt die Bedürfnisse des Projekts, da es über einen Text-Annotierer
                  für Fließtext verfügt und auf der Basis von Semantic Web-Techniken
                  Verknüpfungen flexibel darstellt und auswertet. Auch konnte mit WissKI eine
                  direkte Schnittstelle am Objektdatensatz zu MIMO (2009-*) (<ref target=""http://www.mimo-international.com/"">musical instrument
                  museums</ref>) eingerichtet werden.</p>
               <p>In WissKI können die Forschungsprimärdaten inklusive der Digitalisate in voller TIFF-Qualität und allen Annotierungen der Fachöffentlichkeit zur Verfügung gestellt und eine Präsentationsoberfläche für die breite Öffentlichkeit geschaffen werden.</p>
            </div>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head> Fazit und Ausblick</head>
            <p>Obgleich in ihren Disziplinen und Objektgattungen verschieden, haben beide Anwendungsfälle gemeinsame Herausforderungen: Zum einen sind sie mit heterogenen, stark vernetzten Daten und anwendungsspezifischen Anforderungen konfrontiert. Zum anderen entwickeln sich die Anforderungen an die Software hinsichtlich Umfang und Vernetzung der Metadaten im Projektverlauf. WissKI ist jedoch aufgrund seiner Systemarchitektur darauf bestens vorbereitet.</p>
            <p>Im derzeit noch laufenden Projekt WissKI² wird die Entwicklung der WissKI-Software konsequent weitergeführt. Neben den bereits bewährten Formularfeldern und Bildern sollen nun verstärkt auch interaktive Landkarten, Zeitstrahlen, 3D-Animationen und weitere Medientypen eingebunden und dargestellt werden. Damit ergeben sich weitere Möglichkeiten der Datenpräsentation und der Wissensvernetzung für die vorgestellten Anwendungsfälle.</p>
         </div>
      </body>
      <back>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Dreyer, Boris / Holdenried, Marvin / Scholz, Martin</hi>
                    (2014-*): <hi rend=""italic"">EDEN — Epigraphische Datenbank
                    Erlangen-Nürnberg</hi>. WissKi: Friedrich-Alexander-University,
                    Erlangen-Nuremberg (FAU) <ref target=""http://wisski.cs.fau.de/eden/"">http://wisski.cs.fau.de/eden/</ref> [letzter Zugriff 08. Januar 2016]. </bibl>
               <bibl>
                  <hi rend=""bold"">Görz, Günther </hi> (2011): ""WissKI: Semantische Annotation,
                      Wissensverarbeitung und Wissenschaftskommunikation in einer virtuellen
                      Forschungsumgebung"", in: <hi rend=""italic"">Kunstgeschichte. Open Peer
                      Reviewed Journal</hi>
                  <ref target=""http://www.kunstgeschichte-ejournal.net/167/"">http://www.kunstgeschichte-ejournal.net/167/</ref> [letzter Zugriff 08.
                        Januar 2016]. </bibl>
               <bibl>
                  <hi rend=""bold"">Görz, Günther / Scholz, Martin </hi> (2012): ""WissKI: A
                          Virtual Research Environment for Cultural Heritage"", in: De Raedt, Luc, Luc
                          De Raedt, Bessiere, Christian / Dubois, Didier / Doherty, Patrick /
                          Frasconi, Paolo / Heintz, Fredrik / Lucas, Peter (eds.): <hi rend=""italic"">20th European Conference on Artificial Intelligence, ECAI 2012,
                          Proceedings</hi>. IOS Press <ref target=""http://www2.lirmm.fr/ecai2012/"">http://www2.lirmm.fr/ecai2012/</ref> [letzter Zugriff 08. Januar 2016]. </bibl>
               <bibl>
                  <hi rend=""bold"">Görz, Günther / Scholz, Martin / Merz, Dorian / Krause,
                              Siegfried / Fichtner, Mark / Reinfandt, Kerstin / Grobe, Peter /
                              Pfeifer, Maria Anna </hi>(2009-*): <hi rend=""italic"">WissKi: Scientific
                              Communication Infrastructure</hi>. Friedrich-Alexander-University,
                              Erlangen-Nuremberg (FAU); Digital Humanities Research Group, Department of
                              Computer Science, Germanisches Nationalmuseum (GNM) Nuremberg; Biodiversity
                              Informatics Group, Department of Museum Informatics, Zoologisches
                              Forschungsmuseum Alexander Koenig (ZFMK) Bonn <ref target=""http://wiss-ki.eu/"">http://wiss-ki.eu/</ref> [letzter Zugriff:
                              08. Januar 2016]. </bibl>
               <bibl>
                  <hi rend=""bold"">Hohmann, Georg / Fichtner, Mark</hi> (2015): ""Chancen und
                                Herausforderungen in der praktischen Anwendung von Ontologien für das
                                Kulturerbe"", in: Robertson-von Trotha, Caroline Y. / Schneider, Ralf M. (
                                eds.): <hi rend=""italic"">Digitales Kulturerbe</hi>. Bewahrung und
                                Zugänglichkeit in der wissenschaftlichen Praxis (= Kulturelle Überlieferung
                                – digital 2). Karlsruhe: Karlsruhe Scientific Publishing 115-128. </bibl>
               <bibl>
                  <hi rend=""bold"">MIMO</hi> (2009-*): <hi rend=""italic"">Musical Instrument
                                  Museums Online</hi>. Philharmonie de Paris: Cité de la musique, The
                                  University of Edinburgh, Germanisches Nationalmuseum,
                                  Muziekinstrumentenmuseum, Association ""Amici del Museo degli Strumenti
                                  Musicali"" <ref target=""http://www.mimo-international.com/MIMO/accueil-ermes.aspx"">http://www.mimo-international.com/MIMO/accueil-ermes.aspx</ref>
                                  [letzter Zugriff 08. Januar 2016]. </bibl>
               <bibl>
                  <hi rend=""bold"">Scholz, Martin / Holdenried, Marvin / Dreyer, Boris /
                                      Meyer-Wegener, Klaus / Görz, Günther </hi> (2014): ""Und Semantik wuchs
                                      in EDEN - Eine Vorstellung und ein Erfahrungsbericht"", in: <hi rend=""italic"">Magazin für digitale Editionswissenschaften</hi> 1, 1: 22-30 <ref target=""https://www.mde.fau.de/files/2015/03/MdE-2015-01_3_Scholz_et_al.pdf"">https://www.mde.fau.de/files/2015/03/MdE-2015-01_3_Scholz_et_al.pdf</ref>
                                      [letzter Zugriff 08. Januar 2016]. </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,semantic web;text-bild-beziehung;virtuelle forschungsumgebung;wissensmodellierung,German,annotieren;artefakte;bearbeitung;benannte entitäten (named entities);bilderfassung;entdeckung;kollaboration;kommunikation;modellierung;netzwerkanalyse;standards;texttragende gegenstände;transkription;virtuelle forschungsumgebungen;webentwicklung;werkzeuge
10556,2016 - University of Leipzig,University of Leipzig,Modellierung - Vernetzung – Visualisierung: Die Digital Humanities als fächerübergreifendes Forschungsparadigma,2016,DHd,DHd,Universität Leipzig (Leipzig University),Leipzig,,Germany,http://dhd2016.de/,Digitale Workflows in Langzeitprojekten am Beispiel einer Infrastruktur zur Dokumentation indigener nordeurasischer Sprachen (INEL),,Hanna Hedeland;Timm Lehmberg;Beata Wagner-Nagy,paper,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Zusammenfassung</head>
            <p>Gegenstand des Beitrages sind die Arbeiten zu digitalen Workflows und infrastruktureller
          Einbindung im Rahmen des Langzeitprojektes <hi rend=""italic"">INEL</hi> (Grammatical
          Descriptions, Corpora, and Language Technology for Indigenous Northern Eurasian
          Languages). Das Projekt wurde von Prof. Dr. Beata Wagner-Nagy (Institut für
          Finnougristik/Uralistik, Hauptantragstellerin) sowie von Dr. Michael Rießler
          (Skandinavisches Seminar Albert-Ludwigs-Universität Freiburg) und der Geschäftsführung des
          Hamburger Zentrums für Sprachkorpora (Hanna Hedeland und Timm Lehmberg) beantragt. Ziel
          des Projektes ist es, über den Zeitraum von 18 Jahren die dringend erforderliche
          Erschließung der sprachlichen Ressourcen des genealogisch diversen nordeurasischen
          Sprachraums (s. Abbildung 1) zu leisten. Durch den Einsatz von State-of-the-Art-Methoden
          und -Werkzeugen der linguistischen Datenaufbereitung, die bisher nur für gut erforschten
          Sprachen und Varietäten zum Einsatz kamen, wird eine Lücke in diesen für die empirische
          Sprachwissenschaft bisher schlecht zugänglichen Arealen der Welt nachhaltig geschlossen. </p>
            <figure>
               <graphic n=""1001"" width=""16.002cm"" height=""19.939cm"" url=""044-image1.png"" rend=""inline""/>
            </figure>
            <p>
               <hi rend=""bold"">Abb. 1</hi>: Der geographische Skopus des Projekts. </p>
            <p>Dieses ehrgeizige Ziel stellt hohe Anforderungen an die Organisation der Projektworkflows
            und erfordert zudem die Schaffung einer eigenen nachhaltigen und international vernetzten
            digitalen Forschungsinfrastruktur. Das Projekt leistet somit jenseits seiner
            linguistischen Ausrichtung einen wichtigen Beitrag für die Digital Humanities.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Anforderungslage</head>
            <p>Aufgrund des drohenden Verfalls der zum großen Teil auf obsoleten analogen
              Originalträgern (Wachswalzen, Schellackplatten, Mikrofiche etc.) vorhandenen
              Audio-Aufnahmen, Niederschriften und Beschreibungen, schließt sich in absehbarer Zeit das
              Fenster für einen Erhalt dieser Daten. Gleichzeitig gehen die Sprecherzahlen vieler
              Sprachen und Varietäten stetig zurück. Indem existierende Materialien zu digitalen Korpora
              aufbereitet und der bisherige Gesamtbestand um neue Ressourcen ergänzt wird, kann dieses
              Erbe als wertvolle empirische Basis für vielfältige Forschungsvorhaben erhalten werden.
              (Eine detaillierte Beschreibung des Standes der linguistischen Erfassung befindet sich im
              Förderantrag, S. 3 ff.)</p>
            <p>Vielmehr als nur ein digitales Archiv entsteht im Rahmen von <hi rend=""italic"">INEL</hi>
              jedoch eine umfassende virtuelle Forschungsumgebung, die durch die Integration in
              supranationale Forschungsinfrastrukturen der wissenschaftlichen Öffentlichkeit dauerhaft
              zugänglich gemacht wird. Ein primäres Ziel des Projektes besteht zunächst darin,
              existierende Beschreibungen einzelner nordeurasischer Sprachen und Varietäten, die
              aufgrund der bisher begrenzten Auswahl von verfügbaren Sprechern und Genres eher
              partikuläre Idiolekte dokumentieren, zusammenzutragen und mit ergänzenden Korpora als
              umfangreiche digitale Ressource zugänglich zu machen. Durch die so geschaffene, der
              Vielfalt der Sprache angemessenen, Datenbasis werden für zukünftige Generationen von
              Forschenden erstmalig varietätenübergreifende Analysen möglich, etwa die Erforschung
              kontaktinduzierter Sprachveränderungen, Anwendungen aus dem Bereich der Dialektometrie
              oder sprachsoziologische Untersuchungen. Die unterschiedlichen Erhebungszeiten der
              Sprachdaten erlauben zudem erstmalig datengestützte Untersuchungen von diachronem
              Sprachwandel sowie Grammatikalisierungsprozessen. Ebenso bedeutend sind die Art des
              Zugangs zu den Sprachdaten und die damit verbundenen Analysemöglichkeiten. Die Sprachdaten
              können in der entstehenden Forschungsumgebung kollaborativ und dezentral um beliebige
              weitere Beschreibungsebenen angereichert werden, die dann für verschiedene
              Auswertungsszenarien zur Verfügung stehen. Auf diese Weise wird die virtuelle
              Forschungsumgebung modular aufgebaut und in vielen Fällen so generisch sein, dass auch die
              Resultate technologischer und methodologischer Entwicklungen der akademischen
              Öffentlichkeit als Best Practices und als konkrete Grundlage für vergleichbare Vorhaben
              zur Verfügung stehen werden. </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Modularisierung und Workflows</head>
            <p>Die oben beschriebene Ausgangslage determiniert zwei Dimensionen der Entwicklung der
                entstehenden Ressourcen, denen durch entsprechende Modularisierung von Workflows begegnet
                werden muss. </p>
            <p>1. Entwicklung hinsichtlich der arealen Abdeckung durch Erschließung der Einzelsprachen. </p>
            <p>2. Entwicklung hinsichtlich der Komplexität der Daten infolge von hinzuzufügenden
                  Glossierungen und Mehrebenenannotationen, die sowohl innerhalb des Erfassungsprozesses
                  jeder Einzelsprache als auch über den gesamte Laufzeit erfolgen. </p>
            <figure>
               <graphic n=""1002"" width=""16.002cm"" height=""8.71108888888889cm"" url=""044-image2.png"" rend=""inline""/>
            </figure>
            <p>
               <hi rend=""bold"">Abb. 2</hi>: Teilprojekte.</p>
            <p>Der folgtende Abschnitt basiert auf den den im Förderantrag (S. 17 ff) beschriebenen
                      konzeptionellen Vorarbeiten zur Modularisierung der Projektworkflows. </p>
            <p>Das Projekt gliedert sich dem entsprechend in insgesamt zwölf Teilprojekte (s. Abbildung
                        2), von denen elf jeweils die Erschließung einer der zu erfassenden Sprachvarietäten zum
                        Gegenstand haben. Das zwölfte technisch-infrastrukturelle Teilprojekt läuft im Gegensatz
                        zu den elf jeweils auf drei Jahre angelegten Erschließungsprojekten durchgängig über die
                        gesamte Projektlaufzeit und schafft somit die notwendige Kontinuität für die Entwicklung,
                        Anpassung und Vermittlung der Funktionalitäten der technischen Infrastruktur. Das
                        Arbeitsprogramm wird für die jeweiligen Teilprojekte aber auch teilprojektübergreifend in
                        Form von methodischen Arbeitspaketen umgesetzt:</p>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Arbeitspaket 1: Korpusaufbau</head>
               <p>In diesem Arbeitspaket werden existierende Ressourcen erschlossen, kuratiert und
                            aufbereitet sowie ggf. um neu zu akquirierende und zu erhebende Daten ergänzt. Die dabei
                            erforderlichen Verarbeitungsschritte sind, wie in Abbildung 3 dargestellt,
                            modularisiert. Die Module Digitalisierung, linguistische Modellierung, Annotation /
                            Glossierung und Finalisierung/Integration entsprechen jeweils Aufbereitungsschritten,
                            die abhängig vom Ausgangszustand der einzelnen Ressource für die Integration in den
                            Gesamtbestand erforderlich sind. </p>
               <figure>
                  <graphic n=""1003"" width=""16.002cm"" height=""12.0015cm"" url=""044-image3.png"" rend=""inline""/>
               </figure>
               <p>
                  <hi rend=""bold"">Abb. 3</hi>: Modularisierung und Anpassung der
                              Verarbeitungsschritte.</p>
               <p>In der Praxis handelt es sich jedoch keineswegs um einen linearen Prozess, den einzelne
                                Texte einmalig durchlaufen und an dessen Ende die Speicherung und Zugänglichkeit einer
                                in sich geschlossenen Ressource in einem digitalen Repositorium steht. Vielmehr müssen
                                die zu planenden Workflows der Aufbereitung und Speicherung den tatsächlichen
                                Gegebenheiten der Datenaufbereitung in Projekten dieser Art Rechnung tragen (s.
                                Abbildung 4):</p>
               <p>- Es ist in vielen Fällen wünschenswert, Versionen von Ressourcen bereits in einem
                                  frühen Stadium der Aufbereitung (beispielsweise noch vor Abschluss der Annotation und
                                  Glossierung) der wissenschaftlichen Öffentlichkeit zugänglich zu machen. </p>
               <p>- Bei dem Arbeitsschritt der Annotation und Glossierung handelt es sich wiederum um
                                    iterative Prozesse, in deren Rahmen mehrere Ebenen der Auszeichnung, möglichwerweise
                                    sogar zeitlich überlappend, zu den Primärdaten hinzugefügt werden, was hohe
                                    Anforderungen an Koordination und Qualitätskontrolle stellt. </p>
               <p>- Insbesondere bei Langzeitvorhaben entstehen oft neue Versionen von Korpora aus
                                      bereits bestehenden Ressourcen, indem diese mit zusätzlichen Annotationsebenen
                                      ausgezeichnet werden.</p>
               <figure>
                  <graphic n=""1004"" width=""16.002cm"" height=""7.355416666666667cm"" url=""044-image4.png"" rend=""inline""/>
               </figure>
               <p>
                  <hi rend=""bold"">Abb. 4</hi>: Nicht-lineare Abfolge der Verarbeitungsschritte. </p>
               <p>Diesen Gegebenheiten kann in der Praxis durch ein ausdifferenziertes
                                            Versionierungskonzept begegnet werden. Im Rahmen des Vortrages werden einige konkrete
                                            Workflows aus dem Projekt vorgestellt und ihre Implementierung unter Verwendung eines
                                            <ref target=""http://git-scm.com"">Git</ref>-basierten Repositoriums ausführlich
                                            erläutert. </p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Arbeitspaket 2: Infrastruktur und Best Practices</head>
               <p>Die zu errichtende Infrastruktur basiert in vielerlei Hinsicht auf den Vorarbeiten des
                                              HZSK sowie generell auf vorangegangenen Erkenntnissen aus dem Aufbau digitaler
                                              Forschungsumgebungen. Große Teile der gewünschten Funktionalitäten wurden bisher in Form
                                              von Standalone-Werkzeugen (bspw. <ref target=""http://www.exmaralda.org"">EXMARaLDA</ref>)
                                              oder Webapplikationen (bspw. als Teil der CLARIN-D-Infrastruktur) am HZSK entwickelt und
                                              eingesetzt. Einen integralen Bestandteil der Infrastruktur bildet ein Repositorium, mit
                                              dessen Hilfe die nachhaltige Datenvorhaltung gewährleistet werden kann. Weitere
                                              Komponenten, die unmittelbar daran anknüpfen, bilden zusätzliche relevante Aspekte der
                                              gewünschten Funktionalität der Infrastruktur ab, wie etwa die kollaborative Bearbeitung
                                              und Aufbereitung von Daten in der Arbeitsumgebung, die Auslieferung von Metadaten an
                                              Kataloge und Archive, welche die Auffindbarkeit der Ressourcen für andere Forscher
                                              ermöglicht, sowie Schnittstellen für die Exploration und Analyse der vorgehaltenen
                                              Ressourcen. Auch die fortlaufende Anbindung an bzw. Vernetzung mit weiteren bestehenden
                                              Forschungsinfrastrukturen wird als essentielles Merkmal des Projektes betrachtet. </p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Arbeitspaket 3: Evaluation und Dissemination</head>
               <p>Neben der organisatorischen und der technisch-infrastrukturellen Organisation und
                                                Vernetzung, die mit den Arbeitspaketen 1 und 2 abgedeckt ist, erfordert ein
                                                Langzeitprojekt wie <hi rend=""italic"">INEL</hi> zudem umfassende Arbeiten im Bereich der
                                                Dissemination und den Austauschs im supranationalen interdisziplinären Kontexten mit
                                                anderen Forschenden. Um neue Arbeitsinstrumente zu entwickeln und zu diskutieren,
                                                Arbeitspläne zu koordinieren, aktuelle Forschungsfragen und die mit der Projektarbeit
                                                zusammenhängenden praktischen Fragestellungen zu diskutieren sowie zu Zwecken der
                                                Fortbildung werden im Rahmen von <hi rend=""italic"">INEL</hi> jährliche Workshops
                                                abgehalten, an denen ausgewählte Konsultanten sowie Kooperationspartner aus dem Ausland
                                                und die Projektmitarbeiter selbst teilnehmen.</p>
            </div>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Ausblick</head>
            <p>Der Beitrag basiert auf den Planungen zu der <hi rend=""italic"">INEL</hi>-Langzeitprojekt,
                                              dessen 18-jährige Laufzeit im Januar 2016 beginnen wird. Die Vorarbeiten zu dem Projekt
                                              lieferten wichtige Erkenntnisse hinsichtlich der Modularisierung von Projektabläufen sowie
                                              der Priorisierung von Verarbeitungsschritten der Datenaufbereitung. So wird beispielsweise
                                              der technische Fokus nicht auf der Neuentwicklung von weiteren Werkzeugen und Standards
                                              der Datenaufbereitung, sondern der Entwicklung von modularisierten Infrastrukturen und
                                              Workflows liegen, die auf die Interoperabilität, Interaktion und Integration existierende
                                              Komponenten abzielen. Nur so kann ein flexibler Betrieb der <hi rend=""italic"">INEL</hi>-Infrastruktur in einer sich permanent wandelnden Ressourcen- und
                                              Infrastrukturlandschaft gewährleistet werden. </p>
            <p>Von der Entwicklung und Erprobung kontrollierter und modularisierter Workflows der
                                                Datenaufbereitung, die beispielsweise eine transparente Dokumentation und Publikation
                                                entstehender Versionen von Forschungsdatensammlungen erlauben, sind zudem wichtige
                                                Beiträge auf dem Feld des Forschungsdatenmanagements von Projekten in den Digital
                                                Humanities zu erhoffen.</p>
         </div>
      </body>
      <back>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Git-Hub</hi> (o.J.): <hi rend=""italic"">Git</hi>. Local branching on
                                                  the cheap <ref target=""http://git-scm.com/"">http://git-scm.com/</ref> [letzter Zugriff
                                                  16. Februar 2016].</bibl>
               <bibl>
                  <hi rend=""bold"">INEL</hi> (2015): <hi rend=""italic"">Grammatiken, Korpora, Sprachtechnologie
                                                  für indigene nordeurasische Sprachen</hi>. Förderantrag, eingereicht bei der Union der deutschen
                                                  Akademien der Wissenschaften.</bibl>
               <bibl>
                  <hi rend=""bold"">Hedeland, Hanna / Lehmberg, Timm / Schmidt, Thomas / Wörner,
                                                    Kai</hi> (o.J.): <hi rend=""italic"">EXMARaLDA</hi>. Werkzeuge für mündliche Korpora
                                                    <ref target=""http://www.exmaralda.org/"">http://www.exmaralda.org/</ref> [letzter
                                                    Zugriff 16. Februar 2016].</bibl>
               <bibl>
                  <hi rend=""bold"">HZSK</hi> (o.J.): <hi rend=""italic"">Hamburger Zentrum für
                                                    Sprachkorpora</hi>
                  <ref target=""https://corpora.uni-hamburg.de/drupal/"">https://corpora.uni-hamburg.de/drupal/</ref> [letzter Zugriff 16. Februar
                                                      2016].</bibl>
               <bibl>
                  <hi rend=""bold"">Universität Hamburg</hi> (o.J.): <hi rend=""italic"">Institut für
                                                      Finnougristik / Uralistik</hi>
                  <ref target=""https://www.slm.uni-hamburg.de/ifuu"">https://www.slm.uni-hamburg.de/ifuu</ref> [letzter Zugriff 16. Februar 2016].</bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,best practices;forschungsinfrastruktur;langzeitprojekt;sprachdokumentation;workflows,German,annotieren;archivierung;aufzeichnung;bearbeitung;bereinigung;community-bildung;daten;datenerkennung;entdeckung;infrastruktur;karte;kollaboration;konservierung;lehre;manuskript;metadaten;methoden;organisation;programmierung;projekte;projektmanagement;sammlung;sprache;standards;teilen;theoretisierung;ton;transkription;umwandlung;veröffentlichung;virtuelle forschungsumgebungen;webentwicklung
10642,2017 - University of Bern,University of Bern,Digitale Nachhaltigkeit,2017,DHd,DHd,Universität Bern (University of Bern),Bern,,Switzerland,http://www.dhd2017.ch/,"Langzeitinterpretierbarkeit auf Basis des CIDOC-CRM in inter- und transdisziplinären Forschungsprojekten am Germanischen Nationalmuseum (GNM), Nürnberg",,Peggy Große;Sarah Wagner,paper,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <p>Im musealen Bereich spielt die Frage, wie man langfristig interpretierbare Daten erzeugt und bereitstellt, eine immer größere Rolle, insbesondere wenn, wie am Germanischen Nationalmuseum (GNM), drittmittelgeförderte inter- und transdisziplinäre Forschungsprojekte große Datenmengen zu den Objektbeständen erheben. Welche Lösungsansätze für den nachhaltigen Umgang mit Forschungsdaten das GNM verfolgt, soll anhand zweier Forschungsprojekte dargestellt werden.</p>
         <p>1. Anforderungen und Ziele des transdisziplinären Forschungsprojektes zu Friedensrepräsentationen in der Vormoderne</p>
         <p>Das von der Leibniz-Gemeinschaft seit Juli 2015 geförderte internationale Kooperationsprojekt „Repräsentationen des Friedens im vormodernen Europa“ erforscht Friedensbilder im Zeitraum vom 16. bis 18. Jahrhundert. Friedensvereinbarungen mussten über den reinen Vertragstext hinaus erklärt, begründet und vermittelt werden. Das übernahmen Friedensrepräsentationen, die ein multimediales Phänomen der Frühen Neuzeit waren. Folglich nimmt das Forschungsprojekt visuelle Darstellungen, sprachliche Bilder sowie musikalische Ausprägungsformen in den Blick. Dieser breite Ansatz erfordert die Kooperation unterschiedlicher geisteswissenschaftlicher Fachrichtungen mit ihren jeweiligen Analysekompetenzen und Perspektiven sowie Institutionen mit geeigneten Beständen.
                <ref n=""2"" target=""vortrag-GROSSftn2"" type=""note"">2</ref>
         </p>
         <p>Um abstrakte Konzepte wie Frieden, Gerechtigkeit oder Wohlstand darzustellen, verwendeten Künstler, Dichter oder Komponisten einen Kanon von Motiven, die europaweit genutzt und verstanden wurden. Dieses „Vokabular“ des Friedens soll beispielhaft erschlossen und über die Gattungs- und Genregrenzen hinweg analysiert werden. Zudem wurden gemeinsame Fragestellungen zu transmedialen Rezeptionsvorgängen, Veränderungen der Motivik im Zusammenhang mit unterschiedlichen Friedensschlüssen, zu Funktion und Wahrnehmung von visuellen, sprachlichen und musikalischen Konzepten entwickelt. Am Anfang steht daher die transdisziplinäre Erfassung und Nutzung der heterogenen Bestände.</p>
         <p>Ein entsprechendes Dokumentationssystem muss demzufolge für alle beteiligten WissenschaftlerInnen unabhängig von der Art und Darstellungsform der Quellen gewährleisten, dass sie schnell und effizient die relevanten Informationen eingeben und abrufen können. Die erfassten Informationen beziehen sich auf objektbezogene Daten, aber auch auf deren Inhalte und Form, wie Ikonografie, Textgattung oder Instrumentierung. Außerdem soll der inhaltliche Zusammenhang zwischen Objekten und Friedensereignissen dokumentiert werden. Daher muss die Datenbank in der Lage sein, auch Zusammenhänge strukturiert abbilden zu können. Die Ergebnisse sollen in einem virtuellen Themenportal am Ende des Projektes veröffentlicht werden. Die Einbindung von digitalen Bild-, Text- und Musikquellen ist daher wünschenswert, ebenso die Möglichkeit mit einem Thesaurus arbeiten und bereits vorhandene Normdaten einbinden zu können.</p>
         <p>2. Anforderungen und Ziele des interdisziplinären Forschungsprojektes MUSICES</p>
         <p>Das Projekt „MUSICES“ (MUSIkinstrumenten-Computertomographie-Examinierungs-Standard) hat es sich zur Aufgabe gemacht einen Standard zu entwickeln, der die Bedingungen für eine wissenschaftliche und praxisnahe Abbildung von Musikinstrumenten durch 3D-Computertomographie beschreibt. Das zerstörungsfreie, bildgebende Verfahren der Computertomographie ist ein wichtiges Instrument geworden, um Informationen über den Aufbau und die Konstruktion von Musikinstrumenten zu gewinnen und so Aussagen über Herstellungsweise, Erhaltungszustand und klangliche Eigenschaften zu liefern. In Kooperation von WissenschaftlerInnen und RestauratorInnen des Germanischen Nationalmuseums und des Fraunhofer Instituts EZRT (Entwicklungszentrum Röntgentechnik) in Fürth werden gemeinsam die technischen Parameter, effiziente und objektschonende Praxisabläufe sowie die Möglichkeiten und Grenzen dieser Technik intensiv erarbeitet.</p>
         <p>Die Entwicklung des Standards besteht aus verschiedenen Aspekten: Zunächst bedarf es eines Schemas, das den kompletten Ablauf der Untersuchung des Instruments dokumentiert, von der Auswahl eines Objekts und die Fragestellung an dieses bis über den Transport, die eigentliche Messung und deren Parameter sowie die daraus erzeugten 3D-Röntgenbilder. Im Laufe des Projekts werden über 100 verschiedene Instrumente erforscht, die in ihrer Auswahl eine möglichst große Vielfalt an Eigenschaften abbilden sollen, um die Anwendbarkeit des Standards auch auf andere Objekte übertragen zu können. Unterschiedliche Materialien und die geometrischen Formen der Musikinstrumente spielen bei den einzustellenden Parametern der 3D-CT eine entscheidende Rolle, um die gewünschten Resultate zu erzielen. Für die Objekte werden deshalb ihren Eigenschaften entsprechend Kategorien definiert. Auf diese Weise können Richtwerte entwickelt werden, beispielsweise für die Strahlungsdosis, die vom Material und dessen Stärke abhängig sind. Die Relation zwischen Objektkategorie und Messeinstellungen in Abhängigkeit von der Forschungsfrage muss durch das Dokumentationsschema abgebildet werden. Letzteres muss zudem aufgrund der stetigen Optimierung des Untersuchungsprozesses während des Projektverlaufs flexibel gestaltet sein.</p>
         <p>Als Teil des Standards soll das Dokumentationsschema in bestehende Standards integriert werden und als Metadatenmodell für künftige Projekte dienen, die sich mit der 3D-CT von Objekten beschäftigen. Alle gewonnenen Daten sollen zum Projektende in das Objektdokumentationssystem des Germanischen Nationalmuseums integriert, darüber hinaus aber auch an internationale Portale geliefert und öffentlich zugänglich gemacht werden.</p>
         <p>3. CIDOC CRM und WissKI als Werkzeuge der Dokumentation und Langzeitinterpretierbarkeit</p>
         <p>Die semantische Erschließung, die eine nachhaltige Interpretierbarkeit von heterogenen Forschungsdaten zunächst innerhalb einer Institution gewährleistet, erfolgt auf Grundlage einer Ontologie, die es ermöglicht Wissen formal zu definieren, zu kategorisieren, zu beschreiben und auszutauschen. Forschungsprojekte am GNM verwenden das ISO-zertifizierte Conceptual Reference Model (CIDOC CRM, ISO 21127, Doerr / Lampe / Krause 2011).
                <ref n=""3"" target=""vortrag-GROSSftn3"" type=""note"">3</ref> Da das CRM nicht maschinell lesbar ist, wurde dies im sog. „Erlangen-CRM“
                <ref n=""4"" target=""vortrag-GROSSftn4"" type=""note"">4</ref> auf Basis von OWL
                <ref n=""5"" target=""vortrag-GROSSftn5"" type=""note"">5</ref> nachgeholt (Görz 2011).
            </p>
         <p>Damit die Projektdaten in einem gemeinsamen Kontext unter Verwendung einer gemeinsamen „Sprache“ dokumentiert werden können, werden Anwendungs- bzw. Domänenontologie, basierend auf dem CIDOC CRM, für jedes Projekt entwickelt. Der Austausch von Daten und deren Langzeitinterpretierbarkeit wird durch die gemeinsame Basis des CIDOC CRM gewährleistet, während alle Spezifika der jeweiligen Projekte möglichst fachspeziell durch die Domänenontologie abgedeckt sind (Hohmann / Fichtner 2015, 117-118). Dies geschieht unter dem Vorbehalt, dass innerhalb einer Institution die Klassen und Eigenschaften gleich gehandhabt werden.</p>
         <p>Um das angesprochene kollaborative und transdisziplinäre Arbeiten zu ermöglichen, benötigt man eine virtuelle Forschungsumgebung. Ausgewählt wurde WissKI
                <ref n=""6"" target=""vortrag-GROSSftn6"" type=""note"">6</ref>, dessen Fokus auf dem interaktiven und vernetzten Arbeiten basierend auf semantischer Tiefenerschließung mit Hilfe des Erlangen-CRM liegt. Die Erfassung kann text- und formularbasiert erfolgen. Die Oberflächen des Systems können den jeweiligen Bedürfnissen der Projekte angepasst werden, wobei die Form der Wissensrepräsentation und die Wiederverwendung der Daten gattungs- und disziplinübergreifend ermöglicht wird. Darüber hinaus können digitale Bild-, Text- und Audiodateien angezeigt und verwaltet werden. Zudem unterstützt WissKI die Erstellung lokaler Vokabulare und die Nutzung bestehender Normdaten.
            </p>
         <p>3.1 Anwendungsbeispiel Projekt „Friedensrepräsentationen“</p>
         <p>Das zentrale Anliegen des Projektes zur Analyse der Friedensrepräsentationen ist ein transdisziplinärer und vergleichender Forschungsansatz basierend auf einer kooperativen Erschließung und Nutzung heterogener Quellenbestände. Angaben zu den Objekten und ihren Inhalten müssen ebenso wie historische Daten zu Friedensereignissen erfasst werden. Diese unterschiedlichen Informationen sollen semantisch vernetzt sein, um eine langfristige und nachhaltige Interpretierbarkeit sicher zu stellen. Eine Herausforderung ist es, spezifische Daten und Anforderungen unterschiedlicher Fachdisziplinen zu vereinheitlichen und Schnittpunkte zu bilden. Das CIDOC CRM erlaubt durch die Definition geeigneter übergeordneter Abstraktionen und Relationen ein Erkennen und Kommunizieren gleicher Konzepte und dadurch eine disziplinunabhängige semantische Vernetzung der Informationen. Durch die semantische Modellierung in Form von sog. Pfaden ist eine nachhaltige Interpretierbarkeit der Zusammenhänge von unterschiedlichen Informationen möglich, die für die inhaltliche Erschließung der Quellenbestände von Bedeutung ist. Die Pfade wiederum sind netzwerkartig miteinander verbunden. So kann z. B. nachvollzogen werden, in welchem Verhältnis eine Person zu einem Friedensereignis oder zu einem Objekt steht, beides kann für die Forschungsfragen nach Funktion des jeweiligen Quelleninhaltes von Interesse sein.</p>
         <p>Für die Veröffentlichung der Ergebnisse in einem virtuellen Themenportal können zur besseren Strukturierung und auch um Abhängigkeiten darzustellen, Informationen hierarchisch in Beziehung gesetzt werden, wie Friedensschlüsse und auf ihnen basierende Anlässe oder Allegorien zu übergeordneten Bildtopoi. Auf allen hierarchischen Stufen bleiben die entsprechenden Eigenschaften und Relationen der entsprechenden Klassen erhalten und können demzufolge immer mit abgebildet und abgefragt werden.</p>
         <p>Die unterschiedlichen Informationen werden in spezifisch modellierten Masken erfasst, in deren Feldern Normdaten und Vokabulare hinterlegt sind. Durch verschiedene systemimmanente Eigenschaften können Wissenschaftler sehr schnell in einer Objektmaske zugehörige Dokumente und Objekte angezeigt bekommen. Für den Benutzer dient dies bei ca. 2000 angestrebten Einträgen der Übersichtlichkeit, so dass auch auf dieser Ebene die Vernetzung sichtbar sein wird.</p>
         <p>3.2 Anwendungsbeispiel Projekt „MUSICES“</p>
         <p>WissKI dient dem Projekt als Datenbank für die zu untersuchenden Musikinstrumente und als Kommunikationsplattform. Darüber hinaus ist das System in der Lage, den kompletten Untersuchungsablauf sowie die Messergebnisse und die erzeugten 3D-Daten jedes einzelnen Objekts, zusammengefasst das im Standard enthaltene Dokumentationsschema und das Netzwerk der Metadaten, abzubilden. Die zu erfassenden Metadaten beinhalten nicht nur die objektbezogenen des kulturwissenschaftlichen Bereichs, sondern auch die vom Fraunhofer Institut zu dokumentierenden Messparameter, wie die Röntgenspannung, die applizierte Strahlungsdosis, aber auch Informationen zu den CT-Anlagen. Für die Erfassung der Projektdaten in einem gemeinsamen Kontext wurde eine Anwendungsontologie, basierend auf dem CIDOC CRM entwickelt, die ebenfalls Teil des im Projekt zu entwickelnden Standards ist. Durch eine klare Definition der Metadaten, die sich auch in der Modellierung der Pfadstrukturen niederschlägt, entsteht eine Datenstruktur, die eine weitere Nutzbarkeit und Interpretierbarkeit der Projektergebnisse gewährleistet.</p>
         <p>Durch die Verwendung des CIDOC CRM können die Metadaten in das museumsinterne Objektdokumentationssystem und darüber hinaus in internationale Portale integriert werden. Im Rahmen des EU-Projekts MIMO
                <ref n=""7"" target=""vortrag-GROSSftn7"" type=""note"">7</ref> konnte mit MIMO-LIDO ein Metadatenmodell für Musikinstrumente entwickelt werden, das die Grunddatenerfassung und die Zuordnung zu Sammlungskontexten standardisiert. Das Metadatenmodell für die 3D-CT-Aufnahmen des MUSICES-Projekts wird in MIMO-LIDO integriert, steht darüber hinaus aber auch als eigenständige Domänenontologie zur Verfügung. Für den Bereich der Erforschung von Musikinstrumenten und ihrer künftigen Erfassung, insbesondere im Hinblick auf 3D-CT-Maßnahmen, wird das MUSICES-Projekt Wegbereiter für einen Standard sein, der auf verschiedenen bestehenden Standards des kulturellen Bereichs aufbaut und diese für einen spezifischen Anwendungsfall ergänzt. Durch die Publikation mit WissKI und internationalen Portalen kann garantiert werden, dass die Projektdaten verfügbar und zitierbar sind. 
            </p>
         <p>In beiden Forschungsprojekten, obgleich ihrer unterschiedlichen Disziplinen und Objektgattungen, kann durch Anwendung des CIDOC CRM eine nachhaltige Interpretierbarkeit und Austauschbarkeit der in den Projekten erhobenen Daten am GNM gewährleistet werden. In Verbindung mit WissKI sind alle anwendungsspezifischen Anforderungen abgedeckt. Durch seine Systemarchitektur ist WissKI flexibel genug, auch auf sich während der Projektlaufzeit neu ergebende Forschungsfragen zu reagieren.</p>
      </body>
      <back>
         <div type=""Notes"">
            <note n=""1"" rend=""footnote text"" id=""vortrag-GROSSftn1"">MUSICES: Sebastian Kirsch1, Frank Bär1, Theobald Fuchs2, Christian Kretzer2, Markus Raquet1, Gabriele Scholz2, Rebecca Wagner2, Meike Wolters-Rosbach1; 1 Germanisches Nationalmuseum, Nürnberg; 2 Fraunhofer-Entwicklungszentrum Röntgentechnik EZRT, Fürth</note>
            <note n=""2"" rend=""footnote text"" id=""vortrag-GROSSftn2""> Das Leibniz-Institut für Europäische Geschichte, Mainz, untersucht Friedenspredigten, die Herzog August Bibliothek, Wolfenbüttel, Dichtungen und Festschriften, das Germanische Nationalmuseum Objekte aus den graphischen und numismatischen Sammlungen, das Deutsche Historische Institut, Rom, Kantaten, Oratorien und Festmusiken vor allem in Bezug auf Italien und das Tadeusz Manteuffel Institut für Geschichte der Polnischen Akademie der Wissenschaften, Warschau, die Friedensrepräsentationen in den östlichen Gebieten Europas.</note>
            <note n=""3"" rend=""footnote text"" id=""vortrag-GROSSftn3""> Diese Ontologie wurde vom International Committee for Documentation (CIDOC) als Teil des International Council of Museums (ICOM) erstellt (URL: 
                        <ref target=""http://www.cidoc-crm.org/"">http://www.cidoc-crm.org/</ref>), wobei das Germanische Nationalmuseum federführend beteiligt war.
                    </note>
            <note n=""4"" rend=""footnote text"" id=""vortrag-GROSSftn4""> URL: 
                        <ref target=""http://erlangen-crm.org"">http://erlangen-crm.org</ref>
               <hi rend=""Hyperlink"">/</hi> (25.08.2016).
                    </note>
            <note n=""5"" rend=""footnote text"" id=""vortrag-GROSSftn5""> OWL= Web Ontology Language, vgl. URL: 
                        <ref target=""https://www.w3.org/TR/owl2-overview/"">https://www.w3.org/TR/owl2-overview/</ref> (25.08.2016).
                    </note>
            <note n=""6"" rend=""footnote text"" id=""vortrag-GROSSftn6""> WissKI = Wissenschaftliche Kommunikations-Infrastruktur, URL: 
                        <ref target=""http://wiss-ki.eu"">http://wiss-ki.eu</ref>
               <hi rend=""Hyperlink"">/</hi>) basierend auf dem Open-Source Content Management System Drupal (URL: 
                        <ref target=""http://drupal.org"">http://drupal.org</ref>
               <hi rend=""Hyperlink"">/</hi>), und wurde in Zusammenarbeit zwischen dem Germanischen Nationalmuseum, Nürnberg, dem Zoologischen Forschungsmuseum Alexander Koenig, Bonn und der Friedrich-Alexander-Universität Erlangen-Nürnberg entwickelt.
                    </note>
            <note n=""7"" rend=""footnote text"" id=""vortrag-GROSSftn7""> Musical Instrument Museums Online (URL: 
                        <ref target=""http://www.mimo-international.com"">www.mimo-international.com</ref>.). Während der Projektlaufzeit 2009 bis 2011 wurden rund 50.000 Musikinstrumente in öffentlichen Sammlungen digitalisiert und über MIMO-DB zugänglich gemacht (URL: 
                        <ref target=""http://www.mimo-db.eu/"">http://www.mimo-db.eu/</ref> (25.8.2016).
                    </note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Doerr, Martin / Lampe, Karl-Heinz / Krause, Siegfried</hi> (2011): 
                        <hi rend=""italic"">Definition des CIDOC Conceptual Reference Model Version 5.0.1</hi>.
                        autor. durch die CIDOC CRM Special Interest Group (SIG) (= Beiträge zur Museologie 1). 
                        Berlin: ICOM Deutschland.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Görz, Günther</hi> (2011): 
                        „WissKI: Semantische Annotation, Wissensverarbeitung und Wissenschaftskommunikation in einer virtuellen Forschungsumgebung“,
                        in:
                        <hi rend=""italic"">Kunstgeschichte. Open Peer Reviewed Journal</hi> urn:nbn:de:bvb:355-kuge-167-7 [letzter Zugriff 22. November 2016].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Hohmann, Georg / Fichtner, Mark</hi> (2015): 
                        „Chancen und Herausforderungen in der praktischen Anwendung von Ontologien für das Kulturerbe“, 
                        in: Robertson – von Trotta, Caroline Y. / Schneider, Ralf Y. (eds.): 
                        <hi rend=""italic"">Digitales Kulturerbe</hi>. Bewahrung und Zugänglichkeit in der wissenschaftlichen Praxis. (= Kulturelle Überlieferung – digital 2). 
                        Karlsruhe: KIT Scientific Publishing 115-128.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Stein, Regine / Gottschewski, Jürgen / Heuchert, Regina / Ermert, Axel / Hagedorn-Saupe, Monika / Hansen, Hans-Jürgen / Saro, Carlos / Scheffel, Regine / Schulte-Dornberg, Gisela</hi> (2005): 
                        <hi rend=""italic"">Das CIDOC Conceptual Reference Model. Eine Hilfe für den Datenaustausch?</hi> (= Mitteilungen und berichte aus dem Institut für Museumskunde 31). 
                        Berlin: Institut für Museumskunde.
                    </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,cidoc crm;ontologie;virtuelle forschungsumgebung;wisski,German,3d-bilder;artefakte;beziehungsanalyse;bilder;bilderfassung;datei;daten;forschung;forschungsergebnis;forschungsprozess;infrastruktur;inhaltsanalyse;kollaboration;methoden;organisation;projekte;projektmanagement;sammlung;standards;veröffentlichung;virtuelle forschungsumgebungen;visualisierung
10715,2018 - University of Cologne,University of Cologne,Kritik der digitalen vernunft,2018,DHd,DHd,,Cologne,,Germany,https://dhd2018.uni-koeln.de/,Von Drupal 8 zur virtuellen Forschungsumgebung - Der WissKI-Ansatz,,Mark Fichtner,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <p>Im Rahmen des von der DFG finanzierten Projekts “WissKI” entstand in zwei Projektphasen eine digitale Forschungsumgebung für die Anwendung im Bereich der Digital Humanities. Mit dem Ende der zweiten Projektphase 2017 wurde die Forschungsumgebung grundlegend aktualisiert und setzt nun auf das Open Source Content Management System Drupal 8 auf. Damit ging eine Aktualisierung der gesamten zugrundeliegenden Frameworks und Technologien (php 7, SPARQL 1.1) einher. Die aktuelle Version der Forschungsumgebung steht nun der wissenschaftlichen Öffentlichkeit als Open Source zur freien Verfügung.</p>
         <p>Auch die aktuelle Fassung der Software setzt auf die bewährten Kernaspekte: Die Datenerfassung und -haltung in WissKI wird zentral bestimmt durch die semantischen Zusammenhänge zwischen einzelnen Fakten und Datensätzen. Dies wird durch umfassende Unterstützung aktueller Semantic Web Technologien erreicht. Die Einordnung und Speicherung der erhobenen Daten erfolgt auf Grundlage einer Domänenontologie, deren Konzepte und Relationen - zu sogenannten Pfaden verbunden - als Vorlage für die Masken und Felder im System dienen. Auf Basis dieser Technologie werden solitär erscheinende Daten zu einem gemeinsamen, semantischen Netzwerk verbunden und damit die unmittelbare Sichtbarkeit weiterer, tiefergehender Zusammenhänge ermöglicht. Hierdurch werden intuitiv Zusammenhänge in den Daten sichtbar, die sich für den Nutzer als Mehrwert anbieten. Das Webbasierte Systemdesign und der dadurch ermöglichte Zugriff über das Internet, die Anbindung von externen kuratierten Datenquellen (sog. Authority Files) und die Möglichkeit zur Bereitstellung ausgewählter Daten über gängige Online-Schnittstellen (Web-Frontend, SPARQL-Endpoint, ...) betonen den Semantic-Web-Gedanken hinter der Infrastruktur. Die Speicherung der Daten erfolgt in einem TripleStore, der die eingegebenen Fakten in einer Subjekt-Prädikat-Objekt-Satzform ablegt. Die Aneinanderreihung der hier verwendeten Prädikate zu Pfaden erfolgt im Kern des Systems, dem sogenannten Pathbuilder, mit dem die semantische Bedeutung der einzelnen Einträge in Bezug auf das beschriebene Objekt (auch Person, Ort o. Ä.) anhand der Ontologie festgelegt wird. Die Eingabe der Daten erfolgt über eine, mit den gängigen Datenbankoberflächen vergleichbare, Editier-Oberfläche. Sie ist aus Feldern aufgebaut, die wiederum je einem bestimmten Feldtyp zugeordnet sind. Feldtypen bestimmen die Ein- und Ausgabemodalitäten der Daten.</p>
         <p>Dabei verzichtet die Software nicht auf die aus dem Bereich der Content Management Systeme bekannten Funktionalitäten wie z. B. die Generierung von Websites, Foren, Wikis oder auch die detaillierte Verwaltung der Nutzer und ihrer Zugriffsrechte. Inzwischen ist die Software in verschiedenen Forschungsprojekten an unterschiedlichen, namhaften Institutionen im kunst- und kulturhistorischen, sowie biologischen und technischen Bereich erfolgreich im Einsatz. Als Domänenontologie im Museums- und Sammlungsbetrieb kommen individuelle Erweiterungen des “Conceptual Reference Model” des Comité international pour la documentation zum Einsatz (CIDOC-CRM: ISO 21127), dessen Umsetzung in der Web-Ontology-Language OWL ebenfalls vom Projekt besorgt wurde und über die Website http://erlangen-crm.org frei zur Verfügung steht. </p>
         <p>Das Poster stellt den aktuellen Stand der WissKI-Software nach Vollendung der beiden Projektphasen dar. Neben dem bewährten Modell der Anpassung der Software durch die beiden am DFG-Projekt beteiligten Museen und der Friedrich-Alexander-Universität Erlangen-Nürnberg unterstützt die Interessengemeinschaft für semantische Datenverarbeitung e.V. (http://www.igsd-ev.de/) die gemeinnützigen Aspekte der Software weiter. Darüber hinaus werden bewusst auch Dritte zum Einsatz und zur Anpassung von WissKI eingeladen. Daraus resultierte im vergangenen Jahr der zahlreiche Einsatz der Software in Forschungsprojekten z.B. in Kooperation mit der Landesstelle der Nichtstaatlichen Museen in Bayern oder dem Zentralinstitut für Kunstgeschichte. Das System stellte v.a. durch die Nutzung aller Drupal-Basis-Funktionalitäten wie z.B. „Views“ seine Stärken unter Beweis. So können neben den altbewährten Textfeldern und -bereichen und Bildern (incl. Zoomviewer für sehr hochauflösende Bilder) auch interaktive Landkarten, 3D-Animationen, Zeitstrahlen und alle denkbaren Medientypen, sowohl zur direkten Ansicht als auch zum Download als Funktionalität genutzt werden. Zusätzlich zu diesen gängigen Formaten ermöglicht die Standardkonformität von WissKI-D8 auch die Einbindung anderer, gängiger Feldtypmodule, die für Drupal 8 zur Verfügung stehen. Zu den erwähnten Erleichterungen zählt ebenso ein Update des System-Kerns, dem Pathbuilder, mit dem die Pfadschablonen durch die Domänenontologie auf einer graphischen Oberfläche ausgewählt bzw. erzeugt werden können. Daneben wird eine umfassende Bibliothek mit Musterontologien, -masken und -pfaden bereitgestellt, die die Einstiegshürde für Erstbenutzer minimal zu halten.</p>
      </body>
   </text>

",xml,Creative Commons Attribution 4.0 International,,cidoc crm;drupal;ontologie;owl dl;wisski,German,kollaboration;metadaten;modellierung;organisation;virtuelle forschungsumgebungen;visualisierung
10810,2018 - University of Cologne,University of Cologne,Kritik der digitalen vernunft,2018,DHd,DHd,,Cologne,,Germany,https://dhd2018.uni-koeln.de/,Digitale Sammlungserschließung mit WissKI und CIDOC CRM,,Martin Scholz;Sarah Wagner,workshop / tutorial,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <p>
            <anchor id=""id_docs-internal-guid-5392ef94-b94a-0f4d-c8c7-b6a568b87aa3""/>Die systematische Erfassung und wissenschaftliche Erschließung einer Sammlung sind grundlegende Voraussetzungen, um ihr wissenschaftliches Potential sichtbar zu machen. Häufig aber fehlen Software-Lösungen und Know-How für eine flächendeckende Digitalisierung und Online-Präsenz.
            </p>
         <p>Dieser Workshop führt anhand praktischer Beispiele in die digitale Sammlungsarbeit mit WissKI und in die Modellierung mit dem CIDOC Conceptual Reference Model (CRM) ein. Durch die praktische Arbeit lernen die Teilnehmer die im Projekt „Objekte im Netz"" bereitgestellte Modellierung sowie die Konfiguration der Virtuellen Forschungsumgebung (VFU) für universitäre Sammlungen kennen.</p>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Erschließung und Digitalisierung von Sammlungen</head>
            <p>Neben Museen beherbergen auch Universitäten einen großen Schatz an Sammlungen, die der Wissenschaftsrat 2011 „als wertvolle Infrastruktur für [...] Forschung” mit „beachtliche[m] wissenschaftliche[n] Potential” identifiziert hat.
                    <ref target=""ftn1"" n=""1""/> Allein in Deutschland existieren rund 1000 Sammlungen an über 80 Universitäten.
                    <ref target=""ftn2"" n=""2""/> Zwar sind darunter auch renommierte Sammlungen, doch leidet das Gros an unzureichender Erschließung, Sichtbarkeit, Betreuung, Pflege oder Unterbringung.
                    <ref target=""ftn3"" n=""3""/> Auch bei der Digitalisierung gibt es enormen Aufholbedarf: Lediglich ein Drittel der Sammlungen sind digital zugänglich. Grund dafür sind u.a. auch das Fehlen von Software-Lösungen und Know-how für eine flächendeckende Digitalisierung und Online-Präsenz.
                </p>
            <p>Seit einigen Jahren gibt es vermehrt Anstrengungen, universitäre Sammlungen aus ihrem Dornröschenschlaf zu wecken und sie zu einer wichtigen Ergänzung objektgebundener Forschung und Lehre weiter zu entwickeln. Dies drückt sich unter anderem in deutschlandweiten Förderprogrammen aus, wie etwa der „Allianz für universitäre Sammlungen” des Bundesministeriums für Bildung und Forschung. Das darin geförderte Projekt „Objekte im Netz”
                    <ref target=""ftn4"" n=""4""/> konzentriert sich auf die Digitalisierung universitärer Sammlungen und entwickelt in einer Kooperation zwischen der Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) und dem Germanischen Nationalmuseum Nürnberg (GNM) eine gemeinsame Erschließungs- und Digitalisierungsstrategie für die Sammlungen der FAU, um die wissenschaftliche Nutzbarkeit der reichhaltigen Bestände zu verbessern.
                </p>
            <p>Im Fokus stehen jedoch nicht einzelne Sammlungen oder Fachbereiche, sondern die Bereitstellung von Software-Werkzeugen und Lösungswegen, um die digitale Erschließung und Verfügbarkeit an breiter Front voranzutreiben. Die über 20 Sammlungen der FAU bilden dabei eine äußerst heterogene Entwicklungs- und Testlandschaft, um Lösungen zu erarbeiten, die über die FAU hinaus anwendbar sind. Die nötige Generizität der Ansätze und die Nachhaltigkeit sind daher zentrale Herausforderungen, wobei bei letzterem die langfristige Interpretierbarkeit der Daten im Blickpunkt des Projekts steht. Daneben müssen die meist knappen personellen und finanziellen Mittel berücksichtigt werden.</p>
            <p>Als besonders geeignet zur Umsetzung der Ziele erscheinen auf technischer Seite Lösungen, die unter freien Lizenzen (Open Source) zur Verfügung stehen und die Ideen des Semantic Web implementieren: Flexible Wissensnetze mit klar definierter Semantik, die weltweit – und damit auch sammlungsübergreifend – verknüpft werden können. Das Projekt erweitert daher die virtuelle Forschungs- und Dokumentationsumgebung WissKI
                    <ref target=""ftn5"" n=""5""/> zu einem Werkzeug für die digitale Sammlungserschließung und stellt auf verschiedene Sammlungsbereiche abgestimmte Konfigurationen der Software sowie Leitfäden zur Verfügung. Für die standardisierte semantische Auszeichnung der Daten kommt das CIDOC Conceptual Reference Model (CRM) zum Einsatz.
                </p>
            <p>Aufgrund einer erfolgreichen Pilotstudie kann das Projekt bereits auf erste Ergebnisse verweisen. Im Rahmen des Workshops wird die bereits publizierte generische Konfiguration vorgestellt und von den Teilnehmern angewandt.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Die Werkzeuge - WissKI und CIDOC CRM</head>
            <p>Semantische Technologien - im Speziellen Semantic Web und Linked Open Data - erfreuen sich zunehmender Beliebtheit in den Digital Humanities. Für objektbasierte Forschung bieten die flexible, netzwerkartige Grundstruktur des Resource Description Framework (RDF) und darauf aufbauende Formate ein adäquates Mittel zur Repräsentation, Verwaltung und Publikation von (Meta-)Daten. Zahlreiche VFUs unterstützen das Erstellen von komplexen Wissensnetzen und deren Export in Tripelformaten. Wichtige Normdateien und Thesauri stehen als Linked Open Data zur Verfügung. Ontologien wie das CIDOC CRM bilden das semantische Rückgrat dieses Ansatzes und garantieren ein Mindestmaß an Interoperabilität und Datenaustausch, das über das klassische Verlinken von Web-Dokumenten hinausgeht.</p>
            <p>
               <anchor id=""id_docs-internal-guid-877475c3-b944-4e7b-8d03-43f7c56cc8bd2""/>Wenngleich die Nutzung semantischer Technologien zunimmt, stellt der praktische Einsatz unerfahrene 
                    <anchor id=""id_docs-internal-guid-877475c3-b944-4e7b-8d03-43f7c56cc8bd1""/>oder wenig technikaffine 
                    <anchor id=""id_docs-internal-guid-877475c3-b944-4e7b-8d03-43f7c56cc8bd""/>Nutzer meist vor große Herausforderungen. Dies gilt weniger für die Beherrschung bestimmter Formate und Werkzeuge als vielmehr für die semantische Modellierung der Daten, d.h. die Erstellung von und den richtigen Umgang mit Ontologien. Da hierbei die Bedeutung der Daten formalisiert niedergelegt wird, ist mitunter ein gehöriges Maß an Wissen über einen Anwendungs-/Fachbereich erforderlich, um Modellierungsfehler zu vermeiden und so eine spätere korrekte Interpretation zu gewährleisten. Insbesondere das CIDOC CRM
                    <ref target=""ftn6"" n=""6""/>, das eine Top-Level-Ontologie für die Dokumentation kulturellen Erbes darstellt, steht immer wieder in der Kritik, für Einsteiger zu komplex zu sein.
                </p>
            <p>Die virtuelle Forschungs- und Dokumentationsumgebung WissKI nimmt sich dieser Herausforderung an. Die browserbasierte Software ist das Produkt aus zwei DFG-geförderten Projekten und entstand aus Anforderungen an die kooperative Forschung in Museen bzw. im Bereich des Kulturerbes und seiner Dokumentation im digitalen Medium. Zentraler Fokus von WissKI ist das vernetzte Arbeiten auf Basis semantischer Tiefenerschließung von Forschungsdaten. Eine Schlüsselrolle kommt hierbei dem CIDOC CRM zu, das um projektspezifische Anwendungsontologien erweitert werden kann.</p>
            <p>Aus Nutzersicht ist das System an die tradierten Formen der Datenakquise und -präsentation angelehnt. Die Daten werden jedoch semantisch aufbereitet und nativ als RDF mitsamt Ontologie-Konstrukten gespeichert. Dem Nutzer werden so die Vorteile von Linked Open Data und Semantic Web zugänglich, ohne dass dieser sich mit technischen und ontologischen Details auseinandersetzen muss. Kern dieses Ansatzes ist eine Abbildung zwischen den tradierten, meist datensatz-basierten, tabellarischen Darstellungen und der graphbasierten Wissensrepräsentation, die die ontologiegestützte, formale Semantik der verwendeten Datenfelder beinhaltet. Diese Abbildung wird von einem inhaltlichen Administrator festgelegt und ist für die Nutzer standardmäßig nicht sichtbar. Die formale Semantik muss also nicht verstanden werden, um das System effektiv zu nutzen. Abbildungen oder Teile davon können zwischen verschiedenen Systemen wiederverwendet und erweitert werden, so dass sich Best-Practice-Modellierungen herausbilden.</p>
            <p>Die Open-Source-Lizenzierung aller in diesem Workshop verwendeten Werkzeuge und Standards ist ein wichtiger Aspekt. Die kostenfreie Nutzung trägt zum einen der häufig angespannten finanziellen Situation universitärer Sammlungen Rechnung und ist zum anderen Bestandteil des partizipativen Konzepts: Anwender können die Materialien nutzen, sie an ihre Bedürfnisse anpassen und wiederum der Community zur Verfügung stellen.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Zielgruppe sowie Inhalt und Ziele des Workshops</head>
            <p>Der Workshop richtet sich an alle, die mit Sammlungsobjekten oder mit Objekten des kulturellen Erbes im Allgemeinen arbeiten und diese digital dokumentieren oder erschließen. Auch spricht der Workshop interessierte Wissenschaftler an, die Objekte standardisiert dokumentieren und ihre Metadaten semantisch aufbereiten möchten. Es werden von den Teilnehmern keine Vorkenntnisse für die VFU WissKI oder das CIDOC CRM vorausgesetzt. </p>
            <p>Der Workshop zeigt anhand praktischer Beispiele, wie Erfassungsschemata und -modi aus der universitären Sammlungslandschaft mithilfe der Referenzontologie CIDOC CRM und der VFU WissKI auf Objekte universitärer Sammlungen bzw. des kulturellen Erbes im Allgemeinen umgesetzt werden können. </p>
            <p>Während des Workshops arbeiten die Teilnehmer mit ihrem eigenen WissKI-System, wahlweise einzeln oder in Kleingruppen. Dabei stehen weniger die informationstechnischen Details der Werkzeuge im Vordergrund. Vielmehr werden die nötigen Schritte bis zum effektiv einsetzbaren System vermittelt und durchgeführt. Angefangen bei der Installation und einigen grundlegenden Funktionalitäten, binden die Teilnehmer die vom Projekt „Objekte im Netz” angebotene Konfiguration zur Sammlungserschließung in WissKI ein und erhalten somit ein einsetzbares System mit standardisierten Eingabe- und Anzeigemöglichkeiten. Darauf aufbauend werden Möglichkeiten der einfachen Anpassung der semantischen Modellierung aufgezeigt und selbständig geübt. Das Erfassen von (selbst mitgebrachten) Datensätzen rundet die praktische Einführung ab.</p>
            <p>Neben einer allgemeinen Einführung in das Arbeiten mit WissKI und der semantischen Dokumentation von Daten sind die Teilnehmer nach dem Workshop in der Lage, einfache Erfassungsmasken zu modellieren, Daten mit WissKI zu erfassen und zu recherchieren.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Kurzbiographien</head>
            <p>Martin Scholz ist einer der Hauptentwickler der Virtuellen Forschungsumgebung WissKI. Er studierte Informatik und Sinologie an der Friedrich-Alexander-Universität Erlangen-Nürnberg. Nach seinem Diplom 2008 arbeitete er für die Arbeitsgemeinschaft Digital Humanities der FAU für das DFG-geförderte Projekt “Wissenschaftliche KommunikationsInfrastruktur” (WissKI). Seit 2017 engagiert er sich für die Digitalisierung der Sammlungen der FAU im Rahmen des BMBF-geförderten Projekts “Objekte im Netz”. Seine Forschungsinteressen liegen in den Digital Humanities, insbesondere in den Bereichen Wissensrepräsentation, Semantic Web und Verarbeitung natürlicher Sprache.</p>
            <p>Martin Scholz</p>
            <p>Friedrich-Alexander-Universität</p>
            <p>Erlangen-Nürnberg</p>
            <p>Referat H2 – Zentralkustodie</p>
            <p>Hugenottenplatz 1a, 91054 Erlangen</p>
            <p>martin.scholz@fau.de</p>
            <p>Sarah Wagner ist Kunsthistorikerin und arbeitet seit 2012 in der Abteilung für Kulturinformatik am Germanischen Nationalmuseum Nürnberg. Sie studierte Kunstgeschichte und Museumsarbeit in Bamberg, Erlangen und Leiden (NL) und betreut seit 2014 verschiedene Forschungsprojekte, die mit WissKI arbeiten. Aktuell ist sie für das BMBF-geförderte Kooperationsprojekt “Objekte im Netz” tätig und vertritt dort die Seite des Museums. Ihre Forschungsschwerpunkte liegen in der frühneuzeitlichen Sammlungspraxis und der semantischen Wissensmodellierung.</p>
            <p>Sarah Wagner</p>
            <p>Germanisches Nationalmuseum Nürnberg</p>
            <p>Kornmarkt 1, 90402 Nürnberg</p>
            <p>s.wagner@gnm.de</p>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note id=""ftn1"" n=""1"" place=""foot""> Vgl. „Empfehlungen zu wissenschaftlichen Sammlungen als Forschungsinfrastrukturen”, Verfügbar unter: 
                        <ptr target=""https://www.wissenschaftsrat.de/download/archiv/10464-11.pdf""/>) [letzter Zugriff: 10. Januar 2018]
                    </note>
            <note id=""ftn2"" n=""2"" place=""foot""> Kennzahlen zu den folgenden Aussagen sind verfügbar unter: 
                        <ptr target=""https://portal.wissenschaftliche-sammlungen.de/kennzahlen""/>/ [letzter Zugriff: 10. Januar 2018]
                    </note>
            <note id=""ftn3"" n=""3"" place=""foot"">
               <anchor id=""id_docs-internal-guid-5392ef94-b962-c408-911e-ec8242784d66""/> siehe Fußnote 1
                    </note>
            <note id=""ftn4"" n=""4"" place=""foot"">
               <anchor id=""id_docs-internal-guid-5392ef94-b963-dc0a-8780-317aedc2ff32""/>Das Projekt wird vom Bundesministerium für Bildung und Forschung von 2017 bis 2020 im Rahmen der Förderlinie „Vernetzen - Erschließen - Forschen. Allianz für universitäre Sammlungen” gefördert. Mehr Informationen unter URL: 
                        <ptr target=""http://objekte-im-netz.fau.de/""/> [letzter Zugriff: 10. Januar 2018]
                    </note>
            <note id=""ftn5"" n=""5"" place=""foot""> WissKI (=„Wissenschaftliche KommunikationsInfrastruktur, URL: 
                        <ref target=""file:///h"">http://wiss-ki.eu/</ref>[letzter Zugriff: 10. Januar 2018]) basiert auf dem Open-Source Content Management System Drupal (URL: 
                        <ref target=""file:///h"">http://drupal.org/</ref>[letzter Zugriff: 10. Januar 2018]) und wurde in Zusammenarbeit zwischen dem Germanischen Nationalmuseum, Nürnberg, dem Zoologischen Forschungsmuseum Alexander Koenig, Bonn und der Friedrich-Alexander- Universität Erlangen-Nürnberg entwickelt.
                    </note>
            <note id=""ftn6"" n=""6"" place=""foot"">
               <anchor id=""id_docs-internal-guid-5392ef94-b964-6072-0052-3285a497835b""/>Das CIDOC CRM wurde vom International Committee for Documentation als Teil des International Council of Museums (ICOM) als formale Referenzontologie erarbeitet und ist seit 2006 als ISO Norm (ISO 21127) anerkannt. In der „Erlangen CRM“ (URL: 
                        <ptr target=""http://erlangen-crm.org/""/> [letzter Zugriff: 10. Januar 2018]) auf Basis der Web Ontology Language (OWL) liegt eine maschinenlesbare Version vor. Weitere Informationen zum CIDOC CRM unter URL: 
                        <ptr target=""http://cidoc-crm.org/""/> [letzter Zugriff: 10. Januar 2018].
                    </note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""italic"">Definition of the CIDOC Conceptual Reference Model: Version 5.0.4.</hi>, autor. durch die CIDOC CMR Special Interest Group (SIG), 2011.
                      <ptr target=""http://www.cidoc-crm.org/sites/default/files/cidoc_crm_version_5.0.4.pdf""/>) [letzter Zugriff 25.09.2017].
                  </bibl>
               <bibl>Görz, Günther: „WissKI: Semantische Annotation, Wissensverarbeitung und Wissenschaftskommunikation in einer virtuellen Forschungsumgebung” in: Kunstgeschichte, Open Peer Reviewed Journal, urn:nbn:de:bvb:355-kuge-167-7 [letzter Zugriff 10.01.2018].</bibl>
               <bibl>Hohmann, Georg (2011): „Die Anwendung von Ontologien zur Wissensrepräsentation und -kommunikation im Bereich des Kulturellen Erbes” in: Schomburg, Silke u.a. (eds.): Digitale Wissenschaft. Stand und Entwicklung digital vernetzter Forschung in Deutschland. Köln: Hochschulbibliothekszentrum NRW 33-39.</bibl>
               <bibl>Hohmann, Georg / Schiemann, Bernhard (2013): „An Ontology-Based Communication System for Cultural Heritage. Approach and Progress of the WissKI Project” in: Hans Bock u.a. (eds.): Scientific Computing and Cultural Heritage. Berlin: Springer 127-135.</bibl>
               <bibl>Hohmann, Georg/Fichtner, Mark. Chancen und Herausforderungen in der praktischen Anwendung von Ontologien für das Kulturerbe. In: Digitales Kulturerbe. Bewahrung und Zugänglichkeit in der wissenschaftlichen Praxis. Vol. Kulturelle Überlieferung – digital. Karlsruhe 2015. S. 115-128.</bibl>
               <bibl>Wissenschaftsrat (2011): 
                      <hi rend=""italic"">Empfehlungen zu wissenschaftlichen Sammlungen als Forschungsinfrastrukturen</hi>. Berlin 
                      <ptr target=""https://www.wissenschaftsrat.de/download/archiv/10464-11.pdf""/>) [letzter Zugriff 25.09.2017].
                  </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,cidoc crm;digitalisierung;objekte im netz;sammlungserschließung;wisski,German,artefakte;kollaboration;modellierung;sammlung;virtuelle forschungsumgebungen
10998,2020 - University of Paderborn,University of Paderborn,Digital Humanities zwischen Modellierung und Interpretation,2020,DHd,DHd,Universität Paderborn,Paderborn,,Germany,https://zenodo.org/record/3666690,A Linked Open Data Platform for Historical Geographic Data,,Günther Görz;Chiara Seidl;Martin Thiering,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
      <body>
         <p rend=""Plain Text"">
            <hi style=""font-size:11pt"">The goal of Bibliotheca Hertziana's project ""Historical spaces in texts and maps"" is to investigate the relations between historic geographical texts and maps to reconstruct a historical understanding of space and the knowledge associated with it. Starting with a cognitive-semantic analysis of Flavio Biondo's ""Italia Illustrata"" (1474), first of all, toponyms, place descriptions and spatial relations are annotated in the text and Renaissance maps. Our contribution to Spatial Humanities is based on the conviction that all maps are cognitive maps, depicting culture-specific spatial knowledge and practices (Goerz et al., 2018, 2019).</hi>
         </p>
         <p rend=""Plain Text"">
In general, our research combines
cognitive-semantic parameters such as toponyms, landmarks, spatial
frames of reference, geometric relations, gestalt principles and
different perspectives with computational linguistic analysis
(Thiering, 2015). We designed a workflow comprising the steps of
transcription, annotation, geographic verification, export and
ontology-based semantic enrichment of these data, finally stored and
published as Linked Open Data. We use Recogito
(<ref target=""https://recogito.pelagios.org"">https://recogito.pelagios.org</ref>,
15.09.2019) as our main tool for static annotations of places and
persons/peoples in text and maps. Toponyms are georeferenced with
gazetteers, in our case primarily with Pleiades
(<ref target=""https://pleiades.stoa.org"">https://pleiades.stoa.org</ref>,
15.09.2019), and the annotations can be exported in various formats,
in particular, CSV, GeoJSON, and KML. Spatial relations in texts are
annotated in terms of the cognitive-semantic parameters by means of
the brat tool. These annotation data are mapped into triples encoding
cognitive parameters, primarily in ""figure-spatial_relation-ground""
constructions. Furthermore, dependency parsing
(<ref target=""http://ufal.mff.cuni.cz/udpipe"">http://ufal.mff.cuni.cz/udpipe</ref>, 15.09.2019) has been applied to the text for comparison. To achieve a generic semantic level for linguistic and map-related annotations, we perform a transition to an ontology-based representation. For this purpose, we defined a domain ontology 
<hi rend=""italic"" style=""font-size:11pt"">hmap</hi>
for historical maps and geographical texts based on the event-centered
CIDOC Conceptual Reference Model (CRM, ISO standard 21127) and its
spatio-temporal extension CRMgeo in OWL-DL
(<ref target=""http://erlangen-crm.org"">http://erlangen-crm.org</ref>,
15.09.2019). Using the CRM opens up a wide spectrum of
interoperability and linking to many web resources.  The domain
ontology 
<hi rend=""italic"" style=""font-size:11pt"">hmap</hi>
            <hi style=""font-size:11pt"">for the description of historical maps and their content offers a framework for the general metadata of maps and geographical texts as well as for descriptions of their content.</hi>
         </p>
         <p rend=""Plain Text"">
            <hi style=""font-size:11pt"" space=""preserve"">As Linked Data platform we chose the Virtual Research Environment WissKI (Scholz et al., 2016; </hi>
            <ref target=""http://wiss-ki.eu"">http://wiss-ki.eu</ref>, 15.09.2019),
a semantic database extension of the CMS Drupal, in which we defined
our data model in terms of so-called ontology paths. These are
sequences of triples built from entities and properties of the
ontology. As an example, in a map production event
(<hi rend=""italic"" style=""font-size:11pt"">hmap:M9_Map_Production</hi>) there is an actor, the Creator, defined by
</p>
         <code>
  hmap:M28_Map --> hmap:A3i_was_produced_by
  <lb/> -->   hmap:M9_Map_Production --> hmap:A4_carried_out_by_map_author
  --> hmap:M1_Map_Author --> ecrm:P131_is_identified_by
  <lb/>--> ecrm:E82_Actor_Appellation.
</code>
         <p rend=""Plain Text"">
For each map we may have several images, in which depicted objects are
annotated; so there is an analogous data model for images
(<hi rend=""italic"" style=""font-size:11pt"">hmap:M34_Image</hi>). What the image depicts, in our case annotated places, is specified by
</p>
         <code>
  hmap:M34_Image --> hmap:A43_depicts -->
  hmap:M3_Annotated_Place --> ecrm:P1_is_identified_by --> ecrm:E42_Identifier.
</code>
         <p rend=""Plain Text"">
  For each annotated place
  (<hi rend=""italic"" style=""font-size:11pt"">hmap:M3_Annotated_Place</hi> is a subclass of 
<hi rend=""italic"" style=""font-size:11pt"">crmgeo:SP6_Declarative_Place</hi>) where
the (geographical) contents of the annotations are encoded in the
columns of the CSV tables, each column is transformed into a component
for which similar ontology paths are defined. The annotated place is
linked to the image by 
</p>
         <code>
  hmap:M3_Annotated_Place --> hmap:A43i_is_depicted by -->
  hmap:M34_Image
  <lb/>--> ecrm:P48_has_preferred_identifier --> ecrm:E42_Identifier.
</code>
         <p rend=""Plain Text"">
            <hi style=""font-size:11pt"" space=""preserve"">So, e.g., for </hi>
            <hi rend=""italic"" style=""font-size:11pt"">QUOTE_TRANSCRIPTION</hi>, the path is
</p>
         <code>
  hmap:M3_Annotated_Place --> ecrm:P87_is_identified_by --> hmap:M42_Transcribed_Place_Appellation
</code>
         <p rend=""Plain Text"">
            <hi style=""font-size:11pt"">Each annotation, represented as a row in the table, has a unique ID (UUID) and refers, if geographically verified with a gazetteer (Pleiades), via a URL to a graph containing various information such as e.g. sources, archeological data, images, etc. In some maps, annotated places are additionally represented by a visual item (E36) such as a church, a tower, or a wall. There are also further data models for image series and works like map collections or atlases. From these paths, WissKI generates automatically input forms for map and text metadata and provides an interface for importing all table-formatted annotations and converting them into triples. Ontological enrichment of our data with CRM allows for a semantic interpretation of annotations such that, e.g., for each PlaceName, an instantiated CRM description in RDF/OWL triple format is generated and stored in a triple store. Using the semantically enriched geo-information from text (and map) annotations as CRM instances, spatial entities (""figure"", ""ground"") and relations obtained by spatial role labeling as ""figure-spatial_relation-ground"" triples can now be upgraded to this rich semantic level by linking data. Due to the fundamental underlying triple structure for all kinds of annotations, the data are immediately ready for publication as standardized Linked (Open) Data; WissKI provides a SPARQL query interface. These triple data constitute a huge knowledge graph; they are the ""raw material"" for further research steps, i.e. the exploration of the historical understanding of spaces and the associated knowledge. Interpretation of the data has just begun: Actually, a study of Biondo’s spatial language – comprising the whole text for the first time – by Berthele and Thiering is being finalized (2020) and a comparative overview of the toponyms in the Latium book with different maps (traditional and ""modern“ Ptolemaic maps, genuine maps of Italy and portolans) as well as a representation of the hodological dimension of the text is being prepared.</hi>
         </p>
      </body>
      <back>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliography</head>
               <bibl>
                  <hi rend=""bold"">Blakemore, Michael / Harley, Brian J.</hi> (1980): Concepts in the History of Cartography - A Review and Perspective. In: Cartographica. International Publications on Cartography, 17/4, Monograph 26. University of Toronto Press: Toronto.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Bodenhamer, David J. / Corrigan, John / Harris, Trevor M.</hi> (eds.) (2010): The Spatial Humanities. GIS and the Future of Humanities Scholarship. Bloomington & Indianapolis: Indiana University Press.
                    </bibl>
               <bibl>
                  <hi rend=""bold"" space=""preserve"">Görz, Günther / Geus, Klaus / Michalsky, Tanja / Thiering, Martin </hi>(2018): Spatial Cognition in Historical Geographical Texts and Maps: Towards a cognitive-semantic analysis of Flavio Biondo's 
                        <hi rend=""color(222222)"" style=""font-size:12pt"">“</hi>Italia Illustrata'', e-perimetron 13,4: 182-199.
                    </bibl>
               <bibl>
                  <hi rend=""bold"" space=""preserve"">Görz Günther / Seidl Chiara / Thiering, Martin </hi>(2019): Linked Biondo: Modelling Geographical Features in Renaissance Texts and Maps
                        <hi rend=""bold"" space=""preserve"">. </hi>In: Boutoura, Ch., Tsorlini, A., Livieratos, E. (Ed.): Proceedings 14th ICA Conference 
                        <hi rend=""italic"" space=""preserve"">Digital Approaches to Cartographic Heritage, </hi>Thessaloniki, 8-10 May 2019.International Cartographic Association, Commission on Cartographic Heritage into the Digital. AUTH CartoGeoLab, ISSN 2459-3893, 124-140. 
                    </bibl>
               <bibl>
                  <hi rend=""bold"" space=""preserve"">Michalsky, Tanja / Thiering, Martin </hi>(2020): 
                        <hi rend=""color(222222)"" style=""font-size:12pt"">Walking through history. An interdisciplinary approach to Flavio Biondo’s spaces in the “Italia illustrata”. Rome: Bibliotheca Hertziana.</hi>
               </bibl>
               <bibl>
                  <hi rend=""bold"" space=""preserve"">Scholz, Martin / Merz, Dorian / Goerz, Günther </hi>(2016): Working with WissKI - A Virtual Research Environment for Object Documentation and Object-Based Research. Digital Humanities 2016, Conference Abstracts, Krakow, 11-16 July 2016. ISBN 978-83-942760-3-4, 944-945. 
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Thiering, Martin</hi> (2015): Spatial Semiotics and Spatial Mental Models: Figure-Ground Asymmetries in Language. Berlin: De Gruyter Mouton.
                    </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,georeferenzierung;linked open data;semantische modellierung,English,annotieren;karte;modellierung;räumliche analyse;text;transkription
11761,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,How big can a static site be? Staticizing a census database :,,Martin Holmes;Greg Newton,"paper, specified ""long paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <div type=""div1"">
                <head>
                    <anchor xml:id=""id__pv5vauyqebji""/>Project Endings and static websites
                </head>
                <p>It has long been recognized that building DH web applications which do not have perpetual funding on complex computing stacks that require regular updates presents an overwhelming problem for long-term maintenance and archivability (Nowviskie and Porter 2010; Dombrowski 2019; Smithies et al. 2019). Project Endings is a collaboration between DH scholars, librarians and programmers, aiming to create tools and recommendations for building extremely low-maintenance, easily archivable, but still highly functional digital edition projects (Goddard 2018; Holmes and Takeda 2019a). Starting in 2016, the Endings team have converted a number of high-traffic, well-known digital edition projects that previously ran on XML databases and similar back-end infrastructure into entirely static websites
                    <hi rend=""sup"">
                        <note xml:id=""ftn1"" place=""foot"" n=""1"">See 
                            <ptr target=""https://endings.uvic.ca/projects.html""/> for the full list of staticized projects.
                        </note>
                    </hi> built with HTML5, CSS and client-side JavaScript (Holmes 2017; Arneil, Holmes and Newton 2019). We have also developed a set of Principles
                    <hi rend=""sup"">
                        <note xml:id=""ftn2"" place=""foot"" n=""2"">
                            <ptr target=""https://endings.uvic.ca/principles.html""/>.
                        </note>
                    </hi> to guide us in converting existing projects and creating new ones, as well as a client-side pure-JavaScript search engine, staticSearch
                    <hi rend=""sup"">
                        <note xml:id=""ftn3"" place=""foot"" n=""3"">
                            <ptr target=""https://github.com/projectEndings/staticSearch""/>.
                        </note>
                    </hi> (Holmes and Takeda 2020a, 2020b).
                </p>
                <p>At the outset, we assumed that only relatively small digital editions would be suitable candidates for complete staticization, and we began with sites consisting of only a few hundred pages.
                    <hi rend=""sup"">
                        <note xml:id=""ftn4"" place=""foot"" n=""4"">See for example 
                            <hi rend=""italic"">My Norse Digital Image Repository</hi> (
                            <ptr target=""https://myndir.uvic.ca/""/>) or 
                            <hi rend=""italic"">The Robert Graves Diary</hi> (
                            <ptr target=""https://graves.uvic.ca/""/>).
                        </note>
                    </hi> However, seeing how smoothly the process worked with smaller sites, we took on some of our larger projects, including The Map of Early Modern London (13,086 pages), The Colonial Despatches (10,826 pages), and Digital Victorian Periodical Poetry (20,685 pages). The results were very encouraging: the new sites were faster and more responsive than the old, and the staticSearch engine performed very effectively even at these scales.
                    <hi rend=""sup"">
                        <note xml:id=""ftn5"" place=""foot"" n=""5"">See 
                            <ptr target=""https://mapoflondon.uvic.ca/search.htm""/>; 
                            <ptr target=""https://bcgenesis.uvic.ca/search.html""/>; 
                            <ptr target=""https://dvpp.uvic.ca/search.html""/>.
                        </note>
                    </hi>
                </p>
                <p>But there must, presumably, be some practical limits on the scale of a DH project which can be effectively converted to the static model, and it would be helpful to know where those limits might be. We reviewed our own catalogue and discovered a candidate which appears to be precisely the kind of project that would be suitable for testing this: 
                    <hi rend=""italic"">VIHistory</hi>.
                </p>
            </div>
            <div type=""div1"">
                <head>
                    <anchor xml:id=""id__kff2sgs4fgkz""/>The 
                    <hi rend=""italic"">VIHistory</hi> project
                </head>
                <p>
                    <hi rend=""italic"">VIHistory</hi> (
                    <ptr target=""https://vihistory.uvic.ca/""/>), a PostgreSQL/PHP project created about 15 years ago, presents census data from the City of Victoria and Vancouver Island, from censuses taken in 1871, 1881, 1891, 1892, 1901, and 1911, comprising nearly 150,000 individual census records, along with associated tables of occupations, familial relationships, locations, addresses, religion, languages, nationalities and other associated concepts. In terms of the number of individual HTML pages required for a static site, it is between five and ten times the size of the largest project we have previously staticized. VIHistory has undergone successive infrastructure migrations over the years, and various features have broken as a result. A looming server migration will render it non-functional, so we must take action within a short period. Furthermore, a new dataset (an addendum to the 1901 census) is now available for addition to the collection. Rather than invest more time in patching the existing site, we are instead creating a static version, with a completion deadline of April 2022.
                </p>
                <p>Census data brings with it a range of challenges, particularly when multiple censuses are to be presented as an integrated dataset. From one census to another, the range of data collected will vary; ward boundaries change; and descriptors such as nationality, race, occupation, familial relationships and languages mutate as social and political norms evolve.
                    <hi rend=""sup"">
                        <note xml:id=""ftn6"" place=""foot"" n=""6"">See, for example, Stanger-Ross 2008, which discusses the evolution of ethnicity in census data.</note>
                    </hi> The old version of the VIHistory site addressed these issues through a set of PostgreSQL views, which merged disparate datasets into a normalized form which could be queried more easily. One of our challenges will be to accomplish this in static form.
                </p>
                <p>Another challenge will be to devise an appropriate granularity for the HTML pages which constitute the site. We expect to create a single HTML page for every distinct census entry, but other pages will be constructed to bring together collections of similar features (people with the same occupation, on the same street, with the same nationality, etc.) in order to provide a useful browsing approach to the data (something lacking in the existing site, which has only a search interface). The search itself will need to be carefully constructed so that all the features of the existing site search are preserved, and we expect to add more search options too.</p>
            </div>
            <div type=""div1"">
                <head>
                    <anchor xml:id=""id__j1euv3g3eovz""/>Plan, approach, and prospects
                </head>
                <p>We initially considered converting all the existing census data into XML as the first phase of the project, but we have not found any XML standard suitable for this historical census data.
                    We then considered converting the data directly into HTML5, but found
                    it more effective to design an intermediate custom XML schema, which
                    enables records from all the different censuses to be encoded in a
                    single flexible structure, and also permits us to apply datatype
                    constraints and catch errors. The XML is then converted into XHTML5 for
                    the website.
                </p>
                <list type=""unordered"">
                    <item>Each census entry page will present a standardized tabular view of the data from the census record, with explanations to clarify differences between census datasets.</item>
                    <item>Each “page” will have a condensed single-line title capturing essential data, used for display in search results and listings pages. </item>
                    <item>Values for datapoints such as nationality, language and so on will be constrained by the schema, and all pages will be validated during the build process. </item>
                    <item>Detailed diagnostics (Holmes and Takeda 2019b) will expose inconsistencies and errors in the dataset.</item>
                    <item>Metadata will be encoded in the HTML header and used to create search filters, but the text of the pages will also be indexed for a new full-text search feature (the old site search allows only metadata filters).</item>
                </list>
                <p>We fully expect the conversion to succeed, but we also know that we will be pushing the practical limits of this approach, and will need to devise optimizations and workarounds to make the site, and particularly the static search engine, usable. Our paper will report on this work, and present recommendations on strategies and limitations for creating static resources on this scale. </p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>Arneil, Stewart, Martin Holmes, and Greg Newton. 2019. “Clearing the Air for Maintenance and Repair: Strategies, Experiences, Full Disclosure; Paper Three: Ruthless Principles for Digital Longevity.” Presented at Digital Humanities 2019, Utrecht, the Netherlands. 
                        <ptr target=""https://dev.clariah.nl/files/dh2019/boa/0648.html""/>.
                    </bibl>
                    <bibl>Dombrowski, Quinn. 2019. “Sorry for all the Drupal: Reflections on the 3rd anniversary of ‘Drupal for Humanists.’” Quinn Dombrowski (blog), November 8, 2019.
                        <ptr target=""http://www.quinndombrowski.com/?q=blog/2019/11/08/sorry-all-drupal-reflections-3rd-anniversary-drupal-humanists""/>.
                    </bibl>
                    <bibl>Goddard, Lisa. 2018. “The Endings Project @ UVic: Concluding, Archiving, and Preserving Digital Projects for Long-Term Usability.” @Risk North 2: Digital Collections, Montreal, Canada. 
                        <ptr target=""https://github.com/projectEndings/Endings/blob/master/presentations/Goddard_RiskNorth_Endings_final.pptx?raw=true""/>.
                    </bibl>
                    <bibl>Holmes, Martin. 2017. “Selecting Technologies for Long-Term Survival.” Presented at the SHARP Conference 2017: Technologies of the Book, Victoria, BC, Canada. 
                        <ptr target=""https://github.com/projectEndings/Endings/raw/master/presentations/SHARP_2017/mdh_sharp_2017.pdf""/>.
                    </bibl>
                    <bibl>Holmes, Martin. 2021. “Using ODD for HTML.” 
                        <hi rend=""italic"">The Journal of the Text Encoding Initiative.</hi> Text Encoding Initiative Consortium. 
                        <ptr target=""https://journals.openedition.org/jtei/3106""/>.
                    </bibl>
                    <bibl>Holmes, Martin and Joseph Takeda. 2019a. “The Prefabricated Website: Who needs a server anyway?” Text Encoding Initiative Conference, Graz, Austria. 
                        <ptr target=""https://zenodo.org/record/3449197""/>.
                    </bibl>
                    <bibl>Holmes, Martin and Joseph Takeda. 2019b. “Beyond Validation: Using Programmed Diagnostics to Learn About, Monitor, and Successfully Complete Your DH Project.” 
                        <hi rend=""italic"">Digital Scholarship in the Humanities.</hi> Oxford University Press/EADH. 
                        <ptr target=""http://dx.doi.org/10.1093/llc/fqz011""/>.
                    </bibl>
                    <bibl>Holmes, Martin, and Joey Takeda. 2020a. “Static Search: An Archivable and Sustainable Search Engine for the Digital Humanities.” Presented at the Digital Humanities Summer Institute (DHSI) Colloquium (#VirtualDHSI). [
                        <ptr target=""https://zenodo.org/record/3883150""/>].
                    </bibl>
                    <bibl>Holmes, Martin, and Joey Takeda. 2020b. “Nine Projects, One Codebase: A Static Search Engine for Digital Editions.” Presented at the COLLABORATION Digital Humanities Conference, University of British Columbia / online. 
                        <ptr target=""http://dhconference.sites.olt.ubc.ca/conference-info/program/day-4/""/>.
                    </bibl>
                    <bibl>Nowviskie, Bethany, and Dot Porter. 2010. “<ref target=""http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/papers/html/ab-722.html"">
                            <hi rend=""color(#1155cc) underline"">Graceful Degradation: Results of the Survey</hi>
                        </ref>.” Presented at Digital Humanities 2010, King’s College, London. 
                        <ptr target=""https://nowviskie.org/Graceful_Degradation.pdf""/>.
                    </bibl>
                    <bibl>Smithies, James, Carina Westling, Anna-Maria Sichani, Pam Mellen, and Arianna Ciula. 2019. “Managing 100 Digital Humanities Projects: Digital Scholarship & Archiving in King’s Digital Lab.” 
                        <hi rend=""italic"">Digital Humanities Quarterly</hi> 13, no 1 (2019). 
                        <ptr target=""http://www.digitalhumanities.org//dhq/vol/13/1/000411/000411.html""/>.
                    </bibl>
                    <bibl>Stanger-Ross, Jordan. 2008. “Citystats and the History of Community and Segregation in Post-WWII Urban Canada.” 
                        <hi rend=""italic"">Journal of the Canadian Historical Association</hi> 19, 2 (2008), 3-22. 
                        <ptr target=""https://citystats.uvic.ca/Citystats_CHA_19.2.pdf""/>.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,digital preservation;project resiliency;static sites,English,"19th century;20th century;digital publishing projects, systems, and methods;english;history;humanities computing;north america;project design, organization, management"
11809,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,How to Set Up a Web Server for Teaching and Research in the Humanities,,Lisa Tagliaferri,workshop / tutorial,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p style=""text-align: left; "">This workshop will go over how to complete an initial Linux server setup for use with the web. We will go over security, firewalls, HTTPS, and high availability. Administering one’s own server rather than relying on managed web hosting empowers researchers, teachers, and students by providing them with complete control over their web assets. The resulting setup can be used for webapps, static sites like Jekyll and Hugo, or more robust sites like WordPress, Omeka, Scalar, and Drupal. These will be ready for use with domain names. In addition to providing an entry point to the web, servers can also enable teams of researchers and students to collaborate on programming projects or access shared data. </p>
        </body>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,devops;linux;web development;web server,English,"computer science;contemporary;english;global;humanities computing;public humanities collaborations and methods;software development, systems, analysis and methods"
11948,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,The Linked Editorial Academic Framework: Creating an editorial environment for collaborative scholarship and publication,,Diane Katherine Jakacki;Susan Brown;James Cummings;Mihaela Ilovan;Carolyn Black,"paper, specified ""short paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p style=""text-align: left;"">This short paper introduces LEAF (the Linked Editorial Academic Framework virtual research environment), an enhanced and expanded collaborative editorial platform that supports a variety of digital scholarly projects through a pipeline of integrated tools for collaborative production and publication of scholarly and documentary collections. Funded through the Canada Foundation for Innovation and the Andrew W. Mellon Foundation, LEAF aims to address the challenges that face many who undertake and maintain large-scale collaborative DH projects now: namely, the need to ensure that these projects can remain operational and available to editors and audiences over the long-haul. It is only by sharing physical, software, and human infrastructures across institutions that this can be accomplished. In so doing we can support scalability, interoperability, and preservation while allowing for dynamic, iterative, and collaborative editing, and therefore ensure that our materials, collections, and editions will remain viable and accessible. The LEAF team aims to do this by integrating best practices for text encoding, annotation, and metadata standards. This short paper will report on the development of LEAF and the functionalities that it will provide. </p>
            <p style=""text-align: left;"">The implementation, and dissemination of LEAF is built upon a collaboration to extend the Canadian Writing Research Collaboratory (CWRC) built by the Universities of Alberta and Guelph (Susan Brown) with Bucknell University (Diane Jakacki), and Newcastle University (James Cummings) as founding partners. This work enhances CWRC’s functionality through collaborative software development that will ultimately support multiple instances of the LEAF platform in Canada, the US, and the UK. At Bucknell, this work will inform the Liberal Arts Based Digital Edition Publishing Cooperative and the Bucknell Digital Press, funded by an Andrew W. Mellon Digital Publishing Cooperative Implementation grant that will support an expanding portfolio of peer-reviewed digital editions and edition clusters. </p>
            <p style=""text-align: left;"">
                <hi style=""font-size:12pt"" xml:space=""preserve"">The LEAF platform combines hardware, software, and personnel. LEAF is being built on a solid foundation in terms of its data models, core functionality, and code management, so that it is positioned for extension and long-term sustainability. The platform is based on the Islandora 8 framework, which combines Drupal 8 with a Fedora 5 repository for long-term preservation. The LEAF repository will customize and enhance Islandora to enable digital humanities workflows and publication needs. Enhancements include an innovative web-based editing tool that allows users to employ TEI XML along with Web Annotation and IIIF standards-compatible Linked Open Data annotations that enhance discoverability and interoperability. </hi>
            </p>
            <p style=""text-align: left;"">
                The founding LEAF institutions are collaborating to upgrade the existing CWRC environment and produce a fully modular platform that will also be hosted on Bucknell’s servers, further tested at Newcastle University, and offered as containerized open-source code freely available for download and installation by other institutions. In particular, LEAF will facilitate the production and publication of dynamic digital scholarly editions and collections, offering multilingual transcription, translation, and image markup. Entirely browser-based, its functionality includes an in-browser XML markup editor, XML rendering tools, built-in text and data visualization tools including the Voyant Tools suite and its Dynamic Table of Contexts Browser. Overall the LEAF platform will provide a sophisticated interface for digital editions in which the XML markup is leveraged for navigation and active reading, and enhanced with Linked Open Data.
            </p>
        </body>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,collaborative publication;critical infrastructure;preservation,English,"15th-17th century;19th century;20th century;analysis;cultural studies;english;global;literary studies;public humanities collaborations and methods;scholarly editing and editions development, analysis, and methods"
